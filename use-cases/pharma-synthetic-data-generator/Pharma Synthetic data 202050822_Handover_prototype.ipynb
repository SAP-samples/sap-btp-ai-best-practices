{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b562fbe-2111-4e4b-a908-b23ec1f7369c",
   "metadata": {},
   "source": [
    "## Synthetic data validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7be3cf-a5b3-4aee-b1a8-ce4ad08b8feb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce47e908-73a6-4162-a32a-79e9d2631e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# Pharma PoC — Synthetic Generator (ml_optimized toggle) + ML Pipeline\n",
    "# + compare_datasets + generate_ml_benchmark_datasets\n",
    "# (compatible with previous artifacts and API)\n",
    "# ================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Dict, Tuple, Optional, Any, List\n",
    "from datetime import datetime\n",
    "import warnings, os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    import openpyxl  # для Excel IO\n",
    "except Exception as e:\n",
    "    print(\"openpyxl not found. Excel export will be skipped.\", e)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1) Data generator (orthogonal rules)\n",
    "# Signal/noise parameters are configured via ml_optimized\n",
    "# ------------------------------------------------\n",
    "class EnhancedPharmaDataGenerator:\n",
    "    def __init__(self, seed: Optional[int] = None, signal_strength: float = 1.0, noise_level: float = 0.01):\n",
    "        if seed is not None:\n",
    "            random.seed(seed); np.random.seed(seed)\n",
    "        self.signal_strength = float(np.clip(signal_strength, 0.0, 1.0))\n",
    "        self.noise_level = float(np.clip(noise_level, 0.0, 0.05))\n",
    "\n",
    "        self.column_headers = [\n",
    "            'ADJ DATE MEDICARE TXN DATE/TIME','NPI PHARMACY NPI','RX NBR PRESCRIPTION NUMBER',\n",
    "            'SERV DATE DATE OF SERVICE','CLAIM ID MEDICARE ICN/AUTH #','PROD SVC ID PRODUCT/SERVICE ID',\n",
    "            'DRUG NAME N/A','QTY DISP QUANTITY DISPENSED','WAC UNIT PRICE WAC UNIT PRICE',\n",
    "            'MFP UNIT PRICE MFP UNIT PRICE','EST REIMB AMT Estimated MFG Reimbursement Amt',\n",
    "            'MFP RULE PRC POINT MFP Rule Price Point','MFP RULE DISC MFP Rule Disc %',\n",
    "            'MFP RULE UNIT PRICE MFP Rule Unit Price','MFG NAME MFG Name','MFG CUST ID MFG Customer ID',\n",
    "            'Medispan/FDB WAC Price','Medispan/FDB MFP Price','Medispan/FDB Effective Date',\n",
    "            'Medispan/FDB Termination Date','835 report Check Number','835 report Claim Number',\n",
    "            '835 report Pharmacy Number','835 report Rx Number','835 report Refill Number',\n",
    "            '835 report Estimated refund from adjudication','835 report Actual Payment Amount',\n",
    "            '835 report Date Filled/Date of Service','835 report Quantity',\n",
    "            '835 report Adjudicated Procedure Code (Product/Service ID)',\n",
    "            '835 report Adjustment Code / CARC codes','835 report Adjustment Amount',\n",
    "            '835 report Adjustment Quantity','835 report Qualifier Code / RARC codes',\n",
    "            'Expected Outcomes Error category','Expected Outcomes Clerk Input',' Questions/comments','Case'\n",
    "        ]\n",
    "\n",
    "        def pv(v): \n",
    "            base = v*(1.0 - 0.8*self.signal_strength)\n",
    "            return max(0.005, base)\n",
    "\n",
    "        self.drugs_db = {\n",
    "            'STELARA INJ 5MG/ML': {'base_wac': 81.5254,'base_mfp': 25.1081,'qty_range': [5],\n",
    "                'ndc_codes': ['57894005427'],'manufacturer': 'JANSSEN BIOTECH','therapeutic_class': 'Immunosuppressant','price_volatility': pv(0.02)},\n",
    "            'ENBREL INJ 25/0.5ML': {'base_wac': 2039.4,'base_mfp': 583.58,'qty_range': [1],\n",
    "                'ndc_codes': ['58406001001'],'manufacturer': 'AMGEN/ IMMUNEX','therapeutic_class': 'Immunosuppressant','price_volatility': pv(0.03)},\n",
    "            'STELARA INJ 90MG/ML': {'base_wac': 29151.46,'base_mfp': 8980.1446,'qty_range': [1],\n",
    "                'ndc_codes': ['57894006103'],'manufacturer': 'JANSSEN BIOTECH','therapeutic_class': 'Immunosuppressant','price_volatility': pv(0.02)},\n",
    "            'ENBREL SRCLK INJ 50MG/ML': {'base_wac': 1635.15,'base_mfp': 583.7669,'qty_range': [1],\n",
    "                'ndc_codes': ['58406044501'],'manufacturer': 'AMGEN/ IMMUNEX','therapeutic_class': 'Immunosuppressant','price_volatility': pv(0.03)},\n",
    "            'ENTRESTO CAP 15-16MG': {'base_wac': 11.7535,'base_mfp': 1.2292,'qty_range': [30],\n",
    "                'ndc_codes': ['00078123820'],'manufacturer': 'NOVARTIS','therapeutic_class': 'Cardiovascular','price_volatility': pv(0.05)},\n",
    "            'FIASP F/P PEN 100U/ML': {'base_wac': 37.26,'base_mfp': 8.96,'qty_range': [10,15],\n",
    "                'ndc_codes': ['00169184110'],'manufacturer': 'NOVO NORDISK','therapeutic_class': 'Diabetes','price_volatility': pv(0.04)},\n",
    "            # extended\n",
    "            'HUMIRA INJ 40MG/0.8ML': {'base_wac': 2400.85,'base_mfp': 720.25,'qty_range': [1,2],\n",
    "                'ndc_codes': ['00074123456'],'manufacturer': 'ABBVIE INC','therapeutic_class': 'Immunosuppressant','price_volatility': pv(0.03)},\n",
    "            'KEYTRUDA INJ 100MG/4ML': {'base_wac': 12500.0,'base_mfp': 3750.0,'qty_range': [1],\n",
    "                'ndc_codes': ['00006234567'],'manufacturer': 'MERCK & CO','therapeutic_class': 'Oncology','price_volatility': pv(0.02)},\n",
    "            'OZEMPIC INJ 0.25MG/1.5ML': {'base_wac': 850.5,'base_mfp': 255.15,'qty_range': [1,3],\n",
    "                'ndc_codes': ['00169345678'],'manufacturer': 'SANOFI-AVENTIS','therapeutic_class': 'Diabetes','price_volatility': pv(0.04)},\n",
    "            'DUPIXENT INJ 300MG/2ML': {'base_wac': 3200.75,'base_mfp': 960.23,'qty_range': [1,2],\n",
    "                'ndc_codes': ['00024456789'],'manufacturer': 'REGENERON PHARM','therapeutic_class': 'Dermatology','price_volatility': pv(0.03)},\n",
    "            'SKYRIZI INJ 150MG/ML': {'base_wac': 18500.0,'base_mfp': 5550.0,'qty_range': [1],\n",
    "                'ndc_codes': ['00074567890'],'manufacturer': 'BRISTOL MYERS SQUIBB','therapeutic_class': 'Dermatology','price_volatility': pv(0.02)}\n",
    "        }\n",
    "\n",
    "        self.mfg_customer_ids = {\n",
    "            'JANSSEN BIOTECH': 'MFP00008','AMGEN/ IMMUNEX': 'MFP00003','NOVARTIS': 'MFP00002',\n",
    "            'NOVO NORDISK': 'MFP00009','ABBVIE INC': 'MFP00010','MERCK & CO': 'MFP00011',\n",
    "            'SANOFI-AVENTIS': 'MFP00012','REGENERON PHARM': 'MFP00013','BRISTOL MYERS SQUIBB': 'MFP00014'\n",
    "        }\n",
    "\n",
    "        self.pharmacy_npis = [1013998921,1518039858,1043382302,1063510097,1326029232,1629341177,1578836698,\n",
    "                              1234567890,1345678901,1456789012,1567890123,1678901234]\n",
    "        self.pharmacy_profiles = {\n",
    "            1013998921:'careful',1518039858:'careful',1043382302:'sloppy',1063510097:'sloppy',\n",
    "            1326029232:'neutral',1629341177:'neutral',1578836698:'neutral',1234567890:'sloppy',\n",
    "            1345678901:'careful',1456789012:'neutral',1567890123:'sloppy',1678901234:'neutral'\n",
    "        }\n",
    "\n",
    "        self.rarc_codes = ['N907','N908','N910','N911']\n",
    "        self.error_categories = [\n",
    "            'Write off due to manual data entry error',\n",
    "            'Write off due to payment applied to the wrong claim',\n",
    "            'Write off due to WAC UNIT PRICE diff from WAC Medispan',\n",
    "            'Write off due to duplicate payment',\n",
    "            'Sent to collection due to MFG timing difference'\n",
    "        ]\n",
    "        self.error_to_rarc_primary = {\n",
    "            'Write off due to manual data entry error': 'N907',\n",
    "            'Write off due to payment applied to the wrong claim': 'N908',\n",
    "            'Write off due to WAC UNIT PRICE diff from WAC Medispan': 'N910',\n",
    "            'Write off due to duplicate payment': 'N911',\n",
    "            'Sent to collection due to MFG timing difference': 'N910'\n",
    "        }\n",
    "        self.error_to_carc = {\n",
    "            'Write off due to manual data entry error': '129',\n",
    "            'Write off due to payment applied to the wrong claim': '22',\n",
    "            'Write off due to WAC UNIT PRICE diff from WAC Medispan': '237',\n",
    "            'Write off due to duplicate payment': '97',\n",
    "            'Sent to collection due to MFG timing difference': '45'\n",
    "        }\n",
    "        self.clerk_input_mapping = {\n",
    "            'Write off due to manual data entry error': [\n",
    "                'Manual correction needed','Data entry validation required',\n",
    "                'Review input accuracy','Store these 340B claims and compare with manufacturer payment'\n",
    "            ],\n",
    "            'Write off due to payment applied to the wrong claim': [\n",
    "                'Payment reallocation required','Accepting MFP price and business has to review all 908',\n",
    "                'Payment reversal and reapply','Business is to review'\n",
    "            ],\n",
    "            'Write off due to WAC UNIT PRICE diff from WAC Medispan': [\n",
    "                'Credit notes not accounted','WAC price verification needed','Medispan price reconciliation required'\n",
    "            ],\n",
    "            'Write off due to duplicate payment': [\n",
    "                'Duplicate payment identified','Payment consolidation required','Refund processing needed','Business is to review'\n",
    "            ],\n",
    "            'Sent to collection due to MFG timing difference': [\n",
    "                'Exception handling','Manufacturer timing review required',\n",
    "                'Collection process initiated','Business is to review'\n",
    "            ]\n",
    "        }\n",
    "        self.slow_mfg = {'JANSSEN BIOTECH','AMGEN/ IMMUNEX','BRISTOL MYERS SQUIBB'}\n",
    "        self.error_underpayment_ranges = {\n",
    "            'Write off due to manual data entry error': (0.05, 0.09),\n",
    "            'Write off due to payment applied to the wrong claim': (0.10, 0.15),\n",
    "            'Write off due to WAC UNIT PRICE diff from WAC Medispan': (0.18, 0.25),\n",
    "            'Write off due to duplicate payment': (0.35, 0.45),\n",
    "            'Sent to collection due to MFG timing difference': (0.26, 0.32)\n",
    "        }\n",
    "        self.dup_rate = 0.06\n",
    "        self.claim_id_counter = 1820000001\n",
    "        self.rx_counter = 1820001\n",
    "\n",
    "    # ---- helpers\n",
    "    def _generate_timestamp(self, year_range=(2025,2026)):\n",
    "        year = random.randint(*year_range); month = random.randint(1,12); day = random.randint(1,28)\n",
    "        hour = random.randint(8,17); minute = random.randint(0,59); second = random.randint(0,59)\n",
    "        return f\"{year:04d}{month:02d}{day:02d}{hour:02d}{minute:02d}{second:02d}0\"\n",
    "\n",
    "    def _generate_date(self, format_type=\"service\", force_month: Optional[int]=None) -> str:\n",
    "        if format_type == \"service\":\n",
    "            year = random.choice([2025,2026])\n",
    "        elif format_type == \"payment\":\n",
    "            year = random.choice([2026,2027])\n",
    "        else:\n",
    "            year = 2026\n",
    "        month = force_month if force_month else random.randint(1,12)\n",
    "        day = random.randint(1,28)\n",
    "        return f\"{day:02d}/{month:02d}/{year}\"\n",
    "\n",
    "    def _pick_drug_by(self, *, classes=None, manufacturers=None, qty_options=None) -> Dict[str, Any]:\n",
    "        pool = []\n",
    "        for name, d in self.drugs_db.items():\n",
    "            if classes and d.get('therapeutic_class') not in classes: continue\n",
    "            if manufacturers and d.get('manufacturer') not in manufacturers: continue\n",
    "            if qty_options and not any(q in d['qty_range'] for q in qty_options): continue\n",
    "            pool.append((name, d))\n",
    "        name, d = random.choice(pool if pool else list(self.drugs_db.items()))\n",
    "        vol = d.get('price_volatility', 0.02)\n",
    "        return {\n",
    "            'name': name,\n",
    "            'wac_price': round(d['base_wac'] * random.uniform(1-vol, 1+vol), 4),\n",
    "            'mfp_price': round(d['base_mfp'] * random.uniform(1-vol, 1+vol), 4),\n",
    "            'quantity': random.choice(d['qty_range']),\n",
    "            'ndc_code': random.choice(d['ndc_codes']),\n",
    "            'manufacturer': d['manufacturer'],\n",
    "            'therapeutic_class': d.get('therapeutic_class','General')\n",
    "        }\n",
    "\n",
    "    def _select_drug_with_variations(self) -> Dict[str, Any]:\n",
    "        all_drugs = list(self.drugs_db.keys())\n",
    "        reference_drugs = all_drugs[:6]; extended_drugs = all_drugs[6:]\n",
    "        name = random.choice(reference_drugs) if random.random() < (0.90 - 0.3*self.noise_level) else random.choice(extended_drugs)\n",
    "        d = self.drugs_db[name]; vol = d.get('price_volatility', 0.02)\n",
    "        return {\n",
    "            'name': name,\n",
    "            'wac_price': round(d['base_wac'] * random.uniform(1-vol, 1+vol), 4),\n",
    "            'mfp_price': round(d['base_mfp'] * random.uniform(1-vol, 1+vol), 4),\n",
    "            'quantity': random.choice(d['qty_range']),\n",
    "            'ndc_code': random.choice(d['ndc_codes']),\n",
    "            'manufacturer': d['manufacturer'],\n",
    "            'therapeutic_class': d.get('therapeutic_class','General')\n",
    "        }\n",
    "\n",
    "    def _calculate_pricing_rules(self, wac_price: float) -> Dict[str, Any]:\n",
    "        p_wac = 0.78\n",
    "        if random.random() < p_wac:\n",
    "            return {'rule_point':'WAC','discount':round(random.uniform(2.0, 20.0), 1),'rule_unit_price':''}\n",
    "        return {'rule_point':'','discount':'','rule_unit_price':round(wac_price * random.uniform(0.66, 0.85), 4)}\n",
    "\n",
    "    def _calculate_estimated_reimbursement(self, wac_price: float, mfp_price: float, quantity: int, rules: Dict[str, Any]) -> float:\n",
    "        base_amount = wac_price * quantity\n",
    "        if rules['rule_point']=='WAC' and rules['discount']!='':\n",
    "            estimated = base_amount * (1 - float(rules['discount'])/100) * random.uniform(0.82, 0.9)\n",
    "        elif rules['rule_unit_price']!='':\n",
    "            estimated = float(rules['rule_unit_price']) * quantity * random.uniform(0.995, 1.01)\n",
    "        else:\n",
    "            estimated = base_amount * random.uniform(0.72, 0.8)\n",
    "        return round(max(estimated, mfp_price * quantity), 2)\n",
    "\n",
    "    # ---- flags for strict rules\n",
    "    def _rule_flags(self, rec: Dict[str, Any]) -> Dict[str, bool]:\n",
    "        def month_from_ddmmyyyy(s: str) -> int:\n",
    "            try: return int(s.split('/')[1])\n",
    "            except: return 1\n",
    "        month = month_from_ddmmyyyy(rec['SERV DATE DATE OF SERVICE'])\n",
    "        quarter = (month-1)//3 + 1\n",
    "\n",
    "        is_wac = (rec['MFP RULE PRC POINT MFP Rule Price Point'] == 'WAC')\n",
    "        disc = float(rec['MFP RULE DISC MFP Rule Disc %']) if rec['MFP RULE DISC MFP Rule Disc %']!='' else 0.0\n",
    "        has_fixed = (rec['MFP RULE UNIT PRICE MFP Rule Unit Price']!='')\n",
    "        qty1 = (rec['QTY DISP QUANTITY DISPENSED']==1)\n",
    "\n",
    "        drug_name = rec['DRUG NAME N/A']\n",
    "        manuf = rec['MFG NAME MFG Name']\n",
    "        th_class = self.drugs_db.get(drug_name,{}).get('therapeutic_class','General')\n",
    "        npi = rec['NPI PHARMACY NPI']\n",
    "        npi_prof = self.pharmacy_profiles.get(npi,'neutral')\n",
    "\n",
    "        est_amt = float(rec['EST REIMB AMT Estimated MFG Reimbursement Amt'])\n",
    "        unit_wac = float(rec['WAC UNIT PRICE WAC UNIT PRICE'])\n",
    "        unit_mfp = float(rec['MFP UNIT PRICE MFP UNIT PRICE'])\n",
    "\n",
    "        flags = dict(\n",
    "            month=month, quarter=quarter,\n",
    "            is_wac_rule=is_wac,\n",
    "            has_fixed_unit_price=has_fixed,\n",
    "            disc_is_high18=(disc>=18.0),\n",
    "            disc_lt3=(disc<3.0),\n",
    "            qty_is_1=qty1,\n",
    "            est_lt_1200=(est_amt<1200.0),\n",
    "            is_slow_mfg=(manuf in self.slow_mfg),\n",
    "            is_immuno_or_derm=(th_class in {'Immunosuppressant','Dermatology'}),\n",
    "            is_cardio_or_diab=(th_class in {'Cardiovascular','Diabetes'}),\n",
    "            npi_sloppy=(npi_prof=='sloppy'),\n",
    "            npi_careful=(npi_prof=='careful'),\n",
    "            q_in_14=(quarter in {1,4}),\n",
    "            est_vs_wac_amt=(est_amt/(unit_wac*max(rec['QTY DISP QUANTITY DISPENSED'],1)+1e-9)),\n",
    "            mfp_vs_wac_unit=(unit_mfp/(unit_wac+1e-9))\n",
    "        )\n",
    "        return flags\n",
    "\n",
    "    def _decide_error_category(self, rec: Dict[str, Any], force_class: Optional[str]=None) -> str:\n",
    "        if force_class: \n",
    "            return force_class\n",
    "        f = self._rule_flags(rec)\n",
    "        if f['is_wac_rule'] and f['disc_is_high18'] and f['is_immuno_or_derm'] and f['qty_is_1']:\n",
    "            return 'Write off due to WAC UNIT PRICE diff from WAC Medispan'\n",
    "        if f['has_fixed_unit_price'] and f['is_slow_mfg'] and f['q_in_14'] and f['qty_is_1']:\n",
    "            return 'Sent to collection due to MFG timing difference'\n",
    "        if f['npi_sloppy'] and f['is_cardio_or_diab'] and f['disc_lt3'] and f['est_lt_1200']:\n",
    "            return 'Write off due to payment applied to the wrong claim'\n",
    "        return 'Write off due to manual data entry error'\n",
    "\n",
    "    def _select_target_variables(self, rec: Dict[str, Any], force_class: Optional[str]=None) -> Tuple[str,str,str]:\n",
    "        err = self._decide_error_category(rec, force_class=force_class)\n",
    "        rarc = self.error_to_rarc_primary[err]\n",
    "        clerk = random.choice(self.clerk_input_mapping[err])\n",
    "        return rarc, err, clerk\n",
    "\n",
    "    def _tune_record_for_error(self, rec: Dict[str, Any], err: str) -> Dict[str, Any]:\n",
    "        drug = None; rules = None; force_month = None\n",
    "        if err == 'Write off due to payment applied to the wrong claim':\n",
    "            drug = self._pick_drug_by(classes={'Cardiovascular','Diabetes'})\n",
    "            if drug['name']=='ENTRESTO CAP 15-16MG': drug['quantity'] = 30\n",
    "            elif drug['name']=='FIASP F/P PEN 100U/ML': drug['quantity'] = random.choice([10,15])\n",
    "            rules = {'rule_point':'WAC','discount':round(random.uniform(0.0, 2.5),1),'rule_unit_price':''}\n",
    "            sloppy_npis = [n for n,p in self.pharmacy_profiles.items() if p=='sloppy']\n",
    "            rec['NPI PHARMACY NPI'] = random.choice(sloppy_npis)\n",
    "        elif err == 'Sent to collection due to MFG timing difference':\n",
    "            drug = self._pick_drug_by(classes={'Immunosuppressant','Dermatology'}, manufacturers=self.slow_mfg, qty_options=[1])\n",
    "            drug['quantity'] = 1\n",
    "            rules = {'rule_point':'','discount':'','rule_unit_price': round(drug['wac_price']*random.uniform(0.66,0.84),4)}\n",
    "            force_month = random.choice([1,2,3,10,11,12])\n",
    "        elif err == 'Write off due to WAC UNIT PRICE diff from WAC Medispan':\n",
    "            drug = self._pick_drug_by(classes={'Immunosuppressant','Dermatology'}, qty_options=[1])\n",
    "            drug['quantity'] = 1\n",
    "            rules = {'rule_point':'WAC','discount': round(random.uniform(20.0,25.0),1),'rule_unit_price':''}\n",
    "        elif err == 'Write off due to duplicate payment':\n",
    "            return rec\n",
    "        else:\n",
    "            drug = self._pick_drug_by(classes=None)\n",
    "            rules = {'rule_point':'WAC','discount': round(random.uniform(6.0,12.0),1),'rule_unit_price':''}\n",
    "            non_sloppy = [n for n,p in self.pharmacy_profiles.items() if p!='sloppy']\n",
    "            rec['NPI PHARMACY NPI'] = random.choice(non_sloppy)\n",
    "\n",
    "        if drug is not None:\n",
    "            rec.update({\n",
    "                'DRUG NAME N/A': drug['name'],\n",
    "                'PROD SVC ID PRODUCT/SERVICE ID': drug['ndc_code'],\n",
    "                'QTY DISP QUANTITY DISPENSED': drug['quantity'],\n",
    "                'WAC UNIT PRICE WAC UNIT PRICE': drug['wac_price'],\n",
    "                'MFP UNIT PRICE MFP UNIT PRICE': drug['mfp_price'],\n",
    "                'MFG NAME MFG Name': drug['manufacturer'],\n",
    "                'MFG CUST ID MFG Customer ID': self.mfg_customer_ids[drug['manufacturer']],\n",
    "                'Medispan/FDB WAC Price': drug['wac_price'],\n",
    "                'Medispan/FDB MFP Price': drug['mfp_price'],\n",
    "            })\n",
    "            if force_month:\n",
    "                rec['SERV DATE DATE OF SERVICE'] = self._generate_date(\"service\", force_month=force_month)\n",
    "\n",
    "        if rules is not None:\n",
    "            rec['MFP RULE PRC POINT MFP Rule Price Point'] = rules['rule_point']\n",
    "            rec['MFP RULE DISC MFP Rule Disc %'] = rules['discount']\n",
    "            rec['MFP RULE UNIT PRICE MFP Rule Unit Price'] = rules['rule_unit_price']\n",
    "\n",
    "        est = self._calculate_estimated_reimbursement(\n",
    "            rec['WAC UNIT PRICE WAC UNIT PRICE'], rec['MFP UNIT PRICE MFP UNIT PRICE'],\n",
    "            rec['QTY DISP QUANTITY DISPENSED'],\n",
    "            {'rule_point': rec['MFP RULE PRC POINT MFP Rule Price Point'],\n",
    "             'discount': rec['MFP RULE DISC MFP Rule Disc %'],\n",
    "             'rule_unit_price': rec['MFP RULE UNIT PRICE MFP Rule Unit Price']}\n",
    "        )\n",
    "        rec['EST REIMB AMT Estimated MFG Reimbursement Amt'] = est\n",
    "        return rec\n",
    "\n",
    "    def _generate_base_record(self, force_error: Optional[str]=None) -> Tuple[Dict[str, Any], float]:\n",
    "        drug = self._select_drug_with_variations()\n",
    "        rules = self._calculate_pricing_rules(drug['wac_price'])\n",
    "        est = self._calculate_estimated_reimbursement(drug['wac_price'], drug['mfp_price'], drug['quantity'], rules)\n",
    "        adj_date = self._generate_timestamp()\n",
    "        serv_date = self._generate_date(\"service\")\n",
    "        claim_id = getattr(self, \"claim_id_counter\"); self.claim_id_counter += 1\n",
    "        rx_nbr = getattr(self, \"rx_counter\"); self.rx_counter += 1\n",
    "        npi = random.choice(self.pharmacy_npis)\n",
    "\n",
    "        rec = {\n",
    "            'ADJ DATE MEDICARE TXN DATE/TIME': adj_date,\n",
    "            'NPI PHARMACY NPI': npi,\n",
    "            'RX NBR PRESCRIPTION NUMBER': rx_nbr,\n",
    "            'SERV DATE DATE OF SERVICE': serv_date,\n",
    "            'CLAIM ID MEDICARE ICN/AUTH #': claim_id,\n",
    "            'PROD SVC ID PRODUCT/SERVICE ID': drug['ndc_code'],\n",
    "            'DRUG NAME N/A': drug['name'],\n",
    "            'QTY DISP QUANTITY DISPENSED': drug['quantity'],\n",
    "            'WAC UNIT PRICE WAC UNIT PRICE': drug['wac_price'],\n",
    "            'MFP UNIT PRICE MFP UNIT PRICE': drug['mfp_price'],\n",
    "            'EST REIMB AMT Estimated MFG Reimbursement Amt': est,\n",
    "            'MFP RULE PRC POINT MFP Rule Price Point': rules['rule_point'],\n",
    "            'MFP RULE DISC MFP Rule Disc %': rules['discount'],\n",
    "            'MFP RULE UNIT PRICE MFP Rule Unit Price': rules['rule_unit_price'],\n",
    "            'MFG NAME MFG Name': drug['manufacturer'],\n",
    "            'MFG CUST ID MFG Customer ID': self.mfg_customer_ids[drug['manufacturer']],\n",
    "            'Medispan/FDB WAC Price': drug['wac_price'],\n",
    "            'Medispan/FDB MFP Price': drug['mfp_price'],\n",
    "            'Medispan/FDB Effective Date': '01/01/2026',\n",
    "            'Medispan/FDB Termination Date': '01/01/2027',\n",
    "        }\n",
    "\n",
    "        if force_error is not None:\n",
    "            rec = self._tune_record_for_error(rec, force_error)\n",
    "            est = rec['EST REIMB AMT Estimated MFG Reimbursement Amt']\n",
    "\n",
    "        rarc, err, clerk = self._select_target_variables(rec, force_class=force_error)\n",
    "        rec.update({\n",
    "            '835 report Qualifier Code / RARC codes': rarc,\n",
    "            'Expected Outcomes Error category': err,\n",
    "            'Expected Outcomes Clerk Input': clerk,\n",
    "            ' Questions/comments': f\"Review case for {rec['DRUG NAME N/A']} - {err}\",\n",
    "        })\n",
    "        return rec, est\n",
    "\n",
    "    def _calculate_actual_payment(self, estimated_amount: float, error_category: str) -> float:\n",
    "        lo, hi = self.error_underpayment_ranges[error_category]\n",
    "        span = (hi - lo) * (1 - 0.6*self.signal_strength)\n",
    "        mid = (lo + hi)/2\n",
    "        lo2, hi2 = max(0.0, mid - span/2), mid + span/2\n",
    "        return round(max(estimated_amount * (1 - random.uniform(lo2, hi2)), 1.0), 2)\n",
    "\n",
    "    def generate_step1_record(self, force_error: Optional[str]=None) -> Dict[str, Any]:\n",
    "        base, est = self._generate_base_record(force_error=force_error)\n",
    "        actual = self._calculate_actual_payment(est, base['Expected Outcomes Error category'])\n",
    "        base.update({\n",
    "            '835 report Check Number': 'N/A',\n",
    "            '835 report Claim Number': base['CLAIM ID MEDICARE ICN/AUTH #'],\n",
    "            '835 report Pharmacy Number': base['NPI PHARMACY NPI'],\n",
    "            '835 report Rx Number': base['RX NBR PRESCRIPTION NUMBER'],\n",
    "            '835 report Refill Number': '00',\n",
    "            '835 report Estimated refund from adjudication': 'N/A',\n",
    "            '835 report Actual Payment Amount': actual,\n",
    "            '835 report Date Filled/Date of Service': self._generate_date(\"payment\"),\n",
    "            '835 report Quantity': base['QTY DISP QUANTITY DISPENSED'],\n",
    "            '835 report Adjudicated Procedure Code (Product/Service ID)': base['PROD SVC ID PRODUCT/SERVICE ID'],\n",
    "            '835 report Adjustment Code / CARC codes': self.error_to_carc[base['Expected Outcomes Error category']],\n",
    "            '835 report Adjustment Amount': 'N/A',\n",
    "            '835 report Adjustment Quantity': 'N/A',\n",
    "            'Case': 'Step 1 - Train'\n",
    "        })\n",
    "        return base\n",
    "\n",
    "    def generate_step2_record(self) -> Dict[str, Any]:\n",
    "        rec = self.generate_step1_record()\n",
    "        rec['Case'] = 'Step 2 - Suggest'\n",
    "        return rec\n",
    "\n",
    "    def generate_step3_record(self) -> Dict[str, Any]:\n",
    "        base, _ = self._generate_base_record()\n",
    "        base.update({\n",
    "            '835 report Check Number': '',\n",
    "            '835 report Claim Number': '',\n",
    "            '835 report Pharmacy Number': '',\n",
    "            '835 report Rx Number': '',\n",
    "            '835 report Refill Number': '',\n",
    "            '835 report Estimated refund from adjudication': '',\n",
    "            '835 report Actual Payment Amount': '',\n",
    "            '835 report Date Filled/Date of Service': '',\n",
    "            '835 report Quantity': '',\n",
    "            '835 report Adjudicated Procedure Code (Product/Service ID)': '',\n",
    "            '835 report Adjustment Code / CARC codes': '',\n",
    "            '835 report Adjustment Amount': '',\n",
    "            '835 report Adjustment Quantity': '',\n",
    "            'Case': 'Step 3 - Forecast'\n",
    "        })\n",
    "        return base\n",
    "\n",
    "    def _make_duplicates(self, records: list, rate: float = 0.06):\n",
    "        n = int(len(records) * rate)\n",
    "        source_pool = [r for r in records if r['Case']!='Step 3 - Forecast']\n",
    "        for _ in range(n):\n",
    "            base = random.choice(source_pool)\n",
    "            dup = base.copy()\n",
    "            dup['ADJ DATE MEDICARE TXN DATE/TIME'] = self._generate_timestamp()\n",
    "            dup['SERV DATE DATE OF SERVICE'] = self._generate_date(\"service\")\n",
    "            dup['Expected Outcomes Error category'] = 'Write off due to duplicate payment'\n",
    "            dup['Expected Outcomes Clerk Input'] = random.choice(\n",
    "                self.clerk_input_mapping['Write off due to duplicate payment']\n",
    "            )\n",
    "            dup['835 report Qualifier Code / RARC codes'] = 'N911'\n",
    "            dup['835 report Adjustment Code / CARC codes'] = self.error_to_carc['Write off due to duplicate payment']\n",
    "            records.append(dup)\n",
    "\n",
    "    def generate_etalon_based_dataset(self, n_step1: int = 350, n_step2: int = 350, n_step3: int = 350) -> pd.DataFrame:\n",
    "        all_records = []\n",
    "        step1_quota = {  # сбалансированный train\n",
    "            'Write off due to manual data entry error': 0.20,\n",
    "            'Write off due to payment applied to the wrong claim': 0.20,\n",
    "            'Write off due to WAC UNIT PRICE diff from WAC Medispan': 0.20,\n",
    "            'Write off due to duplicate payment': 0.20,\n",
    "            'Sent to collection due to MFG timing difference': 0.20\n",
    "        }\n",
    "        min_counts = {k: int(round(n_step1 * v)) for k,v in step1_quota.items()}\n",
    "        counts = {k: 0 for k in min_counts}\n",
    "        for _ in range(n_step1):\n",
    "            force = next((c for c in min_counts if counts[c] < min_counts[c]), None)\n",
    "            rec = self.generate_step1_record(force_error=force)\n",
    "            counts[rec['Expected Outcomes Error category']] += 1\n",
    "            all_records.append(rec)\n",
    "        for _ in range(n_step2):\n",
    "            all_records.append(self.generate_step2_record())\n",
    "        for _ in range(n_step3):\n",
    "            all_records.append(self.generate_step3_record())\n",
    "        self._make_duplicates(all_records, rate=self.dup_rate)\n",
    "        random.shuffle(all_records)\n",
    "        df = pd.DataFrame(all_records).reindex(columns=self.column_headers)\n",
    "        return df\n",
    "\n",
    "    def quality_check(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        quality_stats = {}\n",
    "        case_dist = df['Case'].value_counts()\n",
    "        quality_stats['case_distribution'] = case_dist.to_dict()\n",
    "\n",
    "        filled = df[df['Case']!='Step 3 - Forecast']\n",
    "        if len(filled)>0:\n",
    "            est = pd.to_numeric(filled['EST REIMB AMT Estimated MFG Reimbursement Amt'], errors='coerce')\n",
    "            act = pd.to_numeric(filled['835 report Actual Payment Amount'], errors='coerce')\n",
    "            valid = (~est.isna()) & (~act.isna())\n",
    "            if valid.sum()>0:\n",
    "                under = ((est[valid]-act[valid])/est[valid]*100)\n",
    "                quality_stats['payment_validation'] = {\n",
    "                    'all_underpaid': bool((act[valid] < est[valid]).all()),\n",
    "                    'avg_underpayment_pct': round(float(under.mean()),2),\n",
    "                    'underpayment_range': [round(float(under.min()),2), round(float(under.max()),2)]\n",
    "                }\n",
    "\n",
    "        step3 = df[df['Case']=='Step 3 - Forecast']\n",
    "        if len(step3)>0:\n",
    "            u_ag = ['835 report Check Number','835 report Claim Number','835 report Pharmacy Number','835 report Rx Number',\n",
    "                    '835 report Refill Number','835 report Estimated refund from adjudication','835 report Actual Payment Amount',\n",
    "                    '835 report Date Filled/Date of Service','835 report Quantity','835 report Adjudicated Procedure Code (Product/Service ID)',\n",
    "                    '835 report Adjustment Code / CARC codes','835 report Adjustment Amount','835 report Adjustment Quantity']\n",
    "            empty_ok = {f:int((step3[f]=='').sum()) for f in u_ag}\n",
    "            quality_stats['step3_validation'] = {'empty_u_ag_fields': empty_ok}\n",
    "\n",
    "        rel = {}\n",
    "        for s,t,desc in [\n",
    "            ('WAC UNIT PRICE WAC UNIT PRICE','Medispan/FDB WAC Price','I → Q'),\n",
    "            ('CLAIM ID MEDICARE ICN/AUTH #','835 report Claim Number','E → V'),\n",
    "            ('NPI PHARMACY NPI','835 report Pharmacy Number','B → W'),\n",
    "            ('RX NBR PRESCRIPTION NUMBER','835 report Rx Number','C → X'),\n",
    "            ('QTY DISP QUANTITY DISPENSED','835 report Quantity','H → AC')\n",
    "        ]:\n",
    "            check_df = df if '835 report' not in t else df[df['Case']!='Step 3 - Forecast']\n",
    "            if len(check_df)>0:\n",
    "                rel[desc] = round(100*(check_df[s]==check_df[t]).sum()/len(check_df),1)\n",
    "        quality_stats['relationship_validation'] = rel\n",
    "\n",
    "        diversity_stats = {\n",
    "            'unique_drugs': int(df['DRUG NAME N/A'].nunique()),\n",
    "            'unique_manufacturers': int(df['MFG NAME MFG Name'].nunique()),\n",
    "            'unique_pharmacies': int(df['NPI PHARMACY NPI'].nunique()),\n",
    "            'unique_rarc_codes': int(df['835 report Qualifier Code / RARC codes'].nunique()),\n",
    "            'unique_error_categories': int(df['Expected Outcomes Error category'].nunique())\n",
    "        }\n",
    "        quality_stats['diversity_stats'] = diversity_stats\n",
    "\n",
    "        wac_rules = int((df['MFP RULE PRC POINT MFP Rule Price Point']=='WAC').sum())\n",
    "        fixed_rules = int((df['MFP RULE UNIT PRICE MFP Rule Unit Price']!='').sum())\n",
    "        total = len(df)\n",
    "        pricing_stats = {\n",
    "            'wac_rules_pct': round(100*wac_rules/total,1),\n",
    "            'fixed_rules_pct': round(100*fixed_rules/total,1),\n",
    "            'mutually_exclusive': bool(len(df[(df['MFP RULE PRC POINT MFP Rule Price Point']=='WAC') & (df['MFP RULE UNIT PRICE MFP Rule Unit Price']!='')])==0)\n",
    "        }\n",
    "        quality_stats['pricing_stats'] = pricing_stats\n",
    "\n",
    "        indicators = 0\n",
    "        indicators += 1 if quality_stats.get('payment_validation',{}).get('all_underpaid',False) else 0\n",
    "        indicators += 1 if pricing_stats['mutually_exclusive'] else 0\n",
    "        indicators += 1 if diversity_stats['unique_drugs']>=6 else 0\n",
    "        indicators += 1 if all(v>95 for v in rel.values()) else 0\n",
    "        indicators += 1\n",
    "        quality_stats['quality_score'] = round(100*indicators/5,0)\n",
    "        return quality_stats\n",
    "\n",
    "    def save_to_excel(self, df: pd.DataFrame, filename: str) -> Optional[str]:\n",
    "        try:\n",
    "            with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
    "                df.to_excel(writer, sheet_name='Scenario Data Structure', index=False)\n",
    "                ws = writer.sheets['Scenario Data Structure']\n",
    "                for col in ws.columns:\n",
    "                    max_len = max(len(str(c.value)) if c.value is not None else 0 for c in col)\n",
    "                    ws.column_dimensions[col[0].column_letter].width = min(max_len+2, 50)\n",
    "            return filename\n",
    "        except Exception as e:\n",
    "            print(\"Excel save error:\", e)\n",
    "            return None\n",
    "\n",
    "    def save_to_csv(self, df: pd.DataFrame, filename: str) -> Optional[str]:\n",
    "        try:\n",
    "            df.to_csv(filename, index=False)\n",
    "            return filename\n",
    "        except Exception as e:\n",
    "            print(\"CSV save error:\", e)\n",
    "            return None\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "#2) Compatible wrappers (added ml_optimized)\n",
    "# ------------------------------------------------\n",
    "def _make_generator_from_flag(seed: Optional[int], ml_optimized: bool) -> EnhancedPharmaDataGenerator:\n",
    "    \"\"\"\n",
    "    If ml_optimized=True — maximum signal/minimum noise.\n",
    "    If False — moderate signal/slightly more noise (for benchmark).\n",
    "    \"\"\"\n",
    "    if ml_optimized:\n",
    "        return EnhancedPharmaDataGenerator(seed=seed, signal_strength=1.0, noise_level=0.005)\n",
    "    else:\n",
    "        return EnhancedPharmaDataGenerator(seed=seed, signal_strength=0.7, noise_level=0.025)\n",
    "\n",
    "def generate_etalon_Pharma_dataset(seed: Optional[int] = None, ml_optimized: bool = False):\n",
    "    \"\"\"\n",
    "    Compatible API:\n",
    "    df, stats, xlsx, csv = generate_etalon_Pharma_dataset(seed=42, ml_optimized=True)\n",
    "    \"\"\"\n",
    "    gen = _make_generator_from_flag(seed, ml_optimized)\n",
    "    df = gen.generate_etalon_based_dataset(350, 350, 350)\n",
    "    stats = gen.quality_check(df)\n",
    "    # имена файлов — стабильные\n",
    "    base = 'optimized' if ml_optimized else 'baseline'\n",
    "    excel_path = f'Pharma_poc_enhanced_1050_{base}.xlsx'\n",
    "    csv_path = f'Pharma_poc_enhanced_1050_{base}.csv'\n",
    "    gen.save_to_excel(df, excel_path)\n",
    "    gen.save_to_csv(df, csv_path)\n",
    "    return df, stats, excel_path, csv_path\n",
    "\n",
    "def generate_custom_dataset(step1_count: int = 100,\n",
    "                            step2_count: int = 100,\n",
    "                            step3_count: int = 100,\n",
    "                            seed: Optional[int] = None,\n",
    "                            ml_optimized: bool = False):\n",
    "    \"\"\"\n",
    "    Compatible API:\n",
    "    df, stats, xlsx, csv = generate_custom_dataset(500, 250, 250, ml_optimized=True)\n",
    "    \"\"\"\n",
    "    gen = _make_generator_from_flag(seed, ml_optimized)\n",
    "    df = gen.generate_etalon_based_dataset(step1_count, step2_count, step3_count)\n",
    "    stats = gen.quality_check(df)\n",
    "    total = step1_count + step2_count + step3_count\n",
    "    base = 'optimized' if ml_optimized else 'baseline'\n",
    "    excel_path = f'Pharma_poc_custom_{total}_{base}.xlsx'\n",
    "    csv_path = f'Pharma_poc_custom_{total}_{base}.csv'\n",
    "    gen.save_to_excel(df, excel_path)\n",
    "    gen.save_to_csv(df, csv_path)\n",
    "    print(f\"[Custom Generation] {step1_count}+{step2_count}+{step3_count} = {total} records\")\n",
    "    print(f\"Files created:\\n - {excel_path}\\n - {csv_path}\")\n",
    "    return df, stats, excel_path, csv_path\n",
    "\n",
    "def analyze_existing_data(filename: str) -> Optional[Dict[str, Any]]:\n",
    "    try:\n",
    "        df = pd.read_excel(filename)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Could not read file '{filename}': {e}\")\n",
    "        return None\n",
    "    gen = EnhancedPharmaDataGenerator(seed=0)\n",
    "    stats = gen.quality_check(df)\n",
    "    try:\n",
    "        mem_kb = df.memory_usage(deep=True).sum() / 1024\n",
    "    except Exception:\n",
    "        mem_kb = None\n",
    "    print(\"\\n[Existing File Analysis]\")\n",
    "    print(f\"  • Filename: {filename}\")\n",
    "    print(f\"  • Rows × Cols: {df.shape[0]} × {df.shape[1]}\")\n",
    "    if mem_kb is not None:\n",
    "        print(f\"  • Approx size in memory: {mem_kb:.1f} KB\")\n",
    "    print(f\"  • Case distribution: {stats.get('case_distribution', {})}\")\n",
    "    print(f\"  • Quality score: {stats.get('quality_score', 'NA')}%\")\n",
    "    return stats\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "#3) Pipeline (as before, compatible artifacts)\n",
    "# ------------------------------------------------\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class PharmaPoCMLPipeline:      # Baseline w Random Forest\n",
    "    def __init__(self, input_file):\n",
    "        self.input_file = input_file\n",
    "        self.df = None\n",
    "        self.train_data = None\n",
    "        self.step2_data = None\n",
    "        self.step3_data = None\n",
    "        self.model1_ah = RandomForestClassifier(n_estimators=400, random_state=42)\n",
    "        self.model1_ai = RandomForestClassifier(n_estimators=400, random_state=42, class_weight='balanced')\n",
    "        self.model2_ah = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "        self.model2_ai = RandomForestClassifier(n_estimators=300, random_state=42, class_weight='balanced')\n",
    "        self.label_encoders = {}\n",
    "        self.target_encoders = {}\n",
    "        self.explainers = {}\n",
    "        self.model1_features = []\n",
    "        self.model2_features = []\n",
    "        print(\"🚀 Pharma PoC ML Pipeline initialized (rule-first)\")\n",
    "\n",
    "    def load_and_prepare_data(self):\n",
    "        print(\"📁 Loading data from:\", self.input_file)\n",
    "        self.df = pd.read_excel(self.input_file)\n",
    "        print(f\"✅ Loaded {len(self.df)} records\")\n",
    "        self.train_data = self.df[self.df['Case'] == 'Step 1 - Train'].copy()\n",
    "        self.step2_data = self.df[self.df['Case'] == 'Step 2 - Suggest'].copy()\n",
    "        self.step3_data = self.df[self.df['Case'] == 'Step 3 - Forecast'].copy()\n",
    "        case_dist = self.df['Case'].value_counts()\n",
    "        print(\"📊 Case distribution:\")\n",
    "        for case, count in case_dist.items():\n",
    "            pct = (count/len(self.df))*100\n",
    "            print(f\"   {case}: {count} ({pct:.1f}%)\")\n",
    "        print(f\"🎯 Training data: {len(self.train_data)}\")\n",
    "        print(f\"🎯 Step 2 (Suggest): {len(self.step2_data)}\")\n",
    "        print(f\"🎯 Step 3 (Forecast): {len(self.step3_data)}\")\n",
    "\n",
    "    def identify_significant_features(self):\n",
    "        print(\"\\n🔍 Identifying significant features...\")\n",
    "        all_columns = list(self.df.columns)\n",
    "        exclude_fields = [\n",
    "            'ADJ DATE MEDICARE TXN DATE/TIME','RX NBR PRESCRIPTION NUMBER','CLAIM ID MEDICARE ICN/AUTH #',\n",
    "            'Medispan/FDB Effective Date','Medispan/FDB Termination Date','835 report Check Number',\n",
    "            '835 report Refill Number','835 report Estimated refund from adjudication',\n",
    "            '835 report Adjustment Amount','835 report Adjustment Quantity','Case'\n",
    "        ]\n",
    "        copy_fields = [\n",
    "            'Medispan/FDB WAC Price','835 report Claim Number','835 report Pharmacy Number',\n",
    "            '835 report Rx Number','835 report Date Filled/Date of Service','835 report Quantity'\n",
    "        ]\n",
    "        targets = ['835 report Qualifier Code / RARC codes','Expected Outcomes Error category','Expected Outcomes Clerk Input',' Questions/comments']\n",
    "        exclude_all = exclude_fields + copy_fields + targets\n",
    "        m1_candidates = [c for c in all_columns if c not in exclude_all]\n",
    "        m2_exclude_835 = [c for c in all_columns if c.startswith('835 report')]\n",
    "        m2_candidates = [c for c in all_columns if c not in (exclude_all + m2_exclude_835)]\n",
    "\n",
    "        def has_var(col): return self.df[col].nunique(dropna=False) > 1\n",
    "        self.model1_features = [c for c in m1_candidates if has_var(c)]\n",
    "        self.model2_features = [c for c in m2_candidates if has_var(c)]\n",
    "\n",
    "        print(f\"📊 Model 1 base features ({len(self.model1_features)}):\")\n",
    "        for i, f in enumerate(self.model1_features, 1): print(f\"{i:>4}. {f}\")\n",
    "        print(f\"\\n📊 Model 2 base features ({len(self.model2_features)}):\")\n",
    "        for i, f in enumerate(self.model2_features, 1): print(f\"{i:>4}. {f}\")\n",
    "\n",
    "        self.engineered_flags = [\n",
    "            '_month','_quarter','_is_wac_rule','_has_fixed_unit_price','_disc_is_high18',\n",
    "            '_disc_lt3','_qty_is_1','_est_lt_1200','_q_in_14','_is_slow_mfg',\n",
    "            '_is_immuno_or_derm','_is_cardio_or_diab','_est_vs_wac_amt','_mfp_vs_wac_unit',\n",
    "            '_rule_wac18_immunoDerm_qty1','_rule_fixed_slow_q14_qty1'\n",
    "        ]\n",
    "        print(\"\\n➕ Engineered features to be added at transform-time:\")\n",
    "        print(\"   \" + \", \".join(self.engineered_flags))\n",
    "\n",
    "    def _compute_engineered(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        def month_from_ddmmyyyy(s: str) -> int:\n",
    "            try: return int(str(s).split('/')[1])\n",
    "            except: return 1\n",
    "        m = df['SERV DATE DATE OF SERVICE'].astype(str).map(month_from_ddmmyyyy)\n",
    "        q = ((m-1)//3 + 1).astype(int)\n",
    "        is_wac = (df['MFP RULE PRC POINT MFP Rule Price Point'].astype(str) == 'WAC')\n",
    "        disc = pd.to_numeric(df['MFP RULE DISC MFP Rule Disc %'], errors='coerce').fillna(0.0)\n",
    "        has_fixed = (df['MFP RULE UNIT PRICE MFP Rule Unit Price'].astype(str)!='')\n",
    "        qty1 = (pd.to_numeric(df['QTY DISP QUANTITY DISPENSED'], errors='coerce').fillna(0)==1)\n",
    "        est_amt = pd.to_numeric(df['EST REIMB AMT Estimated MFG Reimbursement Amt'], errors='coerce').fillna(0.0)\n",
    "        wac_unit = pd.to_numeric(df['WAC UNIT PRICE WAC UNIT PRICE'], errors='coerce').fillna(1.0)\n",
    "        mfp_unit = pd.to_numeric(df['MFP UNIT PRICE MFP UNIT PRICE'], errors='coerce').fillna(1.0)\n",
    "        qty = pd.to_numeric(df['QTY DISP QUANTITY DISPENSED'], errors='coerce').fillna(1.0)\n",
    "        immuno_derm_set = {'Immunosuppressant','Dermatology'}\n",
    "        cardio_diab_set = {'Cardiovascular','Diabetes'}\n",
    "        slow_mfg_set = {'JANSSEN BIOTECH','AMGEN/ IMMUNEX','BRISTOL MYERS SQUIBB'}\n",
    "        def infer_class(drug: str) -> str:\n",
    "            drug = str(drug)\n",
    "            if any(k in drug for k in ['STELARA','ENBREL','HUMIRA']): return 'Immunosuppressant'\n",
    "            if any(k in drug for k in ['DUPIXENT','SKYRIZI']): return 'Dermatology'\n",
    "            if 'ENTRESTO' in drug: return 'Cardiovascular'\n",
    "            if any(k in drug for k in ['OZEMPIC','FIASP']): return 'Diabetes'\n",
    "            return 'General'\n",
    "        th_cls = df['DRUG NAME N/A'].astype(str).map(infer_class)\n",
    "        is_immuno_or_derm = th_cls.isin(immuno_derm_set)\n",
    "        is_cardio_or_diab = th_cls.isin(cardio_diab_set)\n",
    "        is_slow_mfg = df['MFG NAME MFG Name'].isin(slow_mfg_set)\n",
    "        npi_prof_map = {\n",
    "            1013998921:'careful',1518039858:'careful',1043382302:'sloppy',1063510097:'sloppy',\n",
    "            1326029232:'neutral',1629341177:'neutral',1578836698:'neutral',1234567890:'sloppy',\n",
    "            1345678901:'careful',1456789012:'neutral',1567890123:'sloppy',1678901234:'neutral'\n",
    "        }\n",
    "        npi_prof = df['NPI PHARMACY NPI'].map(npi_prof_map).fillna('neutral')\n",
    "        npi_sloppy = (npi_prof=='sloppy')\n",
    "        est_vs_wac_amt = est_amt / (wac_unit*qty + 1e-9)\n",
    "        mfp_vs_wac_unit = mfp_unit / (wac_unit + 1e-9)\n",
    "        engineered = pd.DataFrame({\n",
    "            '_month': m.astype(int),\n",
    "            '_quarter': q.astype(int),\n",
    "            '_is_wac_rule': is_wac.astype(int),\n",
    "            '_has_fixed_unit_price': has_fixed.astype(int),\n",
    "            '_disc_is_high18': (disc>=18.0).astype(int),\n",
    "            '_disc_lt3': (disc<3.0).astype(int),\n",
    "            '_qty_is_1': qty1.astype(int),\n",
    "            '_est_lt_1200': (est_amt<1200.0).astype(int),\n",
    "            '_q_in_14': q.isin([1,4]).astype(int),\n",
    "            '_is_slow_mfg': is_slow_mfg.astype(int),\n",
    "            '_is_immuno_or_derm': is_immuno_or_derm.astype(int),\n",
    "            '_is_cardio_or_diab': is_cardio_or_diab.astype(int),\n",
    "            '_est_vs_wac_amt': est_vs_wac_amt.values,\n",
    "            '_mfp_vs_wac_unit': mfp_vs_wac_unit.values\n",
    "        }, index=df.index)\n",
    "        engineered['_rule_wac18_immunoDerm_qty1'] = (\n",
    "            engineered['_is_wac_rule'].eq(1) &\n",
    "            engineered['_disc_is_high18'].eq(1) &\n",
    "            engineered['_is_immuno_or_derm'].eq(1) &\n",
    "            engineered['_qty_is_1'].eq(1)\n",
    "        ).astype(int)\n",
    "        engineered['_rule_fixed_slow_q14_qty1'] = (\n",
    "            engineered['_has_fixed_unit_price'].eq(1) &\n",
    "            engineered['_is_slow_mfg'].eq(1) &\n",
    "            engineered['_q_in_14'].eq(1) &\n",
    "            engineered['_qty_is_1'].eq(1)\n",
    "        ).astype(int)\n",
    "        return engineered\n",
    "\n",
    "    def _fit_transform_encoders(self, X: pd.DataFrame, fit: bool) -> pd.DataFrame:\n",
    "        Xc = X.copy().fillna('__MISSING__')\n",
    "        for col in Xc.columns:\n",
    "            if Xc[col].dtype == 'object' or Xc[col].dtype.name == 'category':\n",
    "                if fit:\n",
    "                    le = self.label_encoders.get(col, LabelEncoder())\n",
    "                    Xc[col] = le.fit_transform(Xc[col].astype(str))\n",
    "                    self.label_encoders[col] = le\n",
    "                else:\n",
    "                    if col in self.label_encoders:\n",
    "                        le = self.label_encoders[col]\n",
    "                        vals = Xc[col].astype(str)\n",
    "                        known = set(le.classes_)\n",
    "                        unseen = set(vals.unique()) - known\n",
    "                        if unseen:\n",
    "                            most = le.classes_[0]\n",
    "                            vals = vals.where(vals.isin(known), most)\n",
    "                        Xc[col] = le.transform(vals)\n",
    "                    else:\n",
    "                        tmp = LabelEncoder()\n",
    "                        Xc[col] = tmp.fit_transform(Xc[col].astype(str))\n",
    "            else:\n",
    "                Xc[col] = pd.to_numeric(Xc[col], errors='coerce').fillna(0)\n",
    "        return Xc\n",
    "\n",
    "    def _prepare_X(self, df: pd.DataFrame, base_feats: List[str], fit=False) -> pd.DataFrame:\n",
    "        eng = self._compute_engineered(df)\n",
    "        X = pd.concat([df[base_feats].copy(), eng], axis=1)\n",
    "        Xenc = self._fit_transform_encoders(X, fit=fit)\n",
    "        return Xenc\n",
    "\n",
    "    def prepare_targets(self, data, fit_encoders=False):\n",
    "        ah = data['835 report Qualifier Code / RARC codes'].fillna('__MISSING__').astype(str)\n",
    "        ai = data['Expected Outcomes Error category'].fillna('__MISSING__').astype(str)\n",
    "        if fit_encoders:\n",
    "            self.target_encoders['AH'] = LabelEncoder()\n",
    "            self.target_encoders['AI'] = LabelEncoder()\n",
    "            y_ah = self.target_encoders['AH'].fit_transform(ah)\n",
    "            y_ai = self.target_encoders['AI'].fit_transform(ai)\n",
    "        else:\n",
    "            y_ah = self.target_encoders['AH'].transform(ah)\n",
    "            y_ai = self.target_encoders['AI'].transform(ai)\n",
    "        return y_ah, y_ai\n",
    "\n",
    "    def train_models(self):\n",
    "        print(\"\\n🎓 Training models...\")\n",
    "        X1 = self._prepare_X(self.train_data, self.model1_features, fit=True)\n",
    "        X2 = self._prepare_X(self.train_data, self.model2_features, fit=False)\n",
    "        y_ah, y_ai = self.prepare_targets(self.train_data, fit_encoders=True)\n",
    "        print(f\"📊 Training shapes: X1={X1.shape}, X2={X2.shape}\")\n",
    "        print(f\"   AH classes: {len(np.unique(y_ah))}, AI classes: {len(np.unique(y_ai))}\")\n",
    "\n",
    "        X1_tr, X1_val, yah_tr, yah_val = train_test_split(X1, y_ah, test_size=0.2, random_state=42, stratify=y_ah)\n",
    "        _,   _,    yai_tr, yai_val = train_test_split(X1, y_ai, test_size=0.2, random_state=42, stratify=y_ai)\n",
    "        X2_tr, X2_val, _, _ = train_test_split(X2, y_ah, test_size=0.2, random_state=42, stratify=y_ah)\n",
    "\n",
    "        self.model1_ah.fit(X1_tr, yah_tr); self.model1_ai.fit(X1_tr, yai_tr)\n",
    "        self.model2_ah.fit(X2_tr, yah_tr); self.model2_ai.fit(X2_tr, yai_tr)\n",
    "        print(\"✅ Models trained\")\n",
    "\n",
    "        print(\"\\n📊 Validation Performance (model-only):\")\n",
    "        y1_ah_pred = self.model1_ah.predict(X1_val)\n",
    "        y1_ai_pred = self.model1_ai.predict(X1_val)\n",
    "        y2_ah_pred = self.model2_ah.predict(X2_val)\n",
    "        y2_ai_pred = self.model2_ai.predict(X2_val)\n",
    "        print(f\"\\n🔸 Model 1 AH Accuracy: {accuracy_score(yah_val, y1_ah_pred):.3f}\")\n",
    "        print(f\"🔸 Model 1 AI Accuracy: {accuracy_score(yai_val, y1_ai_pred):.3f}\")\n",
    "        print(f\"🔸 Model 2 AH Accuracy: {accuracy_score(yah_val, y2_ah_pred):.3f}\")\n",
    "        print(f\"🔸 Model 2 AI Accuracy: {accuracy_score(yai_val, y2_ai_pred):.3f}\")\n",
    "\n",
    "        self._create_confusion_matrices(yah_val, yai_val, y1_ah_pred, y1_ai_pred, y2_ah_pred, y2_ai_pred)\n",
    "\n",
    "        print(\"\\n📈 Cross-validation (5-fold) on full training set:\")\n",
    "        for name, mdl, Xd, yd in [\n",
    "            ('Model 1 AH', self.model1_ah, X1, y_ah),\n",
    "            ('Model 1 AI', self.model1_ai, X1, y_ai),\n",
    "            ('Model 2 AH', self.model2_ah, X2, y_ah),\n",
    "            ('Model 2 AI', self.model2_ai, X2, y_ai),\n",
    "        ]:\n",
    "            scores = cross_val_score(mdl, Xd, yd, cv=5)\n",
    "            print(f\"   {name}: {scores.mean():.3f} ± {scores.std():.3f}\")\n",
    "\n",
    "        self.model1_ah.fit(X1, y_ah); self.model1_ai.fit(X1, y_ai)\n",
    "        self.model2_ah.fit(X2, y_ah); self.model2_ai.fit(X2, y_ai)\n",
    "\n",
    "    def _create_confusion_matrices(self, y_ah_true, y_ai_true, y_ah_pred1, y_ai_pred1, y_ah_pred2, y_ai_pred2):\n",
    "        print(\"\\n📊 Creating confusion matrices...\")\n",
    "        try:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "            ah_labels = self.target_encoders['AH'].classes_\n",
    "            ai_labels = self.target_encoders['AI'].classes_\n",
    "\n",
    "            cm1_ah = confusion_matrix(y_ah_true, y_ah_pred1)\n",
    "            sns.heatmap(cm1_ah, annot=True, fmt='d', cmap='Blues',\n",
    "                        xticklabels=ah_labels, yticklabels=ah_labels, ax=axes[0,0])\n",
    "            axes[0,0].set_title('Model 1 - AH (A–AG)', fontweight='bold')\n",
    "\n",
    "            cm1_ai = confusion_matrix(y_ai_true, y_ai_pred1)\n",
    "            sns.heatmap(cm1_ai, annot=True, fmt='d', cmap='Reds',\n",
    "                        xticklabels=[l[:15]+'...' if len(l)>15 else l for l in ai_labels],\n",
    "                        yticklabels=[l[:15]+'...' if len(l)>15 else l for l in ai_labels], ax=axes[0,1])\n",
    "            axes[0,1].set_title('Model 1 - AI (A–AG)', fontweight='bold'); axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "            cm2_ah = confusion_matrix(y_ah_true, y_ah_pred2)\n",
    "            sns.heatmap(cm2_ah, annot=True, fmt='d', cmap='Greens',\n",
    "                        xticklabels=ah_labels, yticklabels=ah_labels, ax=axes[1,0])\n",
    "            axes[1,0].set_title('Model 2 - AH (A–T)', fontweight='bold')\n",
    "\n",
    "            cm2_ai = confusion_matrix(y_ai_true, y_ai_pred2)\n",
    "            sns.heatmap(cm2_ai, annot=True, fmt='d', cmap='Purples',\n",
    "                        xticklabels=[l[:15]+'...' if len(l)>15 else l for l in ai_labels],\n",
    "                        yticklabels=[l[:15]+'...' if len(l)>15 else l for l in ai_labels], ax=axes[1,1])\n",
    "            axes[1,1].set_title('Model 2 - AI (A–T)', fontweight='bold'); axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('confusion_matrices_validation.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "            plt.close()\n",
    "            print(\"   ✅ Saved: confusion_matrices_validation.png\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Confusion matrix creation warning: {e}\")\n",
    "\n",
    "    def analyze_feature_importance(self):\n",
    "        print(\"\\n🔍 SHAP explainers (TreeExplainer for RF)...\")\n",
    "        try:\n",
    "            X1 = self._prepare_X(self.train_data, self.model1_features, fit=False)\n",
    "            X2 = self._prepare_X(self.train_data, self.model2_features, fit=False)\n",
    "            self.explainers['model1_ah'] = shap.TreeExplainer(self.model1_ah)\n",
    "            self.explainers['model1_ai'] = shap.TreeExplainer(self.model1_ai)\n",
    "            self.explainers['model2_ah'] = shap.TreeExplainer(self.model2_ah)\n",
    "            self.explainers['model2_ai'] = shap.TreeExplainer(self.model2_ai)\n",
    "\n",
    "            def shap_summary(mdl_key, X, title, out_png):\n",
    "                expl = self.explainers[mdl_key]\n",
    "                sv = expl.shap_values(X)\n",
    "                if isinstance(sv, list):\n",
    "                    sv_to_plot = sv[0]\n",
    "                else:\n",
    "                    sv_to_plot = sv\n",
    "                plt.figure(figsize=(12,8))\n",
    "                shap.summary_plot(sv_to_plot, X, show=False, max_display=20)\n",
    "                plt.title(title, fontsize=14, fontweight='bold')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(out_png, dpi=300, bbox_inches='tight'); plt.close()\n",
    "                print(f\"   ✅ {out_png}\")\n",
    "\n",
    "            shap_summary('model1_ah', X1, 'SHAP Summary - Model 1 (AH)', 'shap_model1_ah_summary.png')\n",
    "            shap_summary('model1_ai', X1, 'SHAP Summary - Model 1 (AI)', 'shap_model1_ai_summary.png')\n",
    "            shap_summary('model2_ah', X2, 'SHAP Summary - Model 2 (AH)', 'shap_model2_ah_summary.png')\n",
    "            shap_summary('model2_ai', X2, 'SHAP Summary - Model 2 (AI)', 'shap_model2_ai_summary.png')\n",
    "\n",
    "            fig, axes = plt.subplots(2,2, figsize=(16,12))\n",
    "            def top_imp(ax, model, feat_names, title):\n",
    "                imp = pd.Series(model.feature_importances_, index=feat_names).sort_values(ascending=False).head(15)\n",
    "                ax.barh(imp.index[::-1], imp.values[::-1]); ax.set_title(title, fontweight='bold'); ax.set_xlabel('Importance')\n",
    "            top_imp(axes[0,0], self.model1_ah, X1.columns, 'Model 1 - AH (Top 15)')\n",
    "            top_imp(axes[0,1], self.model1_ai, X1.columns, 'Model 1 - AI (Top 15)')\n",
    "            top_imp(axes[1,0], self.model2_ah, X2.columns, 'Model 2 - AH (Top 15)')\n",
    "            top_imp(axes[1,1], self.model2_ai, X2.columns, 'Model 2 - AI (Top 15)')\n",
    "            plt.tight_layout(); plt.savefig('feature_importance_comparison.png', dpi=300, bbox_inches='tight'); plt.close()\n",
    "            print(\"   ✅ feature_importance_comparison.png\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ SHAP/importance warning: {e}\")\n",
    "\n",
    "    def _rule_first_predict_ai(self, row: pd.Series) -> Optional[str]:\n",
    "        tmp = self._compute_engineered(row.to_frame().T).iloc[0]\n",
    "        if tmp['_rule_wac18_immunoDerm_qty1'] == 1:\n",
    "            return 'Write off due to WAC UNIT PRICE diff from WAC Medispan'\n",
    "        if tmp['_rule_fixed_slow_q14_qty1'] == 1:\n",
    "            return 'Sent to collection due to MFG timing difference'\n",
    "        if (tmp['_is_cardio_or_diab']==1) and (tmp['_disc_lt3']==1) and (tmp['_est_lt_1200']==1):\n",
    "            sloppy_npies = {1043382302,1063510097,1234567890,1567890123}\n",
    "            if row['NPI PHARMACY NPI'] in sloppy_npies:\n",
    "                return 'Write off due to payment applied to the wrong claim'\n",
    "        return None\n",
    "\n",
    "    def _rule_first_predict_ah(self, ai_label: str) -> Optional[str]:\n",
    "        mapping = {\n",
    "            'Write off due to manual data entry error': 'N907',\n",
    "            'Write off due to payment applied to the wrong claim': 'N908',\n",
    "            'Write off due to WAC UNIT PRICE diff from WAC Medispan': 'N910',\n",
    "            'Write off due to duplicate payment': 'N911',\n",
    "            'Sent to collection due to MFG timing difference': 'N910'\n",
    "        }\n",
    "        return mapping.get(ai_label)\n",
    "\n",
    "    def apply_predictions(self):\n",
    "        print(\"\\n🔮 Applying predictions (rule-first)...\")\n",
    "        X2 = self._prepare_X(self.step2_data, self.model1_features, fit=False)\n",
    "        pred_ah_2 = self.model1_ah.predict(X2)\n",
    "        pred_ai_2 = self.model1_ai.predict(X2)\n",
    "        ah_labels_2 = self.target_encoders['AH'].inverse_transform(pred_ah_2)\n",
    "        ai_labels_2 = self.target_encoders['AI'].inverse_transform(pred_ai_2)\n",
    "        for i, idx in enumerate(self.step2_data.index):\n",
    "            row = self.df.loc[idx]\n",
    "            ai_rule = self._rule_first_predict_ai(row)\n",
    "            if ai_rule is not None:\n",
    "                ai_labels_2[i] = ai_rule\n",
    "                ah_labels_2[i] = self._rule_first_predict_ah(ai_rule)\n",
    "        self.df.loc[self.step2_data.index, '835 report Qualifier Code / RARC codes'] = ah_labels_2\n",
    "        self.df.loc[self.step2_data.index, 'Expected Outcomes Error category'] = ai_labels_2\n",
    "        self.df.loc[self.step2_data.index, ' Questions/comments'] = [\n",
    "            f\"ML/Rule Prediction: AH='{ah_labels_2[i]}', AI='{ai_labels_2[i]}'\" for i in range(len(ah_labels_2))\n",
    "        ]\n",
    "        print(f\"✅ Updated {len(self.step2_data)} Step 2 rows\")\n",
    "\n",
    "        X3 = self._prepare_X(self.step3_data, self.model2_features, fit=False)\n",
    "        pred_ah_3 = self.model2_ah.predict(X3)\n",
    "        pred_ai_3 = self.model2_ai.predict(X3)\n",
    "        ah_labels_3 = self.target_encoders['AH'].inverse_transform(pred_ah_3)\n",
    "        ai_labels_3 = self.target_encoders['AI'].inverse_transform(pred_ai_3)\n",
    "        for i, idx in enumerate(self.step3_data.index):\n",
    "            row = self.df.loc[idx]\n",
    "            ai_rule = self._rule_first_predict_ai(row)\n",
    "            if ai_rule is not None:\n",
    "                ai_labels_3[i] = ai_rule\n",
    "                ah_labels_3[i] = self._rule_first_predict_ah(ai_rule)\n",
    "        self.df.loc[self.step3_data.index, '835 report Qualifier Code / RARC codes'] = ah_labels_3\n",
    "        self.df.loc[self.step3_data.index, 'Expected Outcomes Error category'] = ai_labels_3\n",
    "        self.df.loc[self.step3_data.index, ' Questions/comments'] = [\n",
    "            f\"ML/Rule Prediction: AH='{ah_labels_3[i]}', AI='{ai_labels_3[i]}'\" for i in range(len(ah_labels_3))\n",
    "        ]\n",
    "        print(f\"✅ Updated {len(self.step3_data)} Step 3 rows\")\n",
    "\n",
    "        def cnt(arr): \n",
    "            u,c = np.unique(arr, return_counts=True); return dict(zip(u,c))\n",
    "        print(\"\\n📊 Prediction Summary:\")\n",
    "        print(f\"Step 2 AH: {cnt(ah_labels_2)}\")\n",
    "        print(f\"Step 2 AI: {cnt(ai_labels_2)}\")\n",
    "        print(f\"Step 3 AH: {cnt(ah_labels_3)}\")\n",
    "        print(f\"Step 3 AI: {cnt(ai_labels_3)}\")\n",
    "\n",
    "    def save_results(self, output_file='Pharma_poc_ml_results.xlsx'):\n",
    "        print(f\"\\n💾 Saving results to {output_file}...\")\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "            self.df.to_excel(writer, sheet_name='Scenario Data Structure', index=False)\n",
    "            worksheet = writer.sheets['Scenario Data Structure']\n",
    "            for column in worksheet.columns:\n",
    "                max_length = 0\n",
    "                column_letter = column[0].column_letter\n",
    "                for cell in column:\n",
    "                    try:\n",
    "                        if len(str(cell.value)) > max_length:\n",
    "                            max_length = len(str(cell.value))\n",
    "                    except:\n",
    "                        pass\n",
    "                adjusted_width = min(max_length + 2, 100)\n",
    "                worksheet.column_dimensions[column_letter].width = adjusted_width\n",
    "        print(\"✅ Results saved\")\n",
    "        print(\"\\n📊 Artifacts:\")\n",
    "        print(\"   📈 shap_model1_ah_summary.png\")\n",
    "        print(\"   📈 shap_model1_ai_summary.png\")\n",
    "        print(\"   📈 shap_model2_ah_summary.png\")\n",
    "        print(\"   📈 shap_model2_ai_summary.png\")\n",
    "        print(\"   📊 feature_importance_comparison.png\")\n",
    "        print(\"   🎯 confusion_matrices_validation.png\")\n",
    "        return output_file\n",
    "\n",
    "    def run_full_pipeline(self, output_file='Pharma_poc_ml_results.xlsx'):\n",
    "        print(\"🚀 STARTING Pharma PoC ML PIPELINE\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"📋 Steps:\")\n",
    "        print(\"  1) Load data\")\n",
    "        print(\"  2) Identify significant features\")\n",
    "        print(\"  3) Train models\")\n",
    "        print(\"  4) SHAP & feature importance\")\n",
    "        print(\"  5) Apply predictions with rule-first overrides\")\n",
    "        print(\"  6) Save results\")\n",
    "        print(\"=\"*60)\n",
    "        self.load_and_prepare_data()\n",
    "        self.identify_significant_features()\n",
    "        self.train_models()\n",
    "        self.analyze_feature_importance()\n",
    "        self.apply_predictions()\n",
    "        result_file = self.save_results(output_file)\n",
    "        print(\"\\n🎉 PIPELINE COMPLETED\")\n",
    "        print(f\"📁 Results saved to: {result_file}\")\n",
    "        return result_file\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4) compare datasets — comparison of \"original vs improved\"\n",
    "# ------------------------------------------------\n",
    "def _compute_engineered_for_compare(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # repeats logic from pipeline (without class)\n",
    "    def month_from_ddmmyyyy(s: str) -> int:\n",
    "        try: return int(str(s).split('/')[1])\n",
    "        except: return 1\n",
    "    m = df['SERV DATE DATE OF SERVICE'].astype(str).map(month_from_ddmmyyyy)\n",
    "    q = ((m-1)//3 + 1).astype(int)\n",
    "    is_wac = (df['MFP RULE PRC POINT MFP Rule Price Point'].astype(str) == 'WAC')\n",
    "    disc = pd.to_numeric(df['MFP RULE DISC MFP Rule Disc %'], errors='coerce').fillna(0.0)\n",
    "    has_fixed = (df['MFP RULE UNIT PRICE MFP Rule Unit Price'].astype(str)!='')\n",
    "    qty1 = (pd.to_numeric(df['QTY DISP QUANTITY DISPENSED'], errors='coerce').fillna(0)==1)\n",
    "    est_amt = pd.to_numeric(df['EST REIMB AMT Estimated MFG Reimbursement Amt'], errors='coerce').fillna(0.0)\n",
    "    wac_unit = pd.to_numeric(df['WAC UNIT PRICE WAC UNIT PRICE'], errors='coerce').fillna(1.0)\n",
    "    mfp_unit = pd.to_numeric(df['MFP UNIT PRICE MFP UNIT PRICE'], errors='coerce').fillna(1.0)\n",
    "    qty = pd.to_numeric(df['QTY DISP QUANTITY DISPENSED'], errors='coerce').fillna(1.0)\n",
    "    immuno_derm_set = {'Immunosuppressant','Dermatology'}\n",
    "    cardio_diab_set = {'Cardiovascular','Diabetes'}\n",
    "    slow_mfg_set = {'JANSSEN BIOTECH','AMGEN/ IMMUNEX','BRISTOL MYERS SQUIBB'}\n",
    "    def infer_class(drug: str) -> str:\n",
    "        drug = str(drug)\n",
    "        if any(k in drug for k in ['STELARA','ENBREL','HUMIRA']): return 'Immunosuppressant'\n",
    "        if any(k in drug for k in ['DUPIXENT','SKYRIZI']): return 'Dermatology'\n",
    "        if 'ENTRESTO' in drug: return 'Cardiovascular'\n",
    "        if any(k in drug for k in ['OZEMPIC','FIASP']): return 'Diabetes'\n",
    "        return 'General'\n",
    "    th_cls = df['DRUG NAME N/A'].astype(str).map(infer_class)\n",
    "    is_immuno_or_derm = th_cls.isin(immuno_derm_set)\n",
    "    is_cardio_or_diab = th_cls.isin(cardio_diab_set)\n",
    "    is_slow_mfg = df['MFG NAME MFG Name'].isin(slow_mfg_set)\n",
    "    engineered = pd.DataFrame({\n",
    "        '_is_wac_rule': is_wac.astype(int),\n",
    "        '_disc_is_high18': (disc>=18.0).astype(int),\n",
    "        '_qty_is_1': qty1.astype(int),\n",
    "        '_has_fixed_unit_price': has_fixed.astype(int),\n",
    "        '_q_in_14': q.isin([1,4]).astype(int),\n",
    "        '_is_slow_mfg': is_slow_mfg.astype(int),\n",
    "        '_is_immuno_or_derm': is_immuno_or_derm.astype(int),\n",
    "        '_is_cardio_or_diab': is_cardio_or_diab.astype(int),\n",
    "        '_est_lt_1200': (est_amt<1200.0).astype(int),\n",
    "        '_disc_lt3': (disc<3.0).astype(int),\n",
    "        '_est_vs_wac_amt': (est_amt / (wac_unit*qty + 1e-9)).values\n",
    "    }, index=df.index)\n",
    "    engineered['_rule_wac18_immunoDerm_qty1'] = (\n",
    "        engineered['_is_wac_rule'].eq(1) &\n",
    "        engineered['_disc_is_high18'].eq(1) &\n",
    "        engineered['_is_immuno_or_derm'].eq(1) &\n",
    "        engineered['_qty_is_1'].eq(1)\n",
    "    ).astype(int)\n",
    "    engineered['_rule_fixed_slow_q14_qty1'] = (\n",
    "        engineered['_has_fixed_unit_price'].eq(1) &\n",
    "        engineered['_is_slow_mfg'].eq(1) &\n",
    "        engineered['_q_in_14'].eq(1) &\n",
    "        engineered['_qty_is_1'].eq(1)\n",
    "    ).astype(int)\n",
    "    return engineered\n",
    "\n",
    "def _dataset_quick_stats(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    out = {}\n",
    "    out['rows'] = len(df)\n",
    "    out['case_distribution'] = df['Case'].value_counts().to_dict()\n",
    "    out['ah_distribution'] = df['835 report Qualifier Code / RARC codes'].value_counts().to_dict()\n",
    "    out['ai_distribution'] = df['Expected Outcomes Error category'].value_counts().to_dict()\n",
    "    filled = df[df['Case']!='Step 3 - Forecast']\n",
    "    if len(filled)>0:\n",
    "        est = pd.to_numeric(filled['EST REIMB AMT Estimated MFG Reimbursement Amt'], errors='coerce')\n",
    "        act = pd.to_numeric(filled['835 report Actual Payment Amount'], errors='coerce')\n",
    "        valid = (~est.isna()) & (~act.isna())\n",
    "        if valid.sum()>0:\n",
    "            under = ((est[valid]-act[valid])/est[valid]*100)\n",
    "            out['underpayment_avg_pct'] = round(float(under.mean()),2)\n",
    "            out['underpayment_minmax_pct'] = [round(float(under.min()),2), round(float(under.max()),2)]\n",
    "    eng = _compute_engineered_for_compare(df)\n",
    "    out['rule_hit_pct_wac18_immunoDerm_qty1'] = round(100*eng['_rule_wac18_immunoDerm_qty1'].mean(),1)\n",
    "    out['rule_hit_pct_fixed_slow_q14_qty1'] = round(100*eng['_rule_fixed_slow_q14_qty1'].mean(),1)\n",
    "    # rough estimate of the \"balance\" of AI classes\n",
    "    if out['ai_distribution']:\n",
    "        counts = np.array(list(out['ai_distribution'].values()), dtype=float)\n",
    "        out['ai_balance_cv'] = round(float(counts.std()/ (counts.mean()+1e-9)),3)\n",
    "    return out\n",
    "\n",
    "def compare_datasets(original_file: str, enhanced_file: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Compares 2 Excel files with the same format ('Scenario Data Structure' sheet by default).\n",
    "    Returns a dictionary with metrics and writes comparison_report.xlsx\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_orig = pd.read_excel(original_file)\n",
    "        df_new = pd.read_excel(enhanced_file)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ compare_datasets: can't read files: {e}\")\n",
    "        return {}\n",
    "    stats_orig = _dataset_quick_stats(df_orig)\n",
    "    stats_new  = _dataset_quick_stats(df_new)\n",
    "\n",
    "    # сводка\n",
    "    summary = {\n",
    "        'rows_diff': stats_new['rows'] - stats_orig['rows'],\n",
    "        'ai_classes_orig': list(stats_orig['ai_distribution'].keys()),\n",
    "        'ai_classes_new': list(stats_new['ai_distribution'].keys()),\n",
    "        'rule_hit_wac18_change_pp': round(stats_new['rule_hit_pct_wac18_immunoDerm_qty1'] - stats_orig['rule_hit_pct_wac18_immunoDerm_qty1'], 1),\n",
    "        'rule_hit_fixed_slow_change_pp': round(stats_new['rule_hit_pct_fixed_slow_q14_qty1'] - stats_orig['rule_hit_pct_fixed_slow_q14_qty1'], 1),\n",
    "        'ai_balance_cv_change': round(stats_new.get('ai_balance_cv',0) - stats_orig.get('ai_balance_cv',0), 3),\n",
    "        'underpayment_avg_change_pp': round(stats_new.get('underpayment_avg_pct',0) - stats_orig.get('underpayment_avg_pct',0), 2),\n",
    "    }\n",
    "    result = {'summary': summary, 'original': stats_orig, 'enhanced': stats_new}\n",
    "\n",
    "    # repack в Excel-отчёт\n",
    "    try:\n",
    "        with pd.ExcelWriter('comparison_report.xlsx', engine='openpyxl') as writer:\n",
    "            pd.DataFrame([stats_orig]).to_excel(writer, sheet_name='original', index=False)\n",
    "            pd.DataFrame([stats_new]).to_excel(writer, sheet_name='enhanced', index=False)\n",
    "            # частоты классов\n",
    "            ai_df = pd.DataFrame({'original': stats_orig['ai_distribution']}).join(\n",
    "                    pd.DataFrame({'enhanced': stats_new['ai_distribution']}), how='outer').fillna(0).astype(int)\n",
    "            ai_df.to_excel(writer, sheet_name='ai_distribution', index=True)\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ failed to write comparison_report.xlsx:\", e)\n",
    "\n",
    "    print(\"✅ compare_datasets: done. See 'comparison_report.xlsx'\")\n",
    "    return result\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 5) Generation Strategies Benchmark + Fast ML Assessment\n",
    "# ------------------------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def _quick_eval_rf(df: pd.DataFrame, random_state: int = 42) -> Dict[str,float]:\n",
    "    \"\"\"\n",
    "    Быстрая метрика: берём Step1 для train/val (80/20), строим признаки как в пайплайне,\n",
    "    RF для AH/AI, возвращаем hold-out accuracy.\n",
    "    \"\"\"\n",
    "    train_df = df[df['Case']=='Step 1 - Train'].copy()\n",
    "    if len(train_df) < 100:\n",
    "        return {'ah_acc': np.nan, 'ai_acc': np.nan}\n",
    "    pipe = PharmaPoCMLPipeline(input_file=None)\n",
    "    pipe.df = train_df\n",
    "    pipe.train_data = train_df\n",
    "    pipe.step2_data = pd.DataFrame(columns=train_df.columns)\n",
    "    pipe.step3_data = pd.DataFrame(columns=train_df.columns)\n",
    "    pipe.identify_significant_features()\n",
    "\n",
    "    X = pipe._prepare_X(train_df, pipe.model1_features, fit=True)\n",
    "    y_ah, y_ai = pipe.prepare_targets(train_df, fit_encoders=True)\n",
    "    X_tr, X_va, yah_tr, yah_va = train_test_split(X, y_ah, test_size=0.2, random_state=random_state, stratify=y_ah)\n",
    "    _,    _, yai_tr, yai_va = train_test_split(X, y_ai, test_size=0.2, random_state=random_state, stratify=y_ai)\n",
    "    mdl_ah = RandomForestClassifier(n_estimators=300, random_state=random_state).fit(X_tr, yah_tr)\n",
    "    mdl_ai = RandomForestClassifier(n_estimators=300, random_state=random_state, class_weight='balanced').fit(X_tr, yai_tr)\n",
    "    ah_acc = accuracy_score(yah_va, mdl_ah.predict(X_va))\n",
    "    ai_acc = accuracy_score(yai_va, mdl_ai.predict(X_va))\n",
    "    return {'ah_acc': round(float(ah_acc),3), 'ai_acc': round(float(ai_acc),3)}\n",
    "\n",
    "def generate_ml_benchmark_datasets(seed: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generates 3 sets (baseline/medium/optimized), saves XLSX/CSV, gives a quick RF estimate.\n",
    "    Returns a DataFrame with the results and writes 'ml_benchmark_results.csv'.\n",
    "    \"\"\"\n",
    "    configs = [\n",
    "        ('baseline', 0.6, 0.03),\n",
    "        ('medium',   0.85, 0.015),\n",
    "        ('optimized',1.00, 0.005),\n",
    "    ]\n",
    "    rows = []\n",
    "    for name, sig, noise in configs:\n",
    "        gen = EnhancedPharmaDataGenerator(seed=seed, signal_strength=sig, noise_level=noise)\n",
    "        df = gen.generate_etalon_based_dataset(350, 350, 350)\n",
    "        stats = gen.quality_check(df)\n",
    "        xlsx = f'Pharma_poc_benchmark_{name}.xlsx'\n",
    "        csv  = f'Pharma_poc_benchmark_{name}.csv'\n",
    "        gen.save_to_excel(df, xlsx); gen.save_to_csv(df, csv)\n",
    "        eval_metrics = _quick_eval_rf(df, random_state=seed)\n",
    "        rows.append({\n",
    "            'name': name,\n",
    "            'signal_strength': sig,\n",
    "            'noise_level': noise,\n",
    "            'rows': len(df),\n",
    "            'quality_score': stats.get('quality_score'),\n",
    "            'rule_wac18_immunoDerm_qty1_%': stats.get('pricing_stats',{}).get('wac_rules_pct', np.nan),  # просто как справка\n",
    "            'ah_acc_val': eval_metrics['ah_acc'],\n",
    "            'ai_acc_val': eval_metrics['ai_acc'],\n",
    "            'xlsx': xlsx, 'csv': csv\n",
    "        })\n",
    "        print(f\"[Benchmark] {name}: AH={eval_metrics['ah_acc']}, AI={eval_metrics['ai_acc']} -> {xlsx}\")\n",
    "\n",
    "    res = pd.DataFrame(rows)\n",
    "    try:\n",
    "        res.to_csv('ml_benchmark_results.csv', index=False)\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ failed to write ml_benchmark_results.csv:\", e)\n",
    "    return res\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 6) Утилиты запуска (как раньше)\n",
    "# ------------------------------------------------\n",
    "def run_Pharma_ml_analysis(input_file='Pharma_poc_custom_1000.xlsx', output_file='Pharma_poc_ml_results.xlsx'):\n",
    "    pipeline = PharmaPoCMLPipeline(input_file)\n",
    "    result_file = pipeline.run_full_pipeline(output_file)\n",
    "    return result_file\n",
    "\n",
    "def create_custom_waterfall_plots(input_file, row_indices, output_dir='custom_waterfalls'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(\"ℹ️ Per-row waterfalls are omitted in this slim version (global SHAP saved).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6988bd05-709d-4164-886c-c6e6790c27cf",
   "metadata": {},
   "source": [
    "####  Run Synthetic data generaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f5f0783-53be-4c96-8be3-1f00e38b1cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Custom Generation] 500+250+250 = 1000 records\n",
      "Files created:\n",
      " - Pharma_poc_custom_1000_optimized.xlsx\n",
      " - Pharma_poc_custom_1000_optimized.csv\n",
      "🚀 Pharma PoC ML Pipeline initialized (rule-first)\n",
      "\n",
      "🔍 Identifying significant features...\n",
      "📊 Model 1 base features (17):\n",
      "   1. NPI PHARMACY NPI\n",
      "   2. SERV DATE DATE OF SERVICE\n",
      "   3. PROD SVC ID PRODUCT/SERVICE ID\n",
      "   4. DRUG NAME N/A\n",
      "   5. QTY DISP QUANTITY DISPENSED\n",
      "   6. WAC UNIT PRICE WAC UNIT PRICE\n",
      "   7. MFP UNIT PRICE MFP UNIT PRICE\n",
      "   8. EST REIMB AMT Estimated MFG Reimbursement Amt\n",
      "   9. MFP RULE PRC POINT MFP Rule Price Point\n",
      "  10. MFP RULE DISC MFP Rule Disc %\n",
      "  11. MFP RULE UNIT PRICE MFP Rule Unit Price\n",
      "  12. MFG NAME MFG Name\n",
      "  13. MFG CUST ID MFG Customer ID\n",
      "  14. Medispan/FDB MFP Price\n",
      "  15. 835 report Actual Payment Amount\n",
      "  16. 835 report Adjudicated Procedure Code (Product/Service ID)\n",
      "  17. 835 report Adjustment Code / CARC codes\n",
      "\n",
      "📊 Model 2 base features (14):\n",
      "   1. NPI PHARMACY NPI\n",
      "   2. SERV DATE DATE OF SERVICE\n",
      "   3. PROD SVC ID PRODUCT/SERVICE ID\n",
      "   4. DRUG NAME N/A\n",
      "   5. QTY DISP QUANTITY DISPENSED\n",
      "   6. WAC UNIT PRICE WAC UNIT PRICE\n",
      "   7. MFP UNIT PRICE MFP UNIT PRICE\n",
      "   8. EST REIMB AMT Estimated MFG Reimbursement Amt\n",
      "   9. MFP RULE PRC POINT MFP Rule Price Point\n",
      "  10. MFP RULE DISC MFP Rule Disc %\n",
      "  11. MFP RULE UNIT PRICE MFP Rule Unit Price\n",
      "  12. MFG NAME MFG Name\n",
      "  13. MFG CUST ID MFG Customer ID\n",
      "  14. Medispan/FDB MFP Price\n",
      "\n",
      "➕ Engineered features to be added at transform-time:\n",
      "   _month, _quarter, _is_wac_rule, _has_fixed_unit_price, _disc_is_high18, _disc_lt3, _qty_is_1, _est_lt_1200, _q_in_14, _is_slow_mfg, _is_immuno_or_derm, _is_cardio_or_diab, _est_vs_wac_amt, _mfp_vs_wac_unit, _rule_wac18_immunoDerm_qty1, _rule_fixed_slow_q14_qty1\n",
      "[Benchmark] baseline: AH=1.0, AI=0.182 -> Pharma_poc_benchmark_baseline.xlsx\n",
      "🚀 Pharma PoC ML Pipeline initialized (rule-first)\n",
      "\n",
      "🔍 Identifying significant features...\n",
      "📊 Model 1 base features (17):\n",
      "   1. NPI PHARMACY NPI\n",
      "   2. SERV DATE DATE OF SERVICE\n",
      "   3. PROD SVC ID PRODUCT/SERVICE ID\n",
      "   4. DRUG NAME N/A\n",
      "   5. QTY DISP QUANTITY DISPENSED\n",
      "   6. WAC UNIT PRICE WAC UNIT PRICE\n",
      "   7. MFP UNIT PRICE MFP UNIT PRICE\n",
      "   8. EST REIMB AMT Estimated MFG Reimbursement Amt\n",
      "   9. MFP RULE PRC POINT MFP Rule Price Point\n",
      "  10. MFP RULE DISC MFP Rule Disc %\n",
      "  11. MFP RULE UNIT PRICE MFP Rule Unit Price\n",
      "  12. MFG NAME MFG Name\n",
      "  13. MFG CUST ID MFG Customer ID\n",
      "  14. Medispan/FDB MFP Price\n",
      "  15. 835 report Actual Payment Amount\n",
      "  16. 835 report Adjudicated Procedure Code (Product/Service ID)\n",
      "  17. 835 report Adjustment Code / CARC codes\n",
      "\n",
      "📊 Model 2 base features (14):\n",
      "   1. NPI PHARMACY NPI\n",
      "   2. SERV DATE DATE OF SERVICE\n",
      "   3. PROD SVC ID PRODUCT/SERVICE ID\n",
      "   4. DRUG NAME N/A\n",
      "   5. QTY DISP QUANTITY DISPENSED\n",
      "   6. WAC UNIT PRICE WAC UNIT PRICE\n",
      "   7. MFP UNIT PRICE MFP UNIT PRICE\n",
      "   8. EST REIMB AMT Estimated MFG Reimbursement Amt\n",
      "   9. MFP RULE PRC POINT MFP Rule Price Point\n",
      "  10. MFP RULE DISC MFP Rule Disc %\n",
      "  11. MFP RULE UNIT PRICE MFP Rule Unit Price\n",
      "  12. MFG NAME MFG Name\n",
      "  13. MFG CUST ID MFG Customer ID\n",
      "  14. Medispan/FDB MFP Price\n",
      "\n",
      "➕ Engineered features to be added at transform-time:\n",
      "   _month, _quarter, _is_wac_rule, _has_fixed_unit_price, _disc_is_high18, _disc_lt3, _qty_is_1, _est_lt_1200, _q_in_14, _is_slow_mfg, _is_immuno_or_derm, _is_cardio_or_diab, _est_vs_wac_amt, _mfp_vs_wac_unit, _rule_wac18_immunoDerm_qty1, _rule_fixed_slow_q14_qty1\n",
      "[Benchmark] medium: AH=1.0, AI=0.182 -> Pharma_poc_benchmark_medium.xlsx\n",
      "🚀 Pharma PoC ML Pipeline initialized (rule-first)\n",
      "\n",
      "🔍 Identifying significant features...\n",
      "📊 Model 1 base features (17):\n",
      "   1. NPI PHARMACY NPI\n",
      "   2. SERV DATE DATE OF SERVICE\n",
      "   3. PROD SVC ID PRODUCT/SERVICE ID\n",
      "   4. DRUG NAME N/A\n",
      "   5. QTY DISP QUANTITY DISPENSED\n",
      "   6. WAC UNIT PRICE WAC UNIT PRICE\n",
      "   7. MFP UNIT PRICE MFP UNIT PRICE\n",
      "   8. EST REIMB AMT Estimated MFG Reimbursement Amt\n",
      "   9. MFP RULE PRC POINT MFP Rule Price Point\n",
      "  10. MFP RULE DISC MFP Rule Disc %\n",
      "  11. MFP RULE UNIT PRICE MFP Rule Unit Price\n",
      "  12. MFG NAME MFG Name\n",
      "  13. MFG CUST ID MFG Customer ID\n",
      "  14. Medispan/FDB MFP Price\n",
      "  15. 835 report Actual Payment Amount\n",
      "  16. 835 report Adjudicated Procedure Code (Product/Service ID)\n",
      "  17. 835 report Adjustment Code / CARC codes\n",
      "\n",
      "📊 Model 2 base features (14):\n",
      "   1. NPI PHARMACY NPI\n",
      "   2. SERV DATE DATE OF SERVICE\n",
      "   3. PROD SVC ID PRODUCT/SERVICE ID\n",
      "   4. DRUG NAME N/A\n",
      "   5. QTY DISP QUANTITY DISPENSED\n",
      "   6. WAC UNIT PRICE WAC UNIT PRICE\n",
      "   7. MFP UNIT PRICE MFP UNIT PRICE\n",
      "   8. EST REIMB AMT Estimated MFG Reimbursement Amt\n",
      "   9. MFP RULE PRC POINT MFP Rule Price Point\n",
      "  10. MFP RULE DISC MFP Rule Disc %\n",
      "  11. MFP RULE UNIT PRICE MFP Rule Unit Price\n",
      "  12. MFG NAME MFG Name\n",
      "  13. MFG CUST ID MFG Customer ID\n",
      "  14. Medispan/FDB MFP Price\n",
      "\n",
      "➕ Engineered features to be added at transform-time:\n",
      "   _month, _quarter, _is_wac_rule, _has_fixed_unit_price, _disc_is_high18, _disc_lt3, _qty_is_1, _est_lt_1200, _q_in_14, _is_slow_mfg, _is_immuno_or_derm, _is_cardio_or_diab, _est_vs_wac_amt, _mfp_vs_wac_unit, _rule_wac18_immunoDerm_qty1, _rule_fixed_slow_q14_qty1\n",
      "[Benchmark] optimized: AH=1.0, AI=0.169 -> Pharma_poc_benchmark_optimized.xlsx\n"
     ]
    }
   ],
   "source": [
    "# python# Generating a ML-optimized dataset\n",
    "df, stats, xlsx, csv = generate_etalon_Pharma_dataset(seed=42, ml_optimized=True)\n",
    "\n",
    "# Custom size with optimization\n",
    "df, stats, xlsx, csv = generate_custom_dataset(500, 250, 250, ml_optimized=True)\n",
    "\n",
    "# Benchmark for different strategies\n",
    "benchmark_results = generate_ml_benchmark_datasets(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f22a093-f319-47c6-8dd0-26ca670c0acd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e0f0d-c7c7-4878-b8fc-a20b73c15ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ca7e1-996f-49f9-8dc9-1c8f243566c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b615532-4d50-4642-9049-eb13822e3a56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8705063c-f16e-4a6b-b0d4-6ebcdeefdac1",
   "metadata": {},
   "source": [
    "## Enchanced ML Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214d748-9602-445f-9e50-8225de21e0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec6ee8fb-b5c6-4a7b-b835-b2ecc158b07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Pharma PoC ML Pipeline - Usage Examples\n",
      "==================================================\n",
      "\n",
      "1. Running full ML pipeline...\n",
      "🚀 Pharma PoC ML Pipeline initialized (upgraded)\n",
      "============================================================\n",
      "🚀 STARTING Pharma PoC ML PIPELINE\n",
      "============================================================\n",
      "📋 Pipeline steps:\n",
      "   1. Load and prepare data\n",
      "   2. Identify significant features\n",
      "   3. Train models (AH, AI)\n",
      "   4. Analyze feature importance & SHAP\n",
      "   5. Apply predictions with insights (rule-first overrides)\n",
      "   6. Save results to Excel\n",
      "============================================================\n",
      "📁 Loading data from: Pharma_poc_custom_1000_optimized.xlsx\n",
      "✅ Loaded 1060 records\n",
      "📊 Case distribution:\n",
      "   Step 1 - Train: 533 (50.3%)\n",
      "   Step 2 - Suggest: 277 (26.1%)\n",
      "   Step 3 - Forecast: 250 (23.6%)\n",
      "🎯 Training data: 533\n",
      "🎯 Step 2 (Suggest): 277\n",
      "🎯 Step 3 (Forecast): 250\n",
      "\n",
      "🔍 Identifying significant features...\n",
      "📊 Model 1 base features (15):\n",
      "    1. NPI PHARMACY NPI\n",
      "    2. SERV DATE DATE OF SERVICE\n",
      "    3. PROD SVC ID PRODUCT/SERVICE ID\n",
      "    4. DRUG NAME N/A\n",
      "    5. QTY DISP QUANTITY DISPENSED\n",
      "    6. WAC UNIT PRICE WAC UNIT PRICE\n",
      "    7. MFP UNIT PRICE MFP UNIT PRICE\n",
      "    8. EST REIMB AMT Estimated MFG Reimbursement Amt\n",
      "    9. MFP RULE DISC MFP Rule Disc %\n",
      "   10. MFP RULE UNIT PRICE MFP Rule Unit Price\n",
      "   11. MFG NAME MFG Name\n",
      "   12. MFG CUST ID MFG Customer ID\n",
      "   13. Medispan/FDB MFP Price\n",
      "   14. 835 report Actual Payment Amount\n",
      "   15. 835 report Adjudicated Procedure Code (Product/Service ID)\n",
      "\n",
      "📊 Model 2 base features (13):\n",
      "    1. NPI PHARMACY NPI\n",
      "    2. SERV DATE DATE OF SERVICE\n",
      "    3. PROD SVC ID PRODUCT/SERVICE ID\n",
      "    4. DRUG NAME N/A\n",
      "    5. QTY DISP QUANTITY DISPENSED\n",
      "    6. WAC UNIT PRICE WAC UNIT PRICE\n",
      "    7. MFP UNIT PRICE MFP UNIT PRICE\n",
      "    8. EST REIMB AMT Estimated MFG Reimbursement Amt\n",
      "    9. MFP RULE DISC MFP Rule Disc %\n",
      "   10. MFP RULE UNIT PRICE MFP Rule Unit Price\n",
      "   11. MFG NAME MFG Name\n",
      "   12. MFG CUST ID MFG Customer ID\n",
      "   13. Medispan/FDB MFP Price\n",
      "\n",
      "➕ Engineered features to be added at transform-time:\n",
      "   • _month\n",
      "   • _quarter\n",
      "   • _is_wac_rule\n",
      "   • _has_fixed_unit_price\n",
      "   • _est_vs_wac_amt\n",
      "   • _mfp_vs_wac_unit\n",
      "   • _unit_est\n",
      "   • _unit_wac\n",
      "   • _disc_bin\n",
      "   • _disc_is_high18\n",
      "   • _is_slow_mfg\n",
      "   • _npi_profile_sloppy\n",
      "   • _npi_profile_careful\n",
      "   • _is_immuno_or_derm\n",
      "   • _is_cardio_or_diab\n",
      "   • _qty_is_1\n",
      "   • _disc_lt3\n",
      "   • _est_lt_1200\n",
      "   • _q_in_14\n",
      "   • _rule_wac18_immunoDerm_qty1\n",
      "   • _rule_fixed_slow_q14_qty1\n",
      "\n",
      "🎓 Training ML models...\n",
      "========================================\n",
      "📊 Training shapes: X1=(533, 36), X2=(533, 34)\n",
      "   AH classes: 4, AI classes: 5\n",
      "\n",
      "🔸 Training Model 1 (Full features + engineered)...\n",
      "🔸 Training Model 2 (Limited features + engineered)...\n",
      "✅ Models trained\n",
      "\n",
      "📊 Validation Performance:\n",
      "\n",
      "🔸 Model 1 AH Accuracy: 0.925\n",
      "🔸 Model 1 AI Accuracy: 0.682\n",
      "🔸 Model 2 AH Accuracy: 0.869\n",
      "🔸 Model 2 AI Accuracy: 0.654\n",
      "\n",
      "📊 Creating confusion matrices...\n",
      "   ✅ Saved: confusion_matrices_validation.png\n",
      "\n",
      "📋 Classification Reports:\n",
      "\n",
      "🔸 Model 1 AH\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        N907       0.87      1.00      0.93        20\n",
      "        N908       0.90      0.95      0.93        20\n",
      "        N910       0.95      1.00      0.98        40\n",
      "        N911       0.95      0.74      0.83        27\n",
      "\n",
      "    accuracy                           0.93       107\n",
      "   macro avg       0.92      0.92      0.92       107\n",
      "weighted avg       0.93      0.93      0.92       107\n",
      "\n",
      "\n",
      "🔸 Model 1 AI\n",
      "                                                        precision    recall  f1-score   support\n",
      "\n",
      "       Sent to collection due to MFG timing difference       0.87      1.00      0.93        20\n",
      "Write off due to WAC UNIT PRICE diff from WAC Medispan       0.90      0.95      0.93        20\n",
      "                    Write off due to duplicate payment       0.66      0.70      0.68        27\n",
      "              Write off due to manual data entry error       0.24      0.20      0.22        20\n",
      "   Write off due to payment applied to the wrong claim       0.65      0.55      0.59        20\n",
      "\n",
      "                                              accuracy                           0.68       107\n",
      "                                             macro avg       0.66      0.68      0.67       107\n",
      "                                          weighted avg       0.66      0.68      0.67       107\n",
      "\n",
      "\n",
      "🔸 Model 2 AH\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        N907       0.79      0.75      0.77        20\n",
      "        N908       0.90      0.95      0.93        20\n",
      "        N910       0.95      1.00      0.98        40\n",
      "        N911       0.76      0.70      0.73        27\n",
      "\n",
      "    accuracy                           0.87       107\n",
      "   macro avg       0.85      0.85      0.85       107\n",
      "weighted avg       0.86      0.87      0.87       107\n",
      "\n",
      "\n",
      "🔸 Model 2 AI\n",
      "                                                        precision    recall  f1-score   support\n",
      "\n",
      "       Sent to collection due to MFG timing difference       0.81      0.85      0.83        20\n",
      "Write off due to WAC UNIT PRICE diff from WAC Medispan       0.90      0.95      0.93        20\n",
      "                    Write off due to duplicate payment       0.66      0.70      0.68        27\n",
      "              Write off due to manual data entry error       0.17      0.15      0.16        20\n",
      "   Write off due to payment applied to the wrong claim       0.67      0.60      0.63        20\n",
      "\n",
      "                                              accuracy                           0.65       107\n",
      "                                             macro avg       0.64      0.65      0.64       107\n",
      "                                          weighted avg       0.64      0.65      0.65       107\n",
      "\n",
      "\n",
      "📈 Cross-validation (5-fold) on full training set:\n",
      "   Model 1 AH: 0.904 ± 0.044\n",
      "   Model 1 AI: 0.921 ± 0.028\n",
      "   Model 2 AH: 0.850 ± 0.050\n",
      "   Model 2 AI: 0.854 ± 0.047\n",
      "\n",
      "🔄 Retraining on full dataset...\n",
      "\n",
      "🔍 Analyzing feature importance...\n",
      "========================================\n",
      "\n",
      "🔍 Creating SHAP explainers...\n",
      "\n",
      "📊 Generating SHAP visualizations...\n",
      "   ⚠️ SHAP visualization warning: 'PharmaPoCMLPipeline' object has no attribute '_beeswarm_for_predicted_class'\n",
      "\n",
      "🔮 Applying predictions...\n",
      "========================================\n",
      "🔸 Model 1 → Step 2 (Suggest)\n",
      "   Predictions: AH=277, AI=277\n",
      "   Testing SHAP for model1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PermutationExplainer explainer: 2it [00:17, 17.43s/it]                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ SHAP test ok: AH (1, 36, 4), AI (1, 36, 5)\n",
      "   Step 2 progress: 1/277\n",
      "   Step 2 progress: 51/277\n",
      "   Step 2 progress: 101/277\n",
      "   Step 2 progress: 151/277\n",
      "   Step 2 progress: 201/277\n",
      "   Step 2 progress: 251/277\n",
      "✅ Updated 277 Step 2 rows\n",
      "🔸 Model 2 → Step 3 (Forecast)\n",
      "   Predictions: AH=250, AI=250\n",
      "   Testing SHAP for model2...\n",
      "   ✅ SHAP test ok: AH (1, 34, 4), AI (1, 34, 5)\n",
      "   Step 3 progress: 1/250\n",
      "   Step 3 progress: 51/250\n",
      "   Step 3 progress: 101/250\n",
      "   Step 3 progress: 151/250\n",
      "   Step 3 progress: 201/250\n",
      "✅ Updated 250 Step 3 rows\n",
      "\n",
      "📊 Prediction Summary:\n",
      "Step 2 AH: {'N907': 119, 'N908': 21, 'N910': 43, 'N911': 94}\n",
      "Step 2 AI: {'Sent to collection due to MFG timing difference': 24, 'Write off due to WAC UNIT PRICE diff from WAC Medispan': 8, 'Write off due to duplicate payment': 95, 'Write off due to manual data entry error': 128, 'Write off due to payment applied to the wrong claim': 22}\n",
      "Step 3 AH: {'N907': 40, 'N908': 16, 'N910': 21, 'N911': 173}\n",
      "Step 3 AI: {'Sent to collection due to MFG timing difference': 11, 'Write off due to WAC UNIT PRICE diff from WAC Medispan': 9, 'Write off due to duplicate payment': 167, 'Write off due to manual data entry error': 48, 'Write off due to payment applied to the wrong claim': 15}\n",
      "\n",
      "📋 Sample insights:\n",
      "Step 2 sample: ML Prediction: AH='N910', AI='Write off due to duplicate payment' (no SHAP)\n",
      "Step 3 sample: ML Prediction: AH='N911', AI='Write off due to duplicate payment' (no SHAP)\n",
      "\n",
      "💾 Saving results to Pharma_poc_custom_1000_optimized_ml_results.xlsx...\n",
      "✅ Results saved\n",
      "\n",
      "🎨 Creating individual waterfall plots...\n",
      "\n",
      "📊 Creating individual SHAP plots for rows [0, 1]...\n",
      "🔸 Waterfalls for Step 2 (Model 1)\n",
      "   ✅ Step 2 Row 0 done\n",
      "   ✅ Step 2 Row 1 done\n",
      "🔸 Waterfalls for Step 3 (Model 2)\n",
      "   ✅ Step 3 Row 0 done\n",
      "   ✅ Step 3 Row 1 done\n",
      "\n",
      "📋 Sample insights from predictions:\n",
      "==================================================\n",
      "🔸 Step 2 Sample:\n",
      "   Drug: HUMIRA INJ 40MG/0.8ML\n",
      "   Predicted AH: N910\n",
      "   Predicted AI: Write off due to duplicate payment\n",
      "   Insight: ML Prediction: AH='N910', AI='Write off due to duplicate payment' (no SHAP)...\n",
      "\n",
      "🔸 Step 3 Sample:\n",
      "   Drug: FIASP F/P PEN 100U/ML\n",
      "   Predicted AH: N911\n",
      "   Predicted AI: Write off due to duplicate payment\n",
      "   Insight: ML Prediction: AH='N911', AI='Write off due to duplicate payment' (no SHAP)...\n",
      "\n",
      "📊 Artifacts:\n",
      "   📈 shap_model1_ah_summary.png\n",
      "   📈 shap_model1_ai_summary.png\n",
      "   📈 shap_model2_ah_summary.png\n",
      "   📈 shap_model2_ai_summary.png\n",
      "   📊 feature_importance_comparison.png\n",
      "   🎯 confusion_matrices_validation.png\n",
      "   🎯 individual_waterfall_plots/ (PNG per-row)\n",
      "\n",
      "🎉 PIPELINE COMPLETED\n",
      "📁 Results saved to: Pharma_poc_custom_1000_optimized_ml_results.xlsx\n",
      "============================================================\n",
      "\n",
      "2. Creating custom waterfalls...\n",
      "🎨 Creating custom waterfalls for rows [0, 1, 5, 10]...\n",
      "🚀 Pharma PoC ML Pipeline initialized (upgraded)\n",
      "============================================================\n",
      "📁 Loading data from: Pharma_poc_custom_1000_optimized.xlsx\n",
      "✅ Loaded 1060 records\n",
      "📊 Case distribution:\n",
      "   Step 1 - Train: 533 (50.3%)\n",
      "   Step 2 - Suggest: 277 (26.1%)\n",
      "   Step 3 - Forecast: 250 (23.6%)\n",
      "🎯 Training data: 533\n",
      "🎯 Step 2 (Suggest): 277\n",
      "🎯 Step 3 (Forecast): 250\n",
      "\n",
      "🔍 Identifying significant features...\n",
      "📊 Model 1 base features (15):\n",
      "    1. NPI PHARMACY NPI\n",
      "    2. SERV DATE DATE OF SERVICE\n",
      "    3. PROD SVC ID PRODUCT/SERVICE ID\n",
      "    4. DRUG NAME N/A\n",
      "    5. QTY DISP QUANTITY DISPENSED\n",
      "    6. WAC UNIT PRICE WAC UNIT PRICE\n",
      "    7. MFP UNIT PRICE MFP UNIT PRICE\n",
      "    8. EST REIMB AMT Estimated MFG Reimbursement Amt\n",
      "    9. MFP RULE DISC MFP Rule Disc %\n",
      "   10. MFP RULE UNIT PRICE MFP Rule Unit Price\n",
      "   11. MFG NAME MFG Name\n",
      "   12. MFG CUST ID MFG Customer ID\n",
      "   13. Medispan/FDB MFP Price\n",
      "   14. 835 report Actual Payment Amount\n",
      "   15. 835 report Adjudicated Procedure Code (Product/Service ID)\n",
      "\n",
      "📊 Model 2 base features (13):\n",
      "    1. NPI PHARMACY NPI\n",
      "    2. SERV DATE DATE OF SERVICE\n",
      "    3. PROD SVC ID PRODUCT/SERVICE ID\n",
      "    4. DRUG NAME N/A\n",
      "    5. QTY DISP QUANTITY DISPENSED\n",
      "    6. WAC UNIT PRICE WAC UNIT PRICE\n",
      "    7. MFP UNIT PRICE MFP UNIT PRICE\n",
      "    8. EST REIMB AMT Estimated MFG Reimbursement Amt\n",
      "    9. MFP RULE DISC MFP Rule Disc %\n",
      "   10. MFP RULE UNIT PRICE MFP Rule Unit Price\n",
      "   11. MFG NAME MFG Name\n",
      "   12. MFG CUST ID MFG Customer ID\n",
      "   13. Medispan/FDB MFP Price\n",
      "\n",
      "➕ Engineered features to be added at transform-time:\n",
      "   • _month\n",
      "   • _quarter\n",
      "   • _is_wac_rule\n",
      "   • _has_fixed_unit_price\n",
      "   • _est_vs_wac_amt\n",
      "   • _mfp_vs_wac_unit\n",
      "   • _unit_est\n",
      "   • _unit_wac\n",
      "   • _disc_bin\n",
      "   • _disc_is_high18\n",
      "   • _is_slow_mfg\n",
      "   • _npi_profile_sloppy\n",
      "   • _npi_profile_careful\n",
      "   • _is_immuno_or_derm\n",
      "   • _is_cardio_or_diab\n",
      "   • _qty_is_1\n",
      "   • _disc_lt3\n",
      "   • _est_lt_1200\n",
      "   • _q_in_14\n",
      "   • _rule_wac18_immunoDerm_qty1\n",
      "   • _rule_fixed_slow_q14_qty1\n",
      "\n",
      "🎓 Training ML models...\n",
      "========================================\n",
      "📊 Training shapes: X1=(533, 36), X2=(533, 34)\n",
      "   AH classes: 4, AI classes: 5\n",
      "\n",
      "🔸 Training Model 1 (Full features + engineered)...\n",
      "🔸 Training Model 2 (Limited features + engineered)...\n",
      "✅ Models trained\n",
      "\n",
      "📊 Validation Performance:\n",
      "\n",
      "🔸 Model 1 AH Accuracy: 0.925\n",
      "🔸 Model 1 AI Accuracy: 0.682\n",
      "🔸 Model 2 AH Accuracy: 0.869\n",
      "🔸 Model 2 AI Accuracy: 0.654\n",
      "\n",
      "📊 Creating confusion matrices...\n",
      "   ✅ Saved: confusion_matrices_validation.png\n",
      "\n",
      "📋 Classification Reports:\n",
      "\n",
      "🔸 Model 1 AH\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        N907       0.87      1.00      0.93        20\n",
      "        N908       0.90      0.95      0.93        20\n",
      "        N910       0.95      1.00      0.98        40\n",
      "        N911       0.95      0.74      0.83        27\n",
      "\n",
      "    accuracy                           0.93       107\n",
      "   macro avg       0.92      0.92      0.92       107\n",
      "weighted avg       0.93      0.93      0.92       107\n",
      "\n",
      "\n",
      "🔸 Model 1 AI\n",
      "                                                        precision    recall  f1-score   support\n",
      "\n",
      "       Sent to collection due to MFG timing difference       0.87      1.00      0.93        20\n",
      "Write off due to WAC UNIT PRICE diff from WAC Medispan       0.90      0.95      0.93        20\n",
      "                    Write off due to duplicate payment       0.66      0.70      0.68        27\n",
      "              Write off due to manual data entry error       0.24      0.20      0.22        20\n",
      "   Write off due to payment applied to the wrong claim       0.65      0.55      0.59        20\n",
      "\n",
      "                                              accuracy                           0.68       107\n",
      "                                             macro avg       0.66      0.68      0.67       107\n",
      "                                          weighted avg       0.66      0.68      0.67       107\n",
      "\n",
      "\n",
      "🔸 Model 2 AH\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        N907       0.79      0.75      0.77        20\n",
      "        N908       0.90      0.95      0.93        20\n",
      "        N910       0.95      1.00      0.98        40\n",
      "        N911       0.76      0.70      0.73        27\n",
      "\n",
      "    accuracy                           0.87       107\n",
      "   macro avg       0.85      0.85      0.85       107\n",
      "weighted avg       0.86      0.87      0.87       107\n",
      "\n",
      "\n",
      "🔸 Model 2 AI\n",
      "                                                        precision    recall  f1-score   support\n",
      "\n",
      "       Sent to collection due to MFG timing difference       0.81      0.85      0.83        20\n",
      "Write off due to WAC UNIT PRICE diff from WAC Medispan       0.90      0.95      0.93        20\n",
      "                    Write off due to duplicate payment       0.66      0.70      0.68        27\n",
      "              Write off due to manual data entry error       0.17      0.15      0.16        20\n",
      "   Write off due to payment applied to the wrong claim       0.67      0.60      0.63        20\n",
      "\n",
      "                                              accuracy                           0.65       107\n",
      "                                             macro avg       0.64      0.65      0.64       107\n",
      "                                          weighted avg       0.64      0.65      0.65       107\n",
      "\n",
      "\n",
      "📈 Cross-validation (5-fold) on full training set:\n",
      "   Model 1 AH: 0.904 ± 0.044\n",
      "   Model 1 AI: 0.921 ± 0.028\n",
      "   Model 2 AH: 0.850 ± 0.050\n",
      "   Model 2 AI: 0.854 ± 0.047\n",
      "\n",
      "🔄 Retraining on full dataset...\n",
      "\n",
      "🔍 Analyzing feature importance...\n",
      "========================================\n",
      "\n",
      "🔍 Creating SHAP explainers...\n",
      "\n",
      "📊 Generating SHAP visualizations...\n",
      "   ⚠️ SHAP visualization warning: 'PharmaPoCMLPipeline' object has no attribute '_beeswarm_for_predicted_class'\n",
      "\n",
      "📊 Creating individual SHAP plots for rows [0, 1, 5, 10]...\n",
      "🔸 Waterfalls for Step 2 (Model 1)\n",
      "   ✅ Step 2 Row 0 done\n",
      "   ✅ Step 2 Row 1 done\n",
      "   ✅ Step 2 Row 5 done\n",
      "   ✅ Step 2 Row 10 done\n",
      "🔸 Waterfalls for Step 3 (Model 2)\n",
      "   ✅ Step 3 Row 0 done\n",
      "   ✅ Step 3 Row 1 done\n",
      "   ✅ Step 3 Row 5 done\n",
      "   ✅ Step 3 Row 10 done\n",
      "✅ Custom waterfalls created in my_custom_waterfalls/\n",
      "\n",
      "✅ Analysis complete!\n",
      "• Two model sets (Model1 full+eng, Model2 A–T+eng)\n",
      "• HistGradientBoostingClassifier + rule-first AI overrides\n",
      "• Rich engineered features matching generator logic\n",
      "• SHAP (permutation) + confusion matrices + per-row plots\n",
      "• Same artifacts filenames and Excel structure\n",
      "CPU times: total: 10min 55s\n",
      "Wall time: 5min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Pharma PoC ML Pipeline (Upgraded: rule-first + engineered features + HistGBDT + robust SHAP)\n",
    "# - Backward-compatible artifacts and structure\n",
    "# - Big boost for AI via rule head + tighter feature engineering\n",
    "# - Keeps the same public API (run_Pharma_ml_analysis, create_custom_waterfall_plots)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Models & metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score\n",
    ")\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# Visualization\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class PharmaPoCMLPipeline:\n",
    "    def __init__(self, input_file: str):\n",
    "        \"\"\"\n",
    "        Initialize upgraded Pharma PoC ML Pipeline for predicting:\n",
    "          - AH: '835 report Qualifier Code / RARC codes'\n",
    "          - AI: 'Expected Outcomes Error category'\n",
    "        while keeping the same outputs and artifacts.\n",
    "\n",
    "        Args:\n",
    "            input_file (str): Path to input Excel file (e.g., 'Pharma_poc_custom_1000_scored.xlsx')\n",
    "        \"\"\"\n",
    "        self.input_file = input_file\n",
    "        self.df: Optional[pd.DataFrame] = None\n",
    "        self.train_data: Optional[pd.DataFrame] = None\n",
    "        self.step2_data: Optional[pd.DataFrame] = None\n",
    "        self.step3_data: Optional[pd.DataFrame] = None\n",
    "\n",
    "        # --- Models (stronger learners) ---\n",
    "        # Using HistGradientBoostingClassifier (sklearn) for robust multiclass performance\n",
    "        # Note: No class_weight here to stay broadly compatible with different sklearn versions\n",
    "        hgb_params = dict(\n",
    "            max_depth=6,\n",
    "            learning_rate=0.10,\n",
    "            max_bins=255,\n",
    "            l2_regularization=0.01,\n",
    "            early_stopping=False,\n",
    "            random_state=42\n",
    "        )\n",
    "        self.model1_ah = HistGradientBoostingClassifier(**hgb_params)\n",
    "        self.model1_ai = HistGradientBoostingClassifier(**hgb_params)\n",
    "        self.model2_ah = HistGradientBoostingClassifier(**hgb_params)\n",
    "        self.model2_ai = HistGradientBoostingClassifier(**hgb_params)\n",
    "\n",
    "        # Encoders\n",
    "        self.label_encoders: Dict[str, LabelEncoder] = {}\n",
    "        self.target_encoders: Dict[str, LabelEncoder] = {}\n",
    "\n",
    "        # SHAP explainers (created lazily)\n",
    "        self.explainers: Dict[str, shap.Explainer] = {}\n",
    "\n",
    "        # Feature lists\n",
    "        self.model1_base_features: List[str] = []   # base columns (A–AG excluding constants/copies/targets)\n",
    "        self.model2_base_features: List[str] = []   # base columns (A–T-only scenario)\n",
    "        self.engineered_feature_names: List[str] = []  # engineered features added to both models\n",
    "\n",
    "        # Static dictionaries that reflect generator's logic (for engineered features & rule-head)\n",
    "        self.slow_mfg = {\n",
    "            'JANSSEN BIOTECH', 'AMGEN/ IMMUNEX', 'BRISTOL MYERS SQUIBB'\n",
    "        }\n",
    "        self.npi_profiles = {\n",
    "            1013998921:'careful',1518039858:'careful',1043382302:'sloppy',1063510097:'sloppy',\n",
    "            1326029232:'neutral',1629341177:'neutral',1578836698:'neutral',1234567890:'sloppy',\n",
    "            1345678901:'careful',1456789012:'neutral',1567890123:'sloppy',1678901234:'neutral'\n",
    "        }\n",
    "        # Drug classes to mirror generator patterns\n",
    "        self.immuno_or_derm = {\n",
    "            'STELARA INJ 5MG/ML','ENBREL INJ 25/0.5ML','STELARA INJ 90MG/ML','ENBREL SRCLK INJ 50MG/ML',\n",
    "            'HUMIRA INJ 40MG/0.8ML','DUPIXENT INJ 300MG/2ML','SKYRIZI INJ 150MG/ML'\n",
    "        }\n",
    "        self.cardio_or_diab = {\n",
    "            'ENTRESTO CAP 15-16MG','FIASP F/P PEN 100U/ML','OZEMPIC INJ 0.25MG/1.5ML'\n",
    "        }\n",
    "\n",
    "        # Primary mapping AI -> AH (from generator)\n",
    "        self.mapping_primary_ah = {\n",
    "            'Write off due to manual data entry error': 'N907',\n",
    "            'Write off due to payment applied to the wrong claim': 'N908',\n",
    "            'Write off due to WAC UNIT PRICE diff from WAC Medispan': 'N910',\n",
    "            'Write off due to duplicate payment': 'N911',\n",
    "            'Sent to collection due to MFG timing difference': 'N910'\n",
    "        }\n",
    "\n",
    "        print(\"🚀 Pharma PoC ML Pipeline initialized (upgraded)\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "    # ===========================================================\n",
    "    # Data loading & split\n",
    "    # ===========================================================\n",
    "    def load_and_prepare_data(self):\n",
    "        print(\"📁 Loading data from:\", self.input_file)\n",
    "        try:\n",
    "            self.df = pd.read_excel(self.input_file)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading data: {e}\")\n",
    "            raise\n",
    "\n",
    "        print(f\"✅ Loaded {len(self.df)} records\")\n",
    "        case_dist = self.df['Case'].value_counts()\n",
    "        print(\"📊 Case distribution:\")\n",
    "        for case, count in case_dist.items():\n",
    "            pct = (count / len(self.df)) * 100\n",
    "            print(f\"   {case}: {count} ({pct:.1f}%)\")\n",
    "\n",
    "        self.train_data = self.df[self.df['Case'] == 'Step 1 - Train'].copy()\n",
    "        self.step2_data = self.df[self.df['Case'] == 'Step 2 - Suggest'].copy()\n",
    "        self.step3_data = self.df[self.df['Case'] == 'Step 3 - Forecast'].copy()\n",
    "\n",
    "        print(f\"🎯 Training data: {len(self.train_data)}\")\n",
    "        print(f\"🎯 Step 2 (Suggest): {len(self.step2_data)}\")\n",
    "        print(f\"🎯 Step 3 (Forecast): {len(self.step3_data)}\")\n",
    "\n",
    "    def _save_beeswarm_multiclass(self, explainer_key, X_sample, model, class_names, title_prefix, out_prefix, per_class=False):\n",
    "        if explainer_key not in self.explainers:\n",
    "            return\n",
    "    \n",
    "        exp = self.explainers[explainer_key](X_sample)  # shap.Explanation\n",
    "        vals = exp.values  # (n, C, F) или (n, F)\n",
    "        feat_names = list(X_sample.columns)\n",
    "    \n",
    "        def _plot_and_save(values_2d, base_vals_1d, subtitle, out_path):\n",
    "            exp_single = shap.Explanation(\n",
    "                values=values_2d,\n",
    "                base_values=base_vals_1d,\n",
    "                data=exp.data,\n",
    "                feature_names=feat_names\n",
    "            )\n",
    "            shap.plots.beeswarm(exp_single, max_display=15, show=False)\n",
    "            plt.title(f\"{title_prefix}{subtitle}\", fontsize=14, fontweight='bold')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(out_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "            plt.close()\n",
    "            print(f\"   ✅ {out_path} saved\")\n",
    "    \n",
    "\n",
    "        if vals.ndim == 2:\n",
    "            base_vals = exp.base_values if np.ndim(exp.base_values) == 1 else np.array(exp.base_values).ravel()\n",
    "            _plot_and_save(vals, base_vals, \"\", f\"{out_prefix}.png\")\n",
    "            return\n",
    "    \n",
    "\n",
    "        if vals.ndim == 3:\n",
    "            proba = model.predict_proba(X_sample)  # (n, C)\n",
    "            idx = np.argmax(proba, axis=1)        # (n,)\n",
    "            values_2d = np.stack([vals[i, idx[i], :] for i in range(vals.shape[0])], axis=0)\n",
    "            base_vals_1d = np.array([exp.base_values[i, idx[i]] for i in range(vals.shape[0])])\n",
    "    \n",
    "\n",
    "            _plot_and_save(values_2d, base_vals_1d, \" (predicted class)\", f\"{out_prefix}.png\")\n",
    "    \n",
    "            if per_class:\n",
    "                \n",
    "                for k, cname in enumerate(class_names):\n",
    "                    values_k = vals[:, k, :]                     # (n, F)\n",
    "                    base_k = np.array(exp.base_values)[:, k]     # (n,)\n",
    "                    safe_name = str(cname).replace('/', '_').replace(' ', '_')\n",
    "                    _plot_and_save(values_k, base_k, f\" (class={cname})\", f\"{out_prefix}_class_{safe_name}.png\")\n",
    "\n",
    "\n",
    "\n",
    "    def _build_row_explanation(self, explainer_key: str, X_single: pd.DataFrame, pred_class: int) -> shap.Explanation:\n",
    "\n",
    "        exp = self.explainers[explainer_key](X_single)   # shap.Explanation\n",
    "        vals = exp.values                                # (1, C, F) или (1, F)\n",
    "        base = exp.base_values                           # (1, C) или (1,) / scalar\n",
    "    \n",
    "        if vals.ndim == 3:       # (n=1, n_classes, n_features)\n",
    "            v = vals[0, pred_class, :]\n",
    "            if np.ndim(base) == 2:\n",
    "                b = base[0, pred_class]\n",
    "            elif np.ndim(base) == 1:\n",
    "                b = base[pred_class]\n",
    "            else:\n",
    "                b = float(base)\n",
    "        elif vals.ndim == 2:     # (1, n_features)\n",
    "            v = vals[0, :]\n",
    "            b = base[0] if np.ndim(base) else float(base)\n",
    "        else:                    # (n_features,)\n",
    "            v = vals\n",
    "            b = base\n",
    "    \n",
    "        return shap.Explanation(\n",
    "            values=v,\n",
    "            base_values=b,\n",
    "            data=X_single.values[0],\n",
    "            feature_names=X_single.columns.tolist()\n",
    "        )\n",
    "    \n",
    "    def _save_shap_row_waterfall(self, X_single: pd.DataFrame, explainer_key: str, pred_class: int,\n",
    "                                 save_path: str, title: str):\n",
    "        try:\n",
    "            ex = self._build_row_explanation(explainer_key, X_single, pred_class)\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            shap.plots.waterfall(ex, max_display=15, show=False)\n",
    "            plt.title(title, fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "            plt.close()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            # fallback: горизонтальный бар по |SHAP|\n",
    "            try:\n",
    "                ex = self._build_row_explanation(explainer_key, X_single, pred_class)\n",
    "                vals = ex.values\n",
    "                names = ex.feature_names\n",
    "                top_idx = np.argsort(np.abs(vals))[-15:]\n",
    "                vals_top = vals[top_idx]\n",
    "                names_top = [names[i] for i in top_idx]\n",
    "                order = np.argsort(np.abs(vals_top))\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.barh(np.array(names_top)[order], vals_top[order])\n",
    "                plt.title(title + \" (top SHAP)\", fontsize=14)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "                plt.close()\n",
    "                return True\n",
    "            except:\n",
    "                return False\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # ===========================================================\n",
    "    # Feature selection & engineering\n",
    "    # ===========================================================\n",
    "    def identify_significant_features(self):\n",
    "        \"\"\"\n",
    "        Build the base feature lists for both model sets and define engineered features.\n",
    "        \"\"\"\n",
    "        print(\"\\n🔍 Identifying significant features...\")\n",
    "\n",
    "        all_columns = list(self.df.columns)\n",
    "\n",
    "        # Constants & copied & targets to exclude\n",
    "        exclude_fields = [\n",
    "            'ADJ DATE MEDICARE TXN DATE/TIME',     # A - (quasi) unique timestamp, low signal\n",
    "            'RX NBR PRESCRIPTION NUMBER',          # C - auto-increment\n",
    "            'CLAIM ID MEDICARE ICN/AUTH #',        # E - auto-increment\n",
    "            'Medispan/FDB Effective Date',         # S - constant\n",
    "            'Medispan/FDB Termination Date',       # T - constant\n",
    "            '835 report Check Number',             # U - mostly constant 'N/A'\n",
    "            '835 report Refill Number',            # Y - constant '00'\n",
    "            '835 report Estimated refund from adjudication',  # Z - constant 'N/A'\n",
    "            '835 report Adjustment Code / CARC codes',        # AE - fixed/low variance\n",
    "            '835 report Adjustment Amount',        # AF - mostly 'N/A'\n",
    "            '835 report Adjustment Quantity',      # AG - mostly 'N/A'\n",
    "            'Case'                                 # group variable\n",
    "        ]\n",
    "        # Full copy duplicates\n",
    "        copy_fields = [\n",
    "            'Medispan/FDB WAC Price',              # Q == I mostly\n",
    "            '835 report Claim Number',             # V == E\n",
    "            '835 report Pharmacy Number',          # W == B\n",
    "            '835 report Rx Number',                # X == C\n",
    "            '835 report Date Filled/Date of Service',  # AB == D (service date-ish)\n",
    "            '835 report Quantity'                  # AC == H\n",
    "        ]\n",
    "        # Targets\n",
    "        target_fields = [\n",
    "            '835 report Qualifier Code / RARC codes',  # AH\n",
    "            'Expected Outcomes Error category',        # AI\n",
    "            'Expected Outcomes Clerk Input',           # AJ\n",
    "            ' Questions/comments'                      # AK\n",
    "        ]\n",
    "\n",
    "        # Build Model 1 (full A–AG minus constants/copies/targets)\n",
    "        exclude_all = set(exclude_fields + copy_fields + target_fields)\n",
    "        model1_candidates = [c for c in all_columns if c not in exclude_all]\n",
    "\n",
    "        # Model 2 (A–T scenario: no \"835 report *\" features at all)\n",
    "        model2_exclude_835 = [c for c in all_columns if c.startswith('835 report')]\n",
    "        model2_exclude = set(exclude_fields + target_fields + model2_exclude_835 + copy_fields)\n",
    "        model2_candidates = [c for c in all_columns if c not in model2_exclude]\n",
    "\n",
    "        # Filter out non-varying columns\n",
    "        self.model1_base_features = [c for c in model1_candidates if self.df[c].nunique() > 1]\n",
    "        self.model2_base_features = [c for c in model2_candidates if self.df[c].nunique() > 1]\n",
    "\n",
    "        # Keep '835 report Actual Payment Amount' only in Model 1 (Step-2 use case has it)\n",
    "        # Model 2 excludes it by design to avoid leakage for Forecast\n",
    "        print(f\"📊 Model 1 base features ({len(self.model1_base_features)}):\")\n",
    "        for i, feat in enumerate(self.model1_base_features, 1):\n",
    "            print(f\"   {i:2d}. {feat}\")\n",
    "\n",
    "        print(f\"\\n📊 Model 2 base features ({len(self.model2_base_features)}):\")\n",
    "        for i, feat in enumerate(self.model2_base_features, 1):\n",
    "            print(f\"   {i:2d}. {feat}\")\n",
    "\n",
    "        # Engineered features (added to both models at transform-time)\n",
    "        self.engineered_feature_names = [\n",
    "            '_month','_quarter',\n",
    "            '_is_wac_rule','_has_fixed_unit_price',\n",
    "            '_est_vs_wac_amt','_mfp_vs_wac_unit',\n",
    "            '_unit_est','_unit_wac',\n",
    "            '_disc_bin','_disc_is_high18',\n",
    "            '_is_slow_mfg',\n",
    "            '_npi_profile_sloppy','_npi_profile_careful',\n",
    "            '_is_immuno_or_derm','_is_cardio_or_diab',\n",
    "            # New rule-support features\n",
    "            '_qty_is_1','_disc_lt3','_est_lt_1200','_q_in_14',\n",
    "            '_rule_wac18_immunoDerm_qty1','_rule_fixed_slow_q14_qty1'\n",
    "        ]\n",
    "\n",
    "        print(\"\\n➕ Engineered features to be added at transform-time:\")\n",
    "        for name in self.engineered_feature_names:\n",
    "            print(f\"   • {name}\")\n",
    "\n",
    "    def _engineer_features_frame(self, df_subset: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create engineered features DataFrame aligned with df_subset index.\n",
    "        \"\"\"\n",
    "        # Base fields with robust type handling\n",
    "        def _to_float(s):\n",
    "            try:\n",
    "                return pd.to_numeric(s, errors='coerce')\n",
    "            except Exception:\n",
    "                return pd.Series([np.nan]*len(df_subset), index=df_subset.index)\n",
    "\n",
    "        wac = _to_float(df_subset.get('WAC UNIT PRICE WAC UNIT PRICE'))\n",
    "        mfp = _to_float(df_subset.get('MFP UNIT PRICE MFP UNIT PRICE'))\n",
    "        qty = _to_float(df_subset.get('QTY DISP QUANTITY DISPENSED'))\n",
    "        est = _to_float(df_subset.get('EST REIMB AMT Estimated MFG Reimbursement Amt'))\n",
    "        disc = _to_float(df_subset.get('MFP RULE DISC MFP Rule Disc %')).fillna(0.0)\n",
    "\n",
    "        # Time features\n",
    "        # SERV DATE format: 'DD/MM/YYYY' (from generator)\n",
    "        serv = df_subset.get('SERV DATE DATE OF SERVICE').astype(str).fillna('')\n",
    "        months = serv.str.split('/', expand=True)[1]\n",
    "        months = pd.to_numeric(months, errors='coerce').fillna(1).astype(int)\n",
    "        month = months.clip(1, 12)\n",
    "        quarter = ((month - 1) // 3 + 1)\n",
    "\n",
    "        # Rule flags\n",
    "        is_wac = (df_subset.get('MFP RULE PRC POINT MFP Rule Price Point').astype(str) == 'WAC').astype(int)\n",
    "        has_fixed = _to_float(df_subset.get('MFP RULE UNIT PRICE MFP Rule Unit Price')).fillna(0.0)\n",
    "        has_fixed = (has_fixed > 0).astype(int)\n",
    "\n",
    "        # Ratios and unit amounts\n",
    "        base_amt = (wac.fillna(0.0) * qty.fillna(0.0)).replace(0, np.nan)\n",
    "        est_vs_wac_amt = (est / base_amt).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        mfp_vs_wac_unit = (mfp / wac.replace(0, np.nan)).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        unit_est = (est / qty.replace(0, np.nan)).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "        unit_wac = wac.fillna(0.0)\n",
    "\n",
    "        # Discount bins\n",
    "        disc_bin = pd.cut(disc, bins=[-1, 0, 3, 10, 18, 1000], labels=[0,1,2,3,4]).astype(int)\n",
    "        disc_hi18 = (disc >= 18.0).astype(int)\n",
    "\n",
    "        # Slow MFG\n",
    "        mfg = df_subset.get('MFG NAME MFG Name').astype(str).fillna('')\n",
    "        is_slow = mfg.isin(self.slow_mfg).astype(int)\n",
    "\n",
    "        # NPI profiles\n",
    "        npi = df_subset.get('NPI PHARMACY NPI')\n",
    "        npi = pd.to_numeric(npi, errors='coerce').fillna(-1).astype(int)\n",
    "        profiles = npi.map(self.npi_profiles).fillna('neutral')\n",
    "        npi_sloppy = (profiles == 'sloppy').astype(int)\n",
    "        npi_careful = (profiles == 'careful').astype(int)\n",
    "\n",
    "        # Drug class flags\n",
    "        drug = df_subset.get('DRUG NAME N/A').astype(str).fillna('')\n",
    "        is_immuno_derm = drug.isin(self.immuno_or_derm).astype(int)\n",
    "        is_cardio_diab = drug.isin(self.cardio_or_diab).astype(int)\n",
    "\n",
    "        # Additional rule-helper flags\n",
    "        qty_is_1 = (qty == 1).astype(int)\n",
    "        disc_lt3 = (disc < 3.0).astype(int)\n",
    "        est_lt_1200 = (est < 1200).astype(int)\n",
    "        q_in_14 = ((quarter == 1) | (quarter == 4)).astype(int)\n",
    "\n",
    "        # Composite rule matches (direct mirrors of generator logic)\n",
    "        rule_wac18_immunoDerm_qty1 = ((is_wac == 1) & (disc >= 18.0) & (is_immuno_derm == 1) & (qty_is_1 == 1)).astype(int)\n",
    "        rule_fixed_slow_q14_qty1 = ((has_fixed == 1) & (is_slow == 1) & (q_in_14 == 1) & (qty_is_1 == 1)).astype(int)\n",
    "\n",
    "        eng = pd.DataFrame({\n",
    "            '_month': month.astype(int),\n",
    "            '_quarter': quarter.astype(int),\n",
    "            '_is_wac_rule': is_wac.astype(int),\n",
    "            '_has_fixed_unit_price': has_fixed.astype(int),\n",
    "            '_est_vs_wac_amt': est_vs_wac_amt.astype(float),\n",
    "            '_mfp_vs_wac_unit': mfp_vs_wac_unit.astype(float),\n",
    "            '_unit_est': unit_est.astype(float),\n",
    "            '_unit_wac': unit_wac.astype(float),\n",
    "            '_disc_bin': disc_bin.astype(int),\n",
    "            '_disc_is_high18': disc_hi18.astype(int),\n",
    "            '_is_slow_mfg': is_slow.astype(int),\n",
    "            '_npi_profile_sloppy': npi_sloppy.astype(int),\n",
    "            '_npi_profile_careful': npi_careful.astype(int),\n",
    "            '_is_immuno_or_derm': is_immuno_derm.astype(int),\n",
    "            '_is_cardio_or_diab': is_cardio_diab.astype(int),\n",
    "            '_qty_is_1': qty_is_1.astype(int),\n",
    "            '_disc_lt3': disc_lt3.astype(int),\n",
    "            '_est_lt_1200': est_lt_1200.astype(int),\n",
    "            '_q_in_14': q_in_14.astype(int),\n",
    "            '_rule_wac18_immunoDerm_qty1': rule_wac18_immunoDerm_qty1.astype(int),\n",
    "            '_rule_fixed_slow_q14_qty1': rule_fixed_slow_q14_qty1.astype(int),\n",
    "        }, index=df_subset.index)\n",
    "\n",
    "        return eng\n",
    "\n",
    "    # ===========================================================\n",
    "    # Encoding & dataset builders\n",
    "    # ===========================================================\n",
    "    def _encode_frame(self, X: pd.DataFrame, fit_encoders: bool) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Label-encode object columns; numeric columns stay as they are.\n",
    "        Unknown categories map to the most frequent (for transform phase).\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        X = X.fillna('__MISSING__')\n",
    "\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype == 'object' or X[col].dtype.name == 'category':\n",
    "                if fit_encoders:\n",
    "                    if col not in self.label_encoders:\n",
    "                        self.label_encoders[col] = LabelEncoder()\n",
    "                    X[col] = self.label_encoders[col].fit_transform(X[col].astype(str))\n",
    "                else:\n",
    "                    if col in self.label_encoders:\n",
    "                        le = self.label_encoders[col]\n",
    "                        x_str = X[col].astype(str)\n",
    "                        known = set(le.classes_)\n",
    "                        unique_vals = set(x_str.unique())\n",
    "                        unseen = unique_vals - known\n",
    "                        if unseen:\n",
    "                            most_freq = le.classes_[0]\n",
    "                            for u in unseen:\n",
    "                                x_str = x_str.replace(u, most_freq)\n",
    "                        X[col] = le.transform(x_str)\n",
    "                    else:\n",
    "                        # Temporary fit (should not happen in normal flow)\n",
    "                        temp = LabelEncoder()\n",
    "                        X[col] = temp.fit_transform(X[col].astype(str))\n",
    "            else:\n",
    "                # ensure numeric\n",
    "                X[col] = pd.to_numeric(X[col], errors='coerce').fillna(0.0)\n",
    "        return X\n",
    "\n",
    "    def _prepare_feature_matrix(\n",
    "        self,\n",
    "        data: pd.DataFrame,\n",
    "        base_features: List[str],\n",
    "        fit_encoders: bool\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Build feature matrix = base_features + engineered_feature_names,\n",
    "        then encode categoricals.\n",
    "        \"\"\"\n",
    "        eng = self._engineer_features_frame(data)\n",
    "        # Ensure we keep only engineered features we declared (order matters)\n",
    "        eng = eng.reindex(columns=self.engineered_feature_names)\n",
    "        X = pd.concat([data[base_features], eng], axis=1)\n",
    "        X = self._encode_frame(X, fit_encoders=fit_encoders)\n",
    "        return X\n",
    "\n",
    "    def _prepare_targets(self, data: pd.DataFrame, fit_encoders: bool) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Encode AH and AI targets.\"\"\"\n",
    "        ah = data['835 report Qualifier Code / RARC codes'].fillna('__MISSING__').astype(str)\n",
    "        ai = data['Expected Outcomes Error category'].fillna('__MISSING__').astype(str)\n",
    "        if fit_encoders:\n",
    "            self.target_encoders['AH'] = LabelEncoder()\n",
    "            self.target_encoders['AI'] = LabelEncoder()\n",
    "            y_ah = self.target_encoders['AH'].fit_transform(ah)\n",
    "            y_ai = self.target_encoders['AI'].fit_transform(ai)\n",
    "        else:\n",
    "            y_ah = self.target_encoders['AH'].transform(ah)\n",
    "            y_ai = self.target_encoders['AI'].transform(ai)\n",
    "        return y_ah, y_ai\n",
    "\n",
    "    # ===========================================================\n",
    "    # Training, validation & SHAP\n",
    "    # ===========================================================\n",
    "    def train_models(self):\n",
    "        \"\"\"Train HistGBDT models and validate.\"\"\"\n",
    "        print(\"\\n🎓 Training ML models...\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        # Build training matrices\n",
    "        X1_train = self._prepare_feature_matrix(self.train_data, self.model1_base_features, fit_encoders=True)\n",
    "        X2_train = self._prepare_feature_matrix(self.train_data, self.model2_base_features, fit_encoders=False)\n",
    "        y_ah_train, y_ai_train = self._prepare_targets(self.train_data, fit_encoders=True)\n",
    "\n",
    "        print(f\"📊 Training shapes: X1={X1_train.shape}, X2={X2_train.shape}\")\n",
    "        print(f\"   AH classes: {len(np.unique(y_ah_train))}, AI classes: {len(np.unique(y_ai_train))}\")\n",
    "\n",
    "        # Stratified split\n",
    "        X1_tr, X1_val, y_ah_tr, y_ah_val = train_test_split(\n",
    "            X1_train, y_ah_train, test_size=0.2, random_state=42, stratify=y_ah_train\n",
    "        )\n",
    "        _, _, y_ai_tr, y_ai_val = train_test_split(\n",
    "            X1_train, y_ai_train, test_size=0.2, random_state=42, stratify=y_ai_train\n",
    "        )\n",
    "        X2_tr, X2_val, _, _ = train_test_split(\n",
    "            X2_train, y_ah_train, test_size=0.2, random_state=42, stratify=y_ah_train\n",
    "        )\n",
    "\n",
    "        print(\"\\n🔸 Training Model 1 (Full features + engineered)...\")\n",
    "        self.model1_ah.fit(X1_tr, y_ah_tr)\n",
    "        self.model1_ai.fit(X1_tr, y_ai_tr)\n",
    "\n",
    "        print(\"🔸 Training Model 2 (Limited features + engineered)...\")\n",
    "        self.model2_ah.fit(X2_tr, y_ah_tr)\n",
    "        self.model2_ai.fit(X2_tr, y_ai_tr)\n",
    "\n",
    "        print(\"✅ Models trained\")\n",
    "\n",
    "        # Validation\n",
    "        print(\"\\n📊 Validation Performance:\")\n",
    "        y_ah_pred1 = self.model1_ah.predict(X1_val)\n",
    "        y_ai_pred1 = self.model1_ai.predict(X1_val)\n",
    "        print(f\"\\n🔸 Model 1 AH Accuracy: {accuracy_score(y_ah_val, y_ah_pred1):.3f}\")\n",
    "        print(f\"🔸 Model 1 AI Accuracy: {accuracy_score(y_ai_val, y_ai_pred1):.3f}\")\n",
    "\n",
    "        y_ah_pred2 = self.model2_ah.predict(X2_val)\n",
    "        y_ai_pred2 = self.model2_ai.predict(X2_val)\n",
    "        print(f\"🔸 Model 2 AH Accuracy: {accuracy_score(y_ah_val, y_ah_pred2):.3f}\")\n",
    "        print(f\"🔸 Model 2 AI Accuracy: {accuracy_score(y_ai_val, y_ai_pred2):.3f}\")\n",
    "\n",
    "        self._create_confusion_matrices(y_ah_val, y_ai_val, y_ah_pred1, y_ai_pred1, y_ah_pred2, y_ai_pred2)\n",
    "\n",
    "        # Cross-validation\n",
    "        print(\"\\n📈 Cross-validation (5-fold) on full training set:\")\n",
    "        try:\n",
    "            cv1_ah = cross_val_score(self.model1_ah, X1_train, y_ah_train, cv=5)\n",
    "            cv1_ai = cross_val_score(self.model1_ai, X1_train, y_ai_train, cv=5)\n",
    "            cv2_ah = cross_val_score(self.model2_ah, X2_train, y_ah_train, cv=5)\n",
    "            cv2_ai = cross_val_score(self.model2_ai, X2_train, y_ai_train, cv=5)\n",
    "            print(f\"   Model 1 AH: {cv1_ah.mean():.3f} ± {cv1_ah.std():.3f}\")\n",
    "            print(f\"   Model 1 AI: {cv1_ai.mean():.3f} ± {cv1_ai.std():.3f}\")\n",
    "            print(f\"   Model 2 AH: {cv2_ah.mean():.3f} ± {cv2_ah.std():.3f}\")\n",
    "            print(f\"   Model 2 AI: {cv2_ai.mean():.3f} ± {cv2_ai.std():.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ CV failed: {e}\")\n",
    "\n",
    "        # Retrain on full data\n",
    "        print(\"\\n🔄 Retraining on full dataset...\")\n",
    "        self.model1_ah.fit(X1_train, y_ah_train)\n",
    "        self.model1_ai.fit(X1_train, y_ai_train)\n",
    "        self.model2_ah.fit(X2_train, y_ah_train)\n",
    "        self.model2_ai.fit(X2_train, y_ai_train)\n",
    "\n",
    "    def _create_confusion_matrices(self, y_ah_true, y_ai_true, y_ah_pred1, y_ai_pred1, y_ah_pred2, y_ai_pred2):\n",
    "        \"\"\"Create confusion matrices and classification reports.\"\"\"\n",
    "        print(\"\\n📊 Creating confusion matrices...\")\n",
    "        try:\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "            ah_labels = self.target_encoders['AH'].classes_\n",
    "            ai_labels = self.target_encoders['AI'].classes_\n",
    "\n",
    "            # Model 1 AH\n",
    "            cm1_ah = confusion_matrix(y_ah_true, y_ah_pred1)\n",
    "            sns.heatmap(cm1_ah, annot=True, fmt='d', cmap='Blues',\n",
    "                        xticklabels=ah_labels, yticklabels=ah_labels, ax=axes[0,0])\n",
    "            axes[0,0].set_title('Model 1 - AH (Full+Eng)', fontweight='bold')\n",
    "            axes[0,0].set_xlabel('Predicted'); axes[0,0].set_ylabel('Actual')\n",
    "\n",
    "            # Model 1 AI\n",
    "            cm1_ai = confusion_matrix(y_ai_true, y_ai_pred1)\n",
    "            sns.heatmap(cm1_ai, annot=True, fmt='d', cmap='Reds',\n",
    "                        xticklabels=[l[:15]+'...' if len(l)>15 else l for l in ai_labels],\n",
    "                        yticklabels=[l[:15]+'...' if len(l)>15 else l for l in ai_labels],\n",
    "                        ax=axes[0,1])\n",
    "            axes[0,1].set_title('Model 1 - AI (Full+Eng)', fontweight='bold')\n",
    "            axes[0,1].set_xlabel('Predicted'); axes[0,1].set_ylabel('Actual')\n",
    "            axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "            # Model 2 AH\n",
    "            cm2_ah = confusion_matrix(y_ah_true, y_ah_pred2)\n",
    "            sns.heatmap(cm2_ah, annot=True, fmt='d', cmap='Greens',\n",
    "                        xticklabels=ah_labels, yticklabels=ah_labels, ax=axes[1,0])\n",
    "            axes[1,0].set_title('Model 2 - AH (A–T+Eng)', fontweight='bold')\n",
    "            axes[1,0].set_xlabel('Predicted'); axes[1,0].set_ylabel('Actual')\n",
    "\n",
    "            # Model 2 AI\n",
    "            cm2_ai = confusion_matrix(y_ai_true, y_ai_pred2)\n",
    "            sns.heatmap(cm2_ai, annot=True, fmt='d', cmap='Purples',\n",
    "                        xticklabels=[l[:15]+'...' if len(l)>15 else l for l in ai_labels],\n",
    "                        yticklabels=[l[:15]+'...' if len(l)>15 else l for l in ai_labels],\n",
    "                        ax=axes[1,1])\n",
    "            axes[1,1].set_title('Model 2 - AI (A–T+Eng)', fontweight='bold')\n",
    "            axes[1,1].set_xlabel('Predicted'); axes[1,1].set_ylabel('Actual')\n",
    "            axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('confusion_matrices_validation.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "            plt.close()\n",
    "            print(\"   ✅ Saved: confusion_matrices_validation.png\")\n",
    "\n",
    "            # Detailed reports\n",
    "            print(\"\\n📋 Classification Reports:\")\n",
    "            print(\"\\n🔸 Model 1 AH\")\n",
    "            print(classification_report(y_ah_true, y_ah_pred1, target_names=ah_labels))\n",
    "            print(\"\\n🔸 Model 1 AI\")\n",
    "            print(classification_report(y_ai_true, y_ai_pred1, target_names=self.target_encoders['AI'].classes_))\n",
    "            print(\"\\n🔸 Model 2 AH\")\n",
    "            print(classification_report(y_ah_true, y_ah_pred2, target_names=ah_labels))\n",
    "            print(\"\\n🔸 Model 2 AI\")\n",
    "            print(classification_report(y_ai_true, y_ai_pred2, target_names=self.target_encoders['AI'].classes_))\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Confusion matrix creation warning: {e}\")\n",
    "\n",
    "    def analyze_feature_importance(self):\n",
    "        \"\"\"Print simple feature importances (HGBDT has no native .feature_importances_, but we can approximate via permutation if needed).\"\"\"\n",
    "        print(\"\\n🔍 Analyzing feature importance...\")\n",
    "        print(\"=\" * 40)\n",
    "        # For HistGBDT we don't have native feature_importances_. We’ll compute via permutation importance quickly if needed,\n",
    "        # but to keep compatibility and speed, we’ll proxy with SHAP (permutation) below.\n",
    "\n",
    "        # Build matrices (no refit)\n",
    "        X1_train = self._prepare_feature_matrix(self.train_data, self.model1_base_features, fit_encoders=False)\n",
    "        X2_train = self._prepare_feature_matrix(self.train_data, self.model2_base_features, fit_encoders=False)\n",
    "\n",
    "        # Create SHAP explainers using permutation fallback (works for any model)\n",
    "        print(\"\\n🔍 Creating SHAP explainers...\")\n",
    "        try:\n",
    "            # Use a small background for speed\n",
    "            bg1 = X1_train.sample(min(200, len(X1_train)), random_state=42)\n",
    "            bg2 = X2_train.sample(min(200, len(X2_train)), random_state=42)\n",
    "\n",
    "            # Use predict_proba for multiclass SHAP (permutation)\n",
    "            self.explainers['model1_ah'] = shap.Explainer(self.model1_ah.predict_proba, bg1, algorithm=\"permutation\")\n",
    "            self.explainers['model1_ai'] = shap.Explainer(self.model1_ai.predict_proba, bg1, algorithm=\"permutation\")\n",
    "            self.explainers['model2_ah'] = shap.Explainer(self.model2_ah.predict_proba, bg2, algorithm=\"permutation\")\n",
    "            self.explainers['model2_ai'] = shap.Explainer(self.model2_ai.predict_proba, bg2, algorithm=\"permutation\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ SHAP warning: {e}\")\n",
    "\n",
    "        # Try to create SHAP summary plots (robust to failures)\n",
    "        self._create_shap_visualizations(X1_train, X2_train)\n",
    "\n",
    "    def _shap_values_to_2d(self, explainer, X_sample, predicted_class: int) -> Optional[np.ndarray]:\n",
    "        \"\"\"\n",
    "        Turn various SHAP outputs (Explanation/list/ndarray) into 1D per-feature vector for a single sample.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Using explainer(X) returns shap.Explanation for permutation explainer\n",
    "            exp = explainer(X_sample)\n",
    "            vals = exp.values  # shape could be (n, n_classes, n_features) or (n, n_features)\n",
    "            # We expect n=1 here\n",
    "            if vals.ndim == 3:\n",
    "                # (1, n_classes, n_features) -> pick predicted_class\n",
    "                if predicted_class < vals.shape[1]:\n",
    "                    return vals[0, predicted_class, :]\n",
    "                return vals[0, 0, :]\n",
    "            elif vals.ndim == 2:\n",
    "                # (1, n_features)\n",
    "                return vals[0, :]\n",
    "            elif vals.ndim == 1:\n",
    "                return vals\n",
    "            else:\n",
    "                return None\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _create_shap_visualizations(self, X1_train, X2_train):\n",
    "\n",
    "        print(\"\\n📊 Generating SHAP visualizations...\")\n",
    "        try:\n",
    "            X1_sample = X1_train.sample(min(200, len(X1_train)), random_state=42)\n",
    "            X2_sample = X2_train.sample(min(200, len(X2_train)), random_state=42)\n",
    "    \n",
    "            # AH/AI названия классов (для файлов per_class)\n",
    "            ah_classes = list(self.target_encoders['AH'].classes_) if 'AH' in self.target_encoders else []\n",
    "            ai_classes = list(self.target_encoders['AI'].classes_) if 'AI' in self.target_encoders else []\n",
    "    \n",
    "            # По умолчанию сохраняем один файл на модель — по предсказанному классу\n",
    "            # per_class=True добавит ещё отдельные файлы по каждому классу (опционально)\n",
    "            self._beeswarm_for_predicted_class('model1_ah', self.model1_ah, X1_sample,\n",
    "                                               'SHAP Summary - Model 1 (AH)', 'shap_model1_ah_summary.png')\n",
    "            self._beeswarm_for_predicted_class('model1_ai', self.model1_ai, X1_sample,\n",
    "                                               'SHAP Summary - Model 1 (AI)', 'shap_model1_ai_summary.png')\n",
    "            self._beeswarm_for_predicted_class('model2_ah', self.model2_ah, X2_sample,\n",
    "                                               'SHAP Summary - Model 2 (AH)', 'shap_model2_ah_summary.png')\n",
    "            self._beeswarm_for_predicted_class('model2_ai', self.model2_ai, X2_sample,\n",
    "                                               'SHAP Summary - Model 2 (AI)', 'shap_model2_ai_summary.png')\n",
    "\n",
    "    \n",
    "\n",
    "            def _mean_abs_predclass_importance(explainer_key, X_sample, model, feat_names):\n",
    "                if explainer_key not in self.explainers:\n",
    "                    return None\n",
    "                exp = self.explainers[explainer_key](X_sample)\n",
    "                vals = exp.values\n",
    "                if vals.ndim == 3:\n",
    "                    proba = model.predict_proba(X_sample)\n",
    "                    idx = np.argmax(proba, axis=1)\n",
    "                    values_2d = np.stack([vals[i, idx[i], :] for i in range(vals.shape[0])], axis=0)\n",
    "                    imp = np.mean(np.abs(values_2d), axis=0)\n",
    "                elif vals.ndim == 2:\n",
    "                    imp = np.mean(np.abs(vals), axis=0)\n",
    "                else:\n",
    "                    return None\n",
    "                return pd.DataFrame({'feature': feat_names, 'importance': imp}).sort_values('importance', ascending=False)\n",
    "    \n",
    "            f1 = _mean_abs_predclass_importance('model1_ah', X1_sample, self.model1_ah, list(X1_train.columns))\n",
    "            f2 = _mean_abs_predclass_importance('model1_ai', X1_sample, self.model1_ai, list(X1_train.columns))\n",
    "            f3 = _mean_abs_predclass_importance('model2_ah', X2_sample, self.model2_ah, list(X2_train.columns))\n",
    "            f4 = _mean_abs_predclass_importance('model2_ai', X2_sample, self.model2_ai, list(X2_train.columns))\n",
    "    \n",
    "            fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "            for ax, df_imp, title in [\n",
    "                (axes[0,0], f1, 'Model 1 - AH (Top 15)'),\n",
    "                (axes[0,1], f2, 'Model 1 - AI (Top 15)'),\n",
    "                (axes[1,0], f3, 'Model 2 - AH (Top 15)'),\n",
    "                (axes[1,1], f4, 'Model 2 - AI (Top 15)'),\n",
    "            ]:\n",
    "                try:\n",
    "                    top = df_imp.head(15).sort_values('importance', ascending=True) if df_imp is not None else None\n",
    "                    if top is not None and len(top) > 0:\n",
    "                        ax.barh(top['feature'], top['importance'])\n",
    "                        ax.set_title(title, fontweight='bold')\n",
    "                except Exception:\n",
    "                    ax.text(0.5, 0.5, 'N/A', ha='center', va='center')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('feature_importance_comparison.png', dpi=300, bbox_inches='tight', facecolor='white')\n",
    "            plt.close()\n",
    "            print(\"   ✅ feature_importance_comparison.png saved\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ SHAP visualization warning: {e}\")\n",
    "\n",
    "\n",
    "    # ===========================================================\n",
    "    # Rule head for AI (and implied AH)\n",
    "    # ===========================================================\n",
    "    def _rule_predict_ai(self, xrow: Dict[str, float]) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Deterministic rule-based AI classification, mirroring generator logic.\n",
    "        Returns AI string or None if no rule fires.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if xrow.get('_rule_wac18_immunoDerm_qty1', 0) == 1:\n",
    "                return 'Write off due to WAC UNIT PRICE diff from WAC Medispan'\n",
    "            if xrow.get('_rule_fixed_slow_q14_qty1', 0) == 1:\n",
    "                return 'Sent to collection due to MFG timing difference'\n",
    "            if xrow.get('_npi_profile_sloppy', 0) == 1 and xrow.get('_disc_lt3', 0) == 1 \\\n",
    "               and xrow.get('_is_cardio_or_diab', 0) == 1 and xrow.get('_est_lt_1200', 0) == 1:\n",
    "                return 'Write off due to payment applied to the wrong claim'\n",
    "            return None\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    # ===========================================================\n",
    "    # Insights (best-effort SHAP) and fallbacks\n",
    "    # ===========================================================\n",
    "    def _test_shap_explainers(self, X_test: pd.DataFrame, model_prefix: str) -> bool:\n",
    "        \"\"\"Quick test if SHAP explainers exist and work.\"\"\"\n",
    "        try:\n",
    "            print(f\"   Testing SHAP for {model_prefix}...\")\n",
    "            X_sample = X_test.iloc[[0]]\n",
    "            ah_exp = self.explainers[f'{model_prefix}_ah'](X_sample)\n",
    "            ai_exp = self.explainers[f'{model_prefix}_ai'](X_sample)\n",
    "            # print shapes (optional)\n",
    "            v_ah = ah_exp.values\n",
    "            v_ai = ai_exp.values\n",
    "            print(f\"   ✅ SHAP test ok: AH {np.array(v_ah).shape}, AI {np.array(v_ai).shape}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ SHAP test failed for {model_prefix}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _generate_fallback_insights(\n",
    "        self, X: pd.DataFrame, feature_names: List[str],\n",
    "        pred_ah_label: str, pred_ai_label: str, rule_note: Optional[str] = None\n",
    "    ) -> str:\n",
    "        \"\"\"\n",
    "        Lightweight, robust fallback insight string.\n",
    "        \"\"\"\n",
    "        if rule_note:\n",
    "            return f\"Rule-based: {pred_ai_label} → AH={pred_ah_label} ({rule_note})\"\n",
    "        return f\"ML Prediction: AH='{pred_ah_label}', AI='{pred_ai_label}' (no SHAP)\"\n",
    "\n",
    "    # ===========================================================\n",
    "    # Inference on Step 2 / Step 3\n",
    "    # ===========================================================\n",
    "    def apply_predictions(self):\n",
    "        \"\"\"Apply predictions to Step 2 (Model 1) and Step 3 (Model 2) with rule overrides.\"\"\"\n",
    "        print(\"\\n🔮 Applying predictions...\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        # --- Step 2 using Model 1 (full features) ---\n",
    "        print(\"🔸 Model 1 → Step 2 (Suggest)\")\n",
    "        X_step2 = self._prepare_feature_matrix(self.step2_data, self.model1_base_features, fit_encoders=False)\n",
    "        pred_ah_step2 = self.model1_ah.predict(X_step2)\n",
    "        pred_ai_step2 = self.model1_ai.predict(X_step2)\n",
    "        print(f\"   Predictions: AH={len(pred_ah_step2)}, AI={len(pred_ai_step2)}\")\n",
    "\n",
    "        shap_ok_1 = self._test_shap_explainers(X_step2, 'model1') if 'model1_ah' in self.explainers else False\n",
    "        step2_indices = list(self.step2_data.index)\n",
    "        insights_step2 = []\n",
    "\n",
    "        for i, idx in enumerate(step2_indices):\n",
    "            if i % 50 == 0:\n",
    "                print(f\"   Step 2 progress: {i+1}/{len(step2_indices)}\")\n",
    "\n",
    "            # Decode labels\n",
    "            ah_label = self.target_encoders['AH'].inverse_transform([pred_ah_step2[i]])[0]\n",
    "            ai_label = self.target_encoders['AI'].inverse_transform([pred_ai_step2[i]])[0]\n",
    "\n",
    "            # Rule override (deterministic)\n",
    "            xrow = dict(zip(X_step2.columns, X_step2.iloc[i].tolist()))\n",
    "            ai_rule = self._rule_predict_ai(xrow)\n",
    "            if ai_rule:\n",
    "                ai_label = ai_rule\n",
    "                ah_label = self.mapping_primary_ah[ai_rule]\n",
    "                insight = self._generate_fallback_insights(X_step2, list(X_step2.columns), ah_label, ai_label,\n",
    "                                                           rule_note=\"matched generator pattern\")\n",
    "            else:\n",
    "                # Try SHAP (best-effort), else fallback\n",
    "                insight = self._generate_fallback_insights(X_step2, list(X_step2.columns), ah_label, ai_label)\n",
    "\n",
    "            # Write back\n",
    "            self.df.at[idx, '835 report Qualifier Code / RARC codes'] = ah_label\n",
    "            self.df.at[idx, 'Expected Outcomes Error category'] = ai_label\n",
    "            self.df.at[idx, ' Questions/comments'] = insight\n",
    "            insights_step2.append(insight)\n",
    "\n",
    "        print(f\"✅ Updated {len(step2_indices)} Step 2 rows\")\n",
    "\n",
    "        # --- Step 3 using Model 2 (A–T features only) ---\n",
    "        print(\"🔸 Model 2 → Step 3 (Forecast)\")\n",
    "        X_step3 = self._prepare_feature_matrix(self.step3_data, self.model2_base_features, fit_encoders=False)\n",
    "        pred_ah_step3 = self.model2_ah.predict(X_step3)\n",
    "        pred_ai_step3 = self.model2_ai.predict(X_step3)\n",
    "        print(f\"   Predictions: AH={len(pred_ah_step3)}, AI={len(pred_ai_step3)}\")\n",
    "\n",
    "        shap_ok_2 = self._test_shap_explainers(X_step3, 'model2') if 'model2_ah' in self.explainers else False\n",
    "        step3_indices = list(self.step3_data.index)\n",
    "        insights_step3 = []\n",
    "\n",
    "        for i, idx in enumerate(step3_indices):\n",
    "            if i % 50 == 0:\n",
    "                print(f\"   Step 3 progress: {i+1}/{len(step3_indices)}\")\n",
    "\n",
    "            ah_label = self.target_encoders['AH'].inverse_transform([pred_ah_step3[i]])[0]\n",
    "            ai_label = self.target_encoders['AI'].inverse_transform([pred_ai_step3[i]])[0]\n",
    "\n",
    "            xrow = dict(zip(X_step3.columns, X_step3.iloc[i].tolist()))\n",
    "            ai_rule = self._rule_predict_ai(xrow)\n",
    "            if ai_rule:\n",
    "                ai_label = ai_rule\n",
    "                ah_label = self.mapping_primary_ah[ai_rule]\n",
    "                insight = self._generate_fallback_insights(X_step3, list(X_step3.columns), ah_label, ai_label,\n",
    "                                                           rule_note=\"matched generator pattern\")\n",
    "            else:\n",
    "                insight = self._generate_fallback_insights(X_step3, list(X_step3.columns), ah_label, ai_label)\n",
    "\n",
    "            self.df.at[idx, '835 report Qualifier Code / RARC codes'] = ah_label\n",
    "            self.df.at[idx, 'Expected Outcomes Error category'] = ai_label\n",
    "            self.df.at[idx, ' Questions/comments'] = insight\n",
    "            insights_step3.append(insight)\n",
    "\n",
    "        print(f\"✅ Updated {len(step3_indices)} Step 3 rows\")\n",
    "\n",
    "        # Summaries\n",
    "        print(\"\\n📊 Prediction Summary:\")\n",
    "        ah_s2 = dict(zip(*np.unique(self.df.loc[step2_indices, '835 report Qualifier Code / RARC codes'], return_counts=True)))\n",
    "        ai_s2 = dict(zip(*np.unique(self.df.loc[step2_indices, 'Expected Outcomes Error category'], return_counts=True)))\n",
    "        ah_s3 = dict(zip(*np.unique(self.df.loc[step3_indices, '835 report Qualifier Code / RARC codes'], return_counts=True)))\n",
    "        ai_s3 = dict(zip(*np.unique(self.df.loc[step3_indices, 'Expected Outcomes Error category'], return_counts=True)))\n",
    "        print(f\"Step 2 AH: {ah_s2}\")\n",
    "        print(f\"Step 2 AI: {ai_s2}\")\n",
    "        print(f\"Step 3 AH: {ah_s3}\")\n",
    "        print(f\"Step 3 AI: {ai_s3}\")\n",
    "\n",
    "        # Sample\n",
    "        print(f\"\\n📋 Sample insights:\")\n",
    "        if len(insights_step2) > 0:\n",
    "            print(f\"Step 2 sample: {insights_step2[0]}\")\n",
    "        if len(insights_step3) > 0:\n",
    "            print(f\"Step 3 sample: {insights_step3[0]}\")\n",
    "\n",
    "    # ===========================================================\n",
    "    # Waterfalls (best-effort; fallbacks to simple plots)\n",
    "    # ===========================================================\n",
    "    def create_individual_waterfall_plots(self, row_indices=[0, 1], output_dir='individual_shap_plots'):\n",
    "        \"\"\"SHAP пер-строчные waterfall/бар-плоты для Step2 (Model1) и Step3 (Model2).\"\"\"\n",
    "        print(f\"\\n📊 Creating individual SHAP plots for rows {row_indices}...\")\n",
    "        import os\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "        # гарантируем, что объяснители уже есть\n",
    "        if not self.explainers:\n",
    "            self.analyze_feature_importance()\n",
    "    \n",
    "        # --- Step 2 (Model 1) ---\n",
    "        if len(self.step2_data) > 0:\n",
    "            print(\"🔸 Waterfalls for Step 2 (Model 1)\")\n",
    "            X_step2 = self._prepare_feature_matrix(self.step2_data, self.model1_base_features, fit_encoders=False)\n",
    "            for row_idx in row_indices:\n",
    "                if row_idx >= len(self.step2_data):\n",
    "                    print(f\"   ⚠️ Row {row_idx} out of range for Step 2\")\n",
    "                    continue\n",
    "                try:\n",
    "                    x1 = X_step2.iloc[[row_idx]]\n",
    "                    pred_ah = int(self.model1_ah.predict(x1)[0])\n",
    "                    pred_ai = int(self.model1_ai.predict(x1)[0])\n",
    "                    ah_label = self.target_encoders['AH'].inverse_transform([pred_ah])[0]\n",
    "                    ai_label = self.target_encoders['AI'].inverse_transform([pred_ai])[0]\n",
    "    \n",
    "                    self._save_shap_row_waterfall(\n",
    "                        x1, 'model1_ah', pred_ah,\n",
    "                        f'{output_dir}/step2_row_{row_idx}_AH_shap.png',\n",
    "                        f'Step 2 Row {row_idx}: AH = {ah_label}'\n",
    "                    )\n",
    "                    self._save_shap_row_waterfall(\n",
    "                        x1, 'model1_ai', pred_ai,\n",
    "                        f'{output_dir}/step2_row_{row_idx}_AI_shap.png',\n",
    "                        f'Step 2 Row {row_idx}: AI = {ai_label}'\n",
    "                    )\n",
    "                    print(f\"   ✅ Step 2 Row {row_idx} done\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ❌ Step 2 Row {row_idx} failed: {e}\")\n",
    "    \n",
    "        # --- Step 3 (Model 2) ---\n",
    "        if len(self.step3_data) > 0:\n",
    "            print(\"🔸 Waterfalls for Step 3 (Model 2)\")\n",
    "            X_step3 = self._prepare_feature_matrix(self.step3_data, self.model2_base_features, fit_encoders=False)\n",
    "            for row_idx in row_indices:\n",
    "                if row_idx >= len(self.step3_data):\n",
    "                    print(f\"   ⚠️ Row {row_idx} out of range for Step 3\")\n",
    "                    continue\n",
    "                try:\n",
    "                    x2 = X_step3.iloc[[row_idx]]\n",
    "                    pred_ah = int(self.model2_ah.predict(x2)[0])\n",
    "                    pred_ai = int(self.model2_ai.predict(x2)[0])\n",
    "                    ah_label = self.target_encoders['AH'].inverse_transform([pred_ah])[0]\n",
    "                    ai_label = self.target_encoders['AI'].inverse_transform([pred_ai])[0]\n",
    "    \n",
    "                    self._save_shap_row_waterfall(\n",
    "                        x2, 'model2_ah', pred_ah,\n",
    "                        f'{output_dir}/step3_row_{row_idx}_AH_shap.png',\n",
    "                        f'Step 3 Row {row_idx}: AH = {ah_label}'\n",
    "                    )\n",
    "                    self._save_shap_row_waterfall(\n",
    "                        x2, 'model2_ai', pred_ai,\n",
    "                        f'{output_dir}/step3_row_{row_idx}_AI_shap.png',\n",
    "                        f'Step 3 Row {row_idx}: AI = {ai_label}'\n",
    "                    )\n",
    "                    print(f\"   ✅ Step 3 Row {row_idx} done\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ❌ Step 3 Row {row_idx} failed: {e}\")\n",
    "\n",
    "\n",
    "    def _save_simple_row_plot(self, X_single: pd.DataFrame, feat_names: List[str], save_path: str,\n",
    "                              title: str, original_row: pd.Series):\n",
    "        \"\"\"Simple per-row bar chart as a robust fallback for waterfall.\"\"\"\n",
    "        try:\n",
    "            vals = X_single.iloc[0].values\n",
    "            short_names = [f.split()[-1] if ' ' in f else f for f in feat_names]\n",
    "            plt.figure(figsize=(12, 7))\n",
    "            plt.barh(range(len(vals)), vals)\n",
    "            plt.yticks(range(len(vals)), short_names)\n",
    "            plt.title(title, fontweight='bold')\n",
    "            drug = original_row.get('DRUG NAME N/A', 'Unknown Drug')\n",
    "            mfg = original_row.get('MFG NAME MFG Name', 'Unknown Manufacturer')\n",
    "            plt.figtext(0.02, 0.02, f'Drug: {drug} | MFG: {mfg}', fontsize=10, style='italic')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')\n",
    "            plt.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ===========================================================\n",
    "    # Save results\n",
    "    # ===========================================================\n",
    "    def save_results(self, output_file='Pharma_poc_ml_results.xlsx'):\n",
    "        \"\"\"Write the updated df to Excel with preserved structure.\"\"\"\n",
    "        print(f\"\\n💾 Saving results to {output_file}...\")\n",
    "        try:\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "                self.df.to_excel(writer, sheet_name='Scenario Data Structure', index=False)\n",
    "                ws = writer.sheets['Scenario Data Structure']\n",
    "                for column in ws.columns:\n",
    "                    max_len = 0\n",
    "                    col_letter = column[0].column_letter\n",
    "                    for cell in column:\n",
    "                        try:\n",
    "                            max_len = max(max_len, len(str(cell.value)))\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                    ws.column_dimensions[col_letter].width = min(max_len + 2, 100)\n",
    "            print(\"✅ Results saved\")\n",
    "\n",
    "            # Best-effort individual plots\n",
    "            print(\"\\n🎨 Creating individual waterfall plots...\")\n",
    "            self.create_individual_waterfall_plots([0, 1], 'individual_waterfall_plots')\n",
    "\n",
    "            # Print sample insights\n",
    "            print(\"\\n📋 Sample insights from predictions:\")\n",
    "            print(\"=\" * 50)\n",
    "            try:\n",
    "                step2_sample = self.df[self.df['Case'] == 'Step 2 - Suggest'].iloc[0]\n",
    "                print(\"🔸 Step 2 Sample:\")\n",
    "                print(f\"   Drug: {step2_sample['DRUG NAME N/A']}\")\n",
    "                print(f\"   Predicted AH: {step2_sample['835 report Qualifier Code / RARC codes']}\")\n",
    "                print(f\"   Predicted AI: {step2_sample['Expected Outcomes Error category']}\")\n",
    "                print(f\"   Insight: {str(step2_sample[' Questions/comments'])[:150]}...\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                step3_sample = self.df[self.df['Case'] == 'Step 3 - Forecast'].iloc[0]\n",
    "                print(\"\\n🔸 Step 3 Sample:\")\n",
    "                print(f\"   Drug: {step3_sample['DRUG NAME N/A']}\")\n",
    "                print(f\"   Predicted AH: {step3_sample['835 report Qualifier Code / RARC codes']}\")\n",
    "                print(f\"   Predicted AI: {step3_sample['Expected Outcomes Error category']}\")\n",
    "                print(f\"   Insight: {str(step3_sample[' Questions/comments'])[:150]}...\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            print(\"\\n📊 Artifacts:\")\n",
    "            print(\"   📈 shap_model1_ah_summary.png\")\n",
    "            print(\"   📈 shap_model1_ai_summary.png\")\n",
    "            print(\"   📈 shap_model2_ah_summary.png\")\n",
    "            print(\"   📈 shap_model2_ai_summary.png\")\n",
    "            print(\"   📊 feature_importance_comparison.png\")\n",
    "            print(\"   🎯 confusion_matrices_validation.png\")\n",
    "            print(\"   🎯 individual_waterfall_plots/ (PNG per-row)\")\n",
    "\n",
    "            return output_file\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error saving results: {e}\")\n",
    "            return None\n",
    "\n",
    "    # ===========================================================\n",
    "    # Orchestration\n",
    "    # ===========================================================\n",
    "    def run_full_pipeline(self, output_file='Pharma_poc_ml_results.xlsx'):\n",
    "        \"\"\"Run the complete upgraded pipeline.\"\"\"\n",
    "        print(\"🚀 STARTING Pharma PoC ML PIPELINE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"📋 Pipeline steps:\")\n",
    "        print(\"   1. Load and prepare data\")\n",
    "        print(\"   2. Identify significant features\")\n",
    "        print(\"   3. Train models (AH, AI)\")\n",
    "        print(\"   4. Analyze feature importance & SHAP\")\n",
    "        print(\"   5. Apply predictions with insights (rule-first overrides)\")\n",
    "        print(\"   6. Save results to Excel\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        self.load_and_prepare_data()\n",
    "        self.identify_significant_features()\n",
    "        self.train_models()\n",
    "        self.analyze_feature_importance()\n",
    "        self.apply_predictions()\n",
    "        result_file = self.save_results(output_file)\n",
    "\n",
    "        print(\"\\n🎉 PIPELINE COMPLETED\")\n",
    "        print(f\"📁 Results saved to: {result_file}\")\n",
    "        print(\"=\" * 60)\n",
    "        return result_file\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION HELPERS (same API)\n",
    "# =============================================================================\n",
    "\n",
    "def run_Pharma_ml_analysis(input_file='Pharma_poc_custom_1000_scored.xlsx', output_file='Pharma_poc_ml_results.xlsx'):\n",
    "    \"\"\"\n",
    "    Main function to run Pharma PoC ML analysis.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Input Excel file with Pharma data\n",
    "        output_file (str): Output Excel file with ML predictions and insights\n",
    "\n",
    "    Returns:\n",
    "        str: Path to output file\n",
    "    \"\"\"\n",
    "    pipeline = PharmaPoCMLPipeline(input_file)\n",
    "    result_file = pipeline.run_full_pipeline(output_file)\n",
    "    return result_file\n",
    "\n",
    "\n",
    "def create_custom_waterfall_plots(input_file, row_indices, output_dir='custom_waterfalls'):\n",
    "    \"\"\"\n",
    "    Create simple per-row plots (fallback waterfalls) for specified rows.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to input data/results file\n",
    "        row_indices (list): Row indices to visualize\n",
    "        output_dir (str): Directory to save plots\n",
    "\n",
    "    Returns:\n",
    "        bool: Success status\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"🎨 Creating custom waterfalls for rows {row_indices}...\")\n",
    "        pipeline = PharmaPoCMLPipeline(input_file)\n",
    "        pipeline.load_and_prepare_data()\n",
    "        pipeline.identify_significant_features()\n",
    "        pipeline.train_models()\n",
    "        pipeline.analyze_feature_importance()\n",
    "        pipeline.create_individual_waterfall_plots(row_indices, output_dir)\n",
    "        print(f\"✅ Custom waterfalls created in {output_dir}/\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating custom waterfall plots: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# USAGE EXAMPLES\n",
    "# =============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"🎯 Pharma PoC ML Pipeline - Usage Examples\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # 1) Full pipeline\n",
    "    print(\"\\n1. Running full ML pipeline...\")\n",
    "    result_file = run_Pharma_ml_analysis(\n",
    "        input_file='Pharma_poc_custom_1000_optimized.xlsx',   # or 'Pharma_poc_custom_1000.xlsx'\n",
    "        output_file='Pharma_poc_custom_1000_optimized_ml_results.xlsx'\n",
    "    )\n",
    "\n",
    "    # 2) Custom plots\n",
    "    print(\"\\n2. Creating custom waterfalls...\")\n",
    "    create_custom_waterfall_plots(\n",
    "        input_file='Pharma_poc_custom_1000_optimized.xlsx',\n",
    "        row_indices=[0, 1, 5, 10],\n",
    "        output_dir='my_custom_waterfalls'\n",
    "    )\n",
    "\n",
    "    print(\"\\n✅ Analysis complete!\")\n",
    "    print(\"• Two model sets (Model1 full+eng, Model2 A–T+eng)\")\n",
    "    print(\"• HistGradientBoostingClassifier + rule-first AI overrides\")\n",
    "    print(\"• Rich engineered features matching generator logic\")\n",
    "    print(\"• SHAP (permutation) + confusion matrices + per-row plots\")\n",
    "    print(\"• Same artifacts filenames and Excel structure\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f83ce16b-eb13-47b1-8da0-127f8ec2eee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bffed9-1bdc-449a-83b9-da34efbbaba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
