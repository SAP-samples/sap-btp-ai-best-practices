{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fecf4544",
   "metadata": {},
   "source": [
    "# LangChain RAG with HANA Vector Store and Optional Chat History\n",
    "\n",
    "This notebook walks you through building a **Retrieval-Augmented Generation (RAG)** application using:\n",
    "- **LangChain LCEL**\n",
    "- **HANA Vector Store**\n",
    "- **OpenAI GPT-4o**\n",
    "- And an **optional chat history feature**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65f5b9d-cb1d-4441-a785-d98ee2b7dc53",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e78ac7a-424d-4ff2-8a6a-3d63b499ea86",
   "metadata": {},
   "source": [
    "* **HANA Vector Store**:\n",
    "\n",
    "    The HANA Vector Store is pre-populated with text documents, their corresponding embedding vectors, and associated metadata.\n",
    "\n",
    "    For this tutorial, the necessary source documents have already been processed and stored in the Vector Store.\n",
    "\n",
    "    To view the **source PDF documents** and learn how to prepare and store data in the HANA Vector Store—including embedding generation and metadata handling — refer [Embeddings Best Practices](https://github.com/SAP-samples/sap-btp-ai-best-practices/tree/main/best-practices/vector-rag-embedding) guide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e870a251",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Dependencies\n",
    "Ensure all python packages provided in `requirements.txt` file are installed.\n",
    "``` bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "\n",
    "### Environment Variables\n",
    "Ensure `.env` file is updated with following environment variables.\n",
    "``` env\n",
    "HANA_ADDRESS=your_hana_host\n",
    "HANA_PORT=your_port\n",
    "HANA_USER=your_user\n",
    "HANA_PASSWORD=your_password\n",
    "HANA_AUTOCOMMIT=true\n",
    "HANA_SSL_CERT_VALIDATE=false\n",
    "\n",
    "AICORE_AUTH_URL=your_aicore_auth_url\n",
    "AICORE_CLIENT_ID=your_aicore_client_id\n",
    "AICORE_CLIENT_SECRET=your_aicore_secret\n",
    "AICORE_RESOURCE_GROUP=your_resource_group\n",
    "AICORE_BASE_URL=your_base_url\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8af43d",
   "metadata": {},
   "source": [
    "## Initialization and Imports\n",
    "\n",
    "We begin by importing necessary components and setting up the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3f57356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import section\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from hdbcli import dbapi\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_community.vectorstores import HanaDB\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client\n",
    "from gen_ai_hub.proxy.langchain.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a079e0cf-bcf1-48ad-9c8f-9169ca2cedad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9396d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_bool(value: str) -> bool:\n",
    "    '''\n",
    "    Convert environment variable to boolean based on values.\n",
    "    '''\n",
    "    return value.lower() in (\"true\", \"1\", \"yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5f4de",
   "metadata": {},
   "source": [
    "## Set Up HANA Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d94635da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection to HANA Vector Store\n",
    "connection = dbapi.connect(\n",
    "    os.environ[\"HANA_ADDRESS\"],\n",
    "    os.environ[\"HANA_PORT\"],\n",
    "    os.environ[\"HANA_USER\"],\n",
    "    os.environ[\"HANA_PASSWORD\"],\n",
    "    str_to_bool(os.getenv(\"HANA_AUTOCOMMIT\", \"false\")),\n",
    "    str_to_bool(os.getenv(\"HANA_SSL_CERT_VALIDATE\", \"false\"))\n",
    ")\n",
    "\n",
    "# Establish connection to Generative AI Hub on AI Core for LLM access\n",
    "proxy_client = get_proxy_client(\"gen-ai-hub\")\n",
    "\n",
    "# Initialize embeddings model\n",
    "embedding_model = OpenAIEmbeddings(proxy_model_name=\"text-embedding-ada-002\")\n",
    "\n",
    "# Connect to HANA Vector Store and create LangChain object of the vector store for further operations\n",
    "vdb = HanaDB(\n",
    "    embedding=embedding_model,\n",
    "    connection=connection,\n",
    "    distance_strategy=DistanceStrategy.COSINE, # Use cosine similarity for vector search\n",
    "    table_name=\"SAP_HELP_PUBLIC\",\n",
    "    \n",
    "    # The following column names are the expected defaults used by HanaDB:\n",
    "    content_column=\"VEC_TEXT\",     # (default) Column containing the raw document text\n",
    "    metadata_column=\"VEC_META\",    # (default) Column containing metadata (typically JSON)\n",
    "    vector_column=\"VEC_VECTOR\",    # (default) Column containing embedding vectors\n",
    "\n",
    "    # If your table uses different column names, specify them here to override the defaults\n",
    ")\n",
    "\n",
    "# Set up the vector store as a retriever with top-k document retrieval\n",
    "retriever = vdb.as_retriever(search_kwargs={\"k\": 2}) # Retrieve top 2 most similar documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45b5fcf",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bae4865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt\n",
    "user_prompt = '''\n",
    "Use the following context information to answer to user's query.\n",
    "Here is some context: {context}\n",
    "\n",
    "Based on the above context, answer the following query:\n",
    "{question}\n",
    "\n",
    "The answer tone has to be very professional in nature.\n",
    "\n",
    "If you don't know the answer, politely say that you don't know.\n",
    "'''\n",
    "\n",
    "# Create langchain prompt object to fit into langchain pipeline\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"user\", user_prompt)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45611c82",
   "metadata": {},
   "source": [
    "## Chat History (Optional)\n",
    "\n",
    "We implement a capped in-memory message history. You can toggle storing this history using a flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f203d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports for processing conversation history messages\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"\n",
    "    A class with methods add/get/delete operations on history data.\n",
    "    \"\"\"\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "\n",
    "    def add_message(self, message: BaseMessage) -> None:\n",
    "        self.messages.append(message)\n",
    "        self.messages = self.messages[-4:]\n",
    "\n",
    "    def add_messages(self, messages: List[BaseMessage]) -> None:\n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-4:]\n",
    "\n",
    "    def get_messages(self) -> List[BaseMessage]:\n",
    "        return self.messages\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []\n",
    "\n",
    "store = {} # Global dictionary to store history messages in memory\n",
    "\n",
    "def get_by_session_id(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"\n",
    "    Retrieve history messages for given session id.\n",
    "    \"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1278605",
   "metadata": {},
   "source": [
    "## Define the LLM and RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2e8e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "\n",
    "# Initialize LLM from Generative AI Hub. For now using OpenAI model. Other models are supported as well as mentioned on help.sap.com\n",
    "llm = ChatOpenAI(\n",
    "    proxy_model_name='gpt-4o', \n",
    "    proxy_client=proxy_client,\n",
    "    max_tokens=2000,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# Define langchain pipeline chain from taking a question till output parsing\n",
    "base_chain = (\n",
    "    {\n",
    "        \"context\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"history\": lambda x: x.get(\"history\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671f490f",
   "metadata": {},
   "source": [
    "## Add Optional Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fe751fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the chain with history message processing runnable to add history messages to conversation if opted.\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_by_session_id,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccad97d",
   "metadata": {},
   "source": [
    "## Chat Function with History Toggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41244fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_rag(question: str, session_id: str, use_history: bool = True):\n",
    "    \"\"\"\n",
    "    Function to decide if history messages to be processed based user's input\n",
    "    \"\"\"\n",
    "    if use_history:\n",
    "        return chain_with_history.invoke(\n",
    "            {\"question\": question},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "    else:\n",
    "        return base_chain.invoke({\"question\": question, \"history\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec30402",
   "metadata": {},
   "source": [
    "## Run It!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "550f8796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business AI refers to the application of artificial intelligence technologies within business environments to drive real business results. It encompasses various aspects such as facilitating human-machine interaction through natural user experiences, enabling automation for repetitive tasks, and augmenting human decisions and cognition with insights, optimization, and predictions. The context provided highlights the role of AI within a business technology platform, emphasizing its responsibility, reliability, and relevance.\n",
      "\n",
      "The documents indicate that customers seeking to adopt Business AI are primarily motivated by the desire to achieve tangible business results. The AI foundation on the business technology platform supports this by enhancing user experience, automating processes, and providing valuable insights for decision-making.\n",
      "\n",
      "If you have any further questions or need more detailed information, please feel free to ask.\n",
      "\n",
      "-+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+-\n",
      "\n",
      "Certainly. Business AI refers to the application of artificial intelligence technologies within business environments to achieve real business outcomes. It involves enhancing human-machine interaction through natural user experiences, automating repetitive tasks, and augmenting human decisions with insights, optimization, and predictions. The context provided underscores the importance of AI being relevant, reliable, and responsible within a business technology platform.\n",
      "\n",
      "The documents suggest that customers are motivated to adopt Business AI to realize tangible business results. The AI foundation within the business technology platform plays a crucial role in improving user experiences, automating processes, and offering valuable insights for informed decision-making.\n",
      "\n",
      "If you have any further questions or require additional information, please feel free to ask.\n",
      "\n",
      "-+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+-\n",
      "\n",
      "I apologize, but I do not have access to previous interactions or answers. If you have any specific questions related to the provided context, I would be happy to assist you with those.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    session_id = \"user-123\"\n",
    "    print(chat_with_rag(\"What is Business AI and can you summarize the context you received?\", session_id, use_history=True))\n",
    "    print()\n",
    "    print(\"-+-\" * 30)\n",
    "    print()\n",
    "    print(chat_with_rag(\"Can you repeat?\", session_id, use_history=True))\n",
    "\n",
    "    # Without history\n",
    "    print()\n",
    "    print(\"-+-\" * 30)\n",
    "    print()\n",
    "    print(chat_with_rag(\"What was your previous answer?\", session_id, use_history=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f5abb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- This setup uses **SAP HANA Vector Store** for retrieval.\n",
    "- Responses are generated using **GPT-4o** from OpenAI available on Generative AI Hub. Other models can be used as well as listed on [help.sap.com](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/supported-models)\n",
    "- **Chat history** is optional and supports follow-up questions. Current implementation processes history messages in-memory. However, the same can be stored in files or Database as well. Please refer [LangChain document](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) for processing history messages.\n",
    "- Implemented using **LangChain Expression Language (LCEL)** for composability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281650fa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
