{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Practices for Creating Embeddings in HANA Cloud with SAP GenAI Hub and LangChain\n",
    "\n",
    "## Introduction\n",
    "This notebook demonstrates an efficient way to create text embeddings from PDFs and store them in an SAP HANA Cloud vector database using SAP GenAI Hub SDK and LangChain. The guide follows best practices to ensure optimal chunking, embedding, and retrieval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up and Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Step 1: Setup and Database Connection\n",
    "import os\n",
    "import glob\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# SAP HANA DBAPI\n",
    "from hdbcli import dbapi\n",
    "\n",
    "# LangChain Components\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores.hanavector import HanaDB\n",
    "\n",
    "# SAP GenAI Hub SDK Components\n",
    "from gen_ai_hub.proxy.langchain.openai import OpenAIEmbeddings\n",
    "\n",
    "# Loads configuration from .env file.\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializes connection to the HANA database.\n",
    "connection = dbapi.connect(\n",
    "    address=os.environ.get(\"HANA_ADDRESS\"),\n",
    "    port=os.environ.get(\"HANA_PORT\"),\n",
    "    user=os.environ.get(\"HANA_USER\"),\n",
    "    password=os.environ.get(\"HANA_PASSWORD\"),\n",
    "    autocommit=True,\n",
    "    sslValidateCertificate=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAP help pages for SAP Business AI and SAP Hana Cloud have been downloaded in form of PDF documents and placed in `sample_files` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample files considered for the notebook are PDF documents. However different file types can be processed using corresponding loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDFs from a directory\n",
    "def load_pdfs(directory: str):\n",
    "    \"\"\"Load and extract text from PDF files in the specified directory.\"\"\"\n",
    "    pdf_files = glob.glob(os.path.join(directory, \"*.pdf\"))\n",
    "    docs = []\n",
    "    for file in pdf_files:\n",
    "        loader = PyPDFLoader(file)\n",
    "        docs.extend(loader.load())\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking Strategy\n",
    "\n",
    "Large documents need to be broken down into smaller, meaningful chunks to optimize retrieval. This function splits documents into segments with controlled overlap, ensuring the embeddings capture context effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **RecursiveCharacterTextSplitter:** Works well for most text-heavy documents.\n",
    "- **CharacterTextSplitter:** Suitable for structured text with clear section markers.\n",
    "- **Token-based splitters:** Useful when working with token-limited models.\n",
    "- **MarkdownHeaderTextSplitter:** Suitable for processing Markdown files.\n",
    "\n",
    "Please refer Langchain official page for supported splitters. Custom splitters based on the document structure can be created and integrated with Langchain as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_document(document, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"Split documents into smaller chunks for better processing.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Embeddings model from Generative AI Hub\n",
    "def init_embeddings_model(model_name):\n",
    "    embeddings_model = OpenAIEmbeddings(proxy_model_name=model_name)\n",
    "    return embeddings_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Generate unique ID as an additional metadata for a later step\n",
    "def get_unique_id():\n",
    "    return str(uuid.uuid4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDFs...\n",
      "Loaded 49 documents.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading PDFs...\")\n",
    "documents = load_pdfs(\"../sample_files/\")\n",
    "print(f\"Loaded {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting source documents into chunks...\n",
      "Generated 137 chunks.\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting source documents into chunks...\")\n",
    "chunks = split_document(documents)\n",
    "print(f\"Generated {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Add more metadata to the chunks if required\n",
    "chunks = [\n",
    "    Document(\n",
    "        page_content=c.page_content,\n",
    "        metadata=c.metadata | {'document_number': str(c_number), 'unique_id': get_unique_id()}\n",
    "    ) for c_number, c in enumerate(chunks)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an Embedding model, you can map text to high-dimensional vectors for tasks such as semantic search and clustering.\n",
    "\n",
    "For this example, we considered an OpenAI embedding mode. However, a number of Embedding models are supported on SAP Generative AI Hub. Please refer the SAP help page to select a suitable model for your requirements.\n",
    "\n",
    "[Available Embedding models on SAP Generative AI Hub](https://help.sap.com/doc/DRAFT/de440ae8a54442068c7c7bcb9375c9b4/DEV/en-US/_temp/gen_ai_hub/examples/gen_ai_hub.html#embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Embeddings model from Generative AI Hub\n",
    "embeddings = init_embeddings_model(\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Embeddings in HANA Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HANA Vector Store instance\n",
    "db = HanaDB(\n",
    "    embedding=embeddings, connection=connection, table_name=\"SAP_HELP_PUBLIC\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optional: Remove all entries from the table\n",
    "db.delete(filter={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upsert Document objects to HANA Vector Store\n",
    "db.add_documents(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, HANA Vector Store uses COSINE similarity as Distance Strategy. However, it provides other similarity measures like EUCLIDEAN_DISTANCE and Maximal Marginal Relevance Search (MMR) as well.\n",
    "\n",
    "1. To use Euclidean distance for retrieval\n",
    "``` python\n",
    "db = HanaDB(\n",
    "    embedding=<embedding_model>,\n",
    "    connection=<connection>,\n",
    "    distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE,\n",
    "    table_name=<table_name>,\n",
    ")\n",
    "```\n",
    "\n",
    "2. MMR optimizes for similarity to query AND diversity among selected documents. The first 20 (fetch_k) items will be retrieved from the DB. The MMR algorithm will then find the best 2 (k) matches.\n",
    "``` python\n",
    "docs = db.max_marginal_relevance_search(query, k=2, fetch_k=20)\n",
    "for doc in docs:\n",
    "    print(\"-\" * 80)\n",
    "    print(doc.page_content)\n",
    "```\n",
    "\n",
    "3. HNSW Vector Index: A vector index can significantly speed up top-k nearest neighbor queries for vectors. Users can create a Hierarchical Navigable Small World (HNSW) vector index using the create_hnsw_index function.\n",
    "``` python\n",
    "# HanaDB instance uses cosine similarity as default:\n",
    "db_cosine = HanaDB(\n",
    "    embedding=embeddings, connection=connection, table_name=\"SAP_HELP_PUBLIC\"\n",
    ")\n",
    "\n",
    "# Attempting to create the HNSW index with default parameters\n",
    "db_cosine.create_hnsw_index()  # If no other parameters are specified, the default values will be used\n",
    "# Default values: m=64, ef_construction=128, ef_search=200\n",
    "# The default index name will be: STATE_OF_THE_UNION_COSINE_SIMILARITY_IDX (verify this naming pattern in HanaDB class)\n",
    "\n",
    "\n",
    "# Creating a HanaDB instance with L2 distance as the similarity function and defined values\n",
    "db_l2 = HanaDB(\n",
    "    embedding=embeddings,\n",
    "    connection=connection,\n",
    "    table_name=\"SAP_HELP_PUBLIC\",\n",
    "    distance_strategy=DistanceStrategy.EUCLIDEAN_DISTANCE,  # Specify L2 distance\n",
    ")\n",
    "\n",
    "# This will create an index based on L2 distance strategy.\n",
    "db_l2.create_hnsw_index(\n",
    "    index_name=\"SAP_HELP_PUBLIC_L2_index\",\n",
    "    m=100,  # Max number of neighbors per graph node (valid range: 4 to 1000)\n",
    "    ef_construction=200,  # Max number of candidates during graph construction (valid range: 1 to 100000)\n",
    "    ef_search=500,  # Min number of candidates during the search (valid range: 1 to 100000)\n",
    ")\n",
    "\n",
    "# Use L2 index to perform MMR\n",
    "docs = db_l2.max_marginal_relevance_search(query, k=2, fetch_k=20)\n",
    "for doc in docs:\n",
    "    print(\"-\" * 80)\n",
    "    print(doc.page_content)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "4PUBLIC© 2023 SAP SE or an SAP affiliate company. All rights reserved.  ǀ\n",
      "Customers wanting to adopt Business AI Desire Real Business Results\n",
      "--------------------------------------------------------------------------------\n",
      "9PUBLIC© 2023 SAP SE or an SAP affiliate company. All rights reserved.  ǀ\n",
      "Today, SAP offers a large catalogue of AI-powered scenarios across all business functions \n",
      "…Find out more on SAP Business AI\n",
      "Finance\n",
      "▪ Tax Compliance\n",
      "▪ Cash Application\n",
      "▪ Intelligent accrual\n",
      "▪ Travel expense auditing\n",
      "▪ Travel expense \n",
      "verification\n",
      "▪ Invoice processing\n",
      "▪ Business Integrity \n",
      "screening\n",
      "▪ Goods and invoice receipt \n",
      "matching\n",
      "▪ Mobile expense entry\n",
      "Supply \n",
      "Chain\n",
      "▪ Stock in transit\n",
      "▪ Visual Inspection\n"
     ]
    }
   ],
   "source": [
    "# Retrieve top matching documents from HANA for a given input text\n",
    "query = \"What is SAP Business AI?\"\n",
    "docs = db.similarity_search(query, k=2)\n",
    "for doc in docs:\n",
    "    print(\"-\" * 80)\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "- **Chunking Strategy Matters:** Choose a splitting method based on document structure.\n",
    "- **Batch Processing for Efficiency:** Process documents in batches when working with large datasets.\n",
    "- **Metadata Enrichment:** Adding metadata (e.g., document numbers, unique IDs) enhances traceability and helps in deletion of specific documents.\n",
    "- **Embedding Model Selection:** Choose an embedding model based on the retrieval requirements and performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please check [LangChain documentation](https://python.langchain.com/docs/integrations/vectorstores/sap_hanavector/) for more details on SAP HANA Vector Store and LangChain integration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
