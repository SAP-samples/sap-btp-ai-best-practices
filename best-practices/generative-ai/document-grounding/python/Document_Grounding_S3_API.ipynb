{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Grounding on S3 data using Using Document Management APIs\n",
    "\n",
    "Purpose: Ground LLM responses on your enterprise data stored in S3, with SAP Document Grounding service using Using Document Management APIs. The tutorial demonstrates different steps to set up and implement the grounding service with S3 as a data source.\n",
    "\n",
    "\n",
    "The process consists of three steps:  \n",
    "* Step 1: Upload Documents to S3 bucket  \n",
    "* Step 2: Create Data Repository pointing to S3 bucket \n",
    "* Step 3: Retrieve most similar documents from Data Repository based on input query and generate augmented answer\n",
    "\n",
    "\n",
    "**Pre-requisites:**\n",
    "* Object Store (S3) and its credentials\n",
    "\n",
    "\n",
    "**Step 1:**\n",
    "Push your document(s) to the S3 bucket. \n",
    "\n",
    "**Step 2:**\n",
    "* Create Generic Secret on AI Launchpad with S3 object store credentials.\n",
    "* Use Document Management Pipelines API to create a Data Repository from S3 bucket.\n",
    "\n",
    "**Step 3:**\n",
    "* Use Document Management Retrieval API to fetch most similar documents from the Data Repository\n",
    "* [Optional] Use Gen AI Hub SDK to access an LLM to create answer using the retrieved documents as a context.\n",
    "\n",
    "\n",
    "In this tutorial, we will:\n",
    "1. Create Access Token.\n",
    "2. Create Repository using Pipelines API.\n",
    "3. Retrieve documents using Retrieval API.\n",
    "3. [OPTIONAL] Use GPT-4o model to generate answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Load to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtain Object Store Credentials**  \n",
    "* Download S3 credentials from BTP cockpit > BTP subaccount > Space > Instances > ObjectStore > Credentials\n",
    "\n",
    "**AWS CLI Installation and Configuration**  \n",
    "* Install AWS CLI: Download and install the AWS Command Line Interface from the official AWS documentation [Link](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html).\n",
    "* Verify Installation: Check your installation by running:\n",
    "``` sh\n",
    "aws --version\n",
    "```\n",
    "* Configure AWS CLI: Run the configuration command and pass corresponding values from Object Store credentials.\n",
    "``` sh\n",
    "aws configure\n",
    "```\n",
    "\n",
    "**Push Documents to S3**  \n",
    "You can push your documents to S3 bucket. You can optionally put the documents to a sub-directory in the bucket as well.\n",
    "\n",
    "S3 CLI commands:  \n",
    "``` sh\n",
    "aws s3 cp sample.pdf s3://your-bucket-name/sample.pdf\n",
    "OR\n",
    "aws s3 cp . s3://your-bucket-name/ --recursive\n",
    "OR\n",
    "aws s3 cp . s3://your-bucket-name/<optional_path>/ --recursive\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Data Repository using Pipelines API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Create Generic Secret \n",
    "Create Generic Secret key collection on AI Launchpad with S3 Object Store credentials. [SAP Help](https://help.sap.com/docs/ai-launchpad/sap-ai-launchpad/grounding-management)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Generate Access Token\n",
    "\n",
    "Create Access Token using the AI Core credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access token obtained successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Replace these with your actual service key details\n",
    "client_id = os.getenv(\"AICORE_CLIENT_ID\")\n",
    "client_secret = os.getenv(\"AICORE_CLIENT_SECRET\")\n",
    "auth_url = os.getenv(\"AICORE_AUTH_URL\")\n",
    "\n",
    "# Prepare the payload and headers\n",
    "payload = {\n",
    "    \"grant_type\": \"client_credentials\"\n",
    "}\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "}\n",
    "\n",
    "# Make the POST request to obtain the token\n",
    "response = requests.post(auth_url, data=payload, headers=headers, auth=(client_id, client_secret))\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    access_token = response.json().get(\"access_token\")\n",
    "    print(\"Access token obtained successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to obtain access token: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Create Data Repository using Pipeline API\n",
    "\n",
    "As a pre-requisite, the S3 credentials are added as a generic secret on AI Core. You need to replace the generic secret key name with yours in the following code.\n",
    "\n",
    "Optionally, specify the path in S3 bucket from where the documents are to be considered for grounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 201\n",
      "Response: {\"pipelineId\": \"13ce3a20-26a8-455c-a06e-1343be4a8bf7\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AI_API_URL = r\"https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com\" # Update your AI_API_URL as per the aws region\n",
    "url = f\"{AI_API_URL}/v2/lm/document-grounding/pipelines\"\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\",\n",
    "           \"AI-Resource-Group\": \"default\", # update your resource group name here\n",
    "           \"Content-Type\": \"application/json\"}\n",
    "\n",
    "payload = {\n",
    "    \"type\": \"S3\",\n",
    "    \"configuration\": {\n",
    "        \"destination\": \"ai-best-practices-grounding-s3-b2\", # Update your S3 generic secret key name here\n",
    "        \"s3\": {\n",
    "            \"includePaths\": [\"/new_papers/\"] # Sub-directory path (also called as perfix) in S3 bucket to consider for fetching data from\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.4: Verify created Data Repository\n",
    "\n",
    "**Get the details for newly created Data Repository**\n",
    "  \n",
    "Go to Grounding Management page on AI Launchpad and note the ID of Data Repository for the pipeline ID that you just created in above step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "Response: {\"id\": \"b27f2f08-d6e2-4907-920d-7d08035c4de8\", \"title\": \"pipeline-13ce3a20-26a8-455c-a06e-1343be4a8bf7-collection\", \"metadata\": [{\"key\": \"pipeline\", \"value\": [\"13ce3a20-26a8-455c-a06e-1343be4a8bf7\"]}, {\"key\": \"type\", \"value\": [\"custom\"]}, {\"key\": \"pipelineType\", \"value\": [\"S3\"]}], \"type\": \"vector\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AI_API_URL = r\"https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com\" # Replace with your AI API URL\n",
    "repository_id = \"b27f2f08-d6e2-4907-920d-7d08035c4de8\" # Paste Data Repository ID from Grounding Management page on AI Launchpad\n",
    "\n",
    "url = f\"{AI_API_URL}/v2/lm/document-grounding/retrieval/dataRepositories/{repository_id}\"\n",
    "\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\",\n",
    "           \"AI-Resource-Group\": \"default\", # Update your resource group name here\n",
    "           \"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Retrieve Similar Documents\n",
    "\n",
    "Add ID of your data repository in the following cell. If want to include more data repositories, you can add them in the list as well.\n",
    "\n",
    "Optionally, specify the path in S3 bucket from where the documents are to be considered grounding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "The concept of receptive ﬁeld is important for understanding and diagnosing how deep CNNs work.\n",
      "Since anywhere in an input image outside the receptive ﬁeld of a unit does not affect the value of that\n",
      "unit, it is necessary to carefully control the receptive ﬁeld, to ensure that it covers the entire relevant\n",
      "image region. In many tasks, especially dense prediction tasks like semantic image segmentation,\n",
      "stereo and optical ﬂow estimation, where we make a prediction for each single pixel in the input image,\n",
      "it is critical for each output pixel to have a big receptive ﬁeld, such that no important information is\n",
      "left out when making the prediction.\n",
      "The receptive ﬁeld size of a unit can be increased in a number of ways. One option is to stack more\n",
      "layers to make the network deeper, which increases the receptive ﬁeld size linearly by theory, as\n",
      "each extra layer increases the receptive ﬁeld size by the kernel size. Sub-sampling on the other hand\n",
      "takes up a fraction of the full theoretical receptive ﬁeld. Empirical results echoed the theory we\n",
      "established. We believe this is just the start of the study of effective receptive ﬁeld, which provides a\n",
      "new angle to understand deep CNNs. In the future we hope to study more about what factors impact\n",
      "effective receptive ﬁeld in practice and how we can gain more control over them.\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "url = f\"{AI_API_URL}/v2/lm/document-grounding/retrieval/search\"\n",
    "\n",
    "headers = {\n",
    "    \"AI-Resource-Group\": \"default\",\n",
    "    \"Authorization\": f\"Bearer {access_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"query\": \"What is efficient receptive field?\",\n",
    "    \"filters\": [\n",
    "        {\n",
    "            \"id\": \"string\",\n",
    "            \"searchConfiguration\": {\n",
    "                \"maxChunkCount\": 2\n",
    "            },\n",
    "            \"dataRepositories\": [\"b27f2f08-d6e2-4907-920d-7d08035c4de8\"], # Specify your repository ID(s)\n",
    "            \"dataRepositoryType\": \"vector\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "response_text = response.text\n",
    "\n",
    "import json\n",
    "\n",
    "# Parse the JSON string into a dictionary\n",
    "response_dict = json.loads(response_text)\n",
    "retrieved_docs = [] \n",
    "# Loop through and print each \"content\"\n",
    "for result in response_dict.get(\"results\", []):\n",
    "    for res in result.get(\"results\", []):\n",
    "        for document in res.get(\"dataRepository\", {}).get(\"documents\", []):\n",
    "            for chunk in document.get(\"chunks\", []):\n",
    "                retrieved_docs.append(chunk.get(\"content\", \"\"))\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1 [OPTIONAL]: Augment answer generation with retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ' '.join([c for c in retrieved_docs])\n",
    "\n",
    "query = \"What is receptive field?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Use the following context information to answer to user's query.\n",
    "Here is some context: {context}\n",
    "\n",
    "Based on the above context, answer the following query:\n",
    "{query}\n",
    "\n",
    "The answer tone has to be very professional in nature.\n",
    "\n",
    "If you don't know the answer, politely say that you don't know, don't try to make up an answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the context of deep convolutional neural networks (CNNs), the receptive field refers to the specific portion of the input image that affects the output of a particular unit or neuron within the network. Essentially, it constitutes the region in which information influences the activation of that unit. The concept of the receptive field is crucial for understanding how deep CNNs operate and is particularly important in tasks involving dense predictions like semantic image segmentation, stereo vision, and optical flow estimation, where predictions need to be made for individual pixels within an image.\n",
      "\n",
      "To ensure that CNNs efficiently cover the necessary image regions, the receptive field size must be carefully managed. Increasing the receptive field allows for more comprehensive capture of relevant information needed for accurate predictions. This can be achieved by adding more layers, which theoretically increases the receptive field size linearly, as each layer adds to the receptive field proportional to the kernel size. However, techniques like sub-sampling can reduce the effective receptive field.\n",
      "\n",
      "Understanding and controlling the receptive field size is a developing area in the study of CNNs, with ongoing research into the factors that affect the effective receptive field and how to optimize its control.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.native.openai import chat\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an intelligent assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "kwargs = dict(model_name=\"gpt-4o\", messages=messages)\n",
    "\n",
    "response = chat.completions.create(**kwargs)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
