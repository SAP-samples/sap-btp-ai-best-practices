{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Grounding on using Vector APIs of Document Grounding Management\n",
    "\n",
    "Purpose: Ground LLM responses on your enterprise data, with SAP Document Grounding service using Vector APIs with complete control over chunking and metadata handling process. The tutorial demonstrates different steps to set up and implement the grounding service using Vector APIs for your documents.\n",
    "\n",
    "\n",
    "The process consists of three steps:  \n",
    "* Step 1: Create Data Repository  \n",
    "* Step 2: Prepare the data \n",
    "* Step 3: Ingest data using Vector APIs \n",
    "* Step 4: Retrieve most similar documents from Data Repository based on input query and generate augmented answer\n",
    "\n",
    "\n",
    "**Step 1:**\n",
    "* Generate Access token.\n",
    "* Create a Data Repository (also called as Collection) \n",
    "\n",
    "**Step 2:**\n",
    "* Create chunks and metadata to add as Documents in the newly create Data Repository\n",
    "\n",
    "**Step 3:**\n",
    "* Use Vector API to add the add the documents.\n",
    "\n",
    "**Step 4:**\n",
    "* Use Document Management Retrieval API to fetch most similar documents from the Data Repository\n",
    "* [Optional] Use Gen AI Hub SDK to access an LLM to create answer using the retrieved documents as a context.\n",
    "\n",
    "**Step 5: [OPTIONAL]**\n",
    "* Use GPT-4o model to generate answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Data Repository using Vector API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Generate Access Token\n",
    "\n",
    "Create Access Token using the AI Core credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access token obtained successfully.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Replace these with your actual service key details\n",
    "client_id = os.getenv(\"AICORE_CLIENT_ID\")\n",
    "client_secret = os.getenv(\"AICORE_CLIENT_SECRET\")\n",
    "auth_url = os.getenv(\"AICORE_AUTH_URL\")\n",
    "\n",
    "# Prepare the payload and headers\n",
    "payload = {\n",
    "    \"grant_type\": \"client_credentials\"\n",
    "}\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "}\n",
    "\n",
    "# Make the POST request to obtain the token\n",
    "response = requests.post(auth_url, data=payload, headers=headers, auth=(client_id, client_secret))\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    access_token = response.json().get(\"access_token\")\n",
    "    print(\"Access token obtained successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to obtain access token: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Create a Data Repository\n",
    "\n",
    "Create a data repository for your knowledge base using Vector API. Response code 202 denotes that the Repository (or called as Collection) is created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [202]>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "AI_API_URL = r\"https://api.ai.prod.eu-central-1.aws.ml.hana.ondemand.com\" # Update your AI_API_URL as per the aws region\n",
    "url = f\"{AI_API_URL}/v2/lm/document-grounding/vector/collections\"\n",
    "\n",
    "body={\n",
    "  \"title\": \"bp-dg-vector-data-repo\", # Give a name to the Data Repository (Collection)\n",
    "  \"embeddingConfig\": {\n",
    "    \"modelName\": \"text-embedding-ada-002\" # Mention the name of embeddings model\n",
    "  }\n",
    "}\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\",\n",
    "           \"AI-Resource-Group\": \"default\", # Mention the name of your resource group\n",
    "           \"Content-Type\": \"application/json\"}\n",
    "response = requests.post(url, headers=headers,json=body)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3: Get the Collection ID\n",
    "\n",
    "Go the AI Launchpad and note the colletion ID of the newly created Document Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = \"28e4a470-75f7-4ca0-8d30-1fb9687e73b1\" # Update with your collection id obtained from AI Launchpad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vector APIs expects the data payload in the following format. Hence while chunking and associating metadata, it is convenient to create output structure tailored as per the payload schema. Moreover, dumping the data in JSONL would help in debugging and manual adjustment if required as well.\n",
    "\n",
    "Note: \n",
    "* In each API call, we can push all chunks of a single document only.\n",
    "* When creating Collection from an S3 bucket, the default metadatas contains 'id' as key and document name as value. To maintain consistency, I suggest, while creating a collection using Vector API, we also create similar metadata as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents written to documents.jsonl in JSONL format (one PDF per line)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Path to the directory containing PDF files\n",
    "pdf_directory = '../sample_files/'  # Update this to your folder path\n",
    "\n",
    "# Output JSONL file\n",
    "output_file = 'documents.jsonl'\n",
    "\n",
    "# List all PDF files in the directory\n",
    "pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith('.pdf')]\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "    for pdf_file in pdf_files:\n",
    "        file_path = os.path.join(pdf_directory, pdf_file)\n",
    "        \n",
    "        # Open the PDF file with PyMuPDF\n",
    "        doc = fitz.open(file_path)\n",
    "        full_text = \"\"\n",
    "        \n",
    "        # Extract text from each page\n",
    "        for page_num in range(len(doc)):\n",
    "            page = doc.load_page(page_num)\n",
    "            full_text += page.get_text()\n",
    "        \n",
    "        doc.close()\n",
    "        \n",
    "        # Create unique metadata\n",
    "        doc_meta = os.path.basename(file_path)\n",
    "        \n",
    "        document = Document(\n",
    "            page_content=full_text,\n",
    "            metadata={\n",
    "                'source': pdf_file,\n",
    "                'author': f\"Author of {pdf_file}\",\n",
    "                'category': f\"Category_{os.path.splitext(pdf_file)[0]}\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Split into chunks\n",
    "        chunks = text_splitter.split_documents([document])\n",
    "        \n",
    "        # Build the document entry\n",
    "        doc_entry = {\n",
    "            \"documents\": [\n",
    "                {\n",
    "                    \"metadata\": [\n",
    "                        {\n",
    "                            \"key\": \"id\",\n",
    "                            \"value\": [doc_meta] # I suggest to keep this key-value pair as default one for consistency with S3 collection\n",
    "                        },\n",
    "                        {\n",
    "                            \"key\": \"url\",\n",
    "                            \"value\": [f\"http://example.com/{doc_meta}\"] # Similarly add your metadatas\n",
    "                        }\n",
    "                    ],\n",
    "                    \"chunks\": []\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        for idx, chunk in enumerate(chunks, start=1):\n",
    "            chunk_entry = {\n",
    "                \"content\": chunk.page_content,\n",
    "                \"metadata\": [\n",
    "                    {\n",
    "                        \"key\": \"index\",\n",
    "                        \"value\": [str(idx)]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            doc_entry[\"documents\"][0][\"chunks\"].append(chunk_entry)\n",
    "        \n",
    "        # Write the document entry as one line in JSONL format\n",
    "        f_out.write(json.dumps(doc_entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Documents written to {output_file} in JSONL format (one PDF per line)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Data Ingestion using Vector API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the payload corresponding to each document from documents.jsonl file and use Vector API to add the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Documents:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Documents: 100%|██████████| 6/6 [01:21<00:00, 13.56s/it]\n"
     ]
    }
   ],
   "source": [
    "# Read and send each document (one PDF per line)\n",
    "from tqdm import tqdm\n",
    "\n",
    "jsonl_file = 'documents.jsonl'\n",
    "\n",
    "url = f\"{AI_API_URL}/v2/lm/document-grounding/vector/collections/{collection_id}/documents\"\n",
    "\n",
    "# Headers\n",
    "headers = {\n",
    "    \"AI-Resource-Group\": \"default\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {access_token}\"\n",
    "}\n",
    "\n",
    "# First, count the number of lines to set tqdm's total\n",
    "with open(jsonl_file, 'r', encoding='utf-8') as f_count:\n",
    "    total_docs = sum(1 for _ in f_count)\n",
    "\n",
    "with open(jsonl_file, 'r', encoding='utf-8') as f_in, tqdm(total=total_docs, desc=\"Uploading Documents\") as pbar:\n",
    "    for idx, line in enumerate(f_in, start=1):\n",
    "        document_payload = json.loads(line.strip())\n",
    "        \n",
    "        response = requests.post(url, headers=headers, json=document_payload)\n",
    "        \n",
    "        pbar.update(1)\n",
    "        \n",
    "        if response.status_code not in [200, 201]:\n",
    "            print(f\"Stopping due to error at document {idx} in JSONL file. Error code: {response.status_code}\")\n",
    "            print(f\"Content: {document_payload}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.1 [OPTIONAL]: Update / Delete records from Data Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AI Best Practices.pdf': 'e9885601-596b-4cd4-bfd3-d118228d34b6', 'deep seek technical paper.pdf': 'ff546cb0-22d3-49ca-ada9-9834c2cc6ac0', 'multimodal llm paper.pdf': 'c7e08d3a-db4a-43bd-b77f-8564ecb7b45b', 'NeurIPS 2025 CNN Paper.pdf': '9406ab48-5d83-4ceb-aabc-2e256cc8969a', 'Document AI.pdf': '13d62faf-47b7-437b-8504-bfa6192d4534', 'Paper ConTextTab.pdf': '6312faba-b4d4-40ce-bc09-10e9ff3dab29'}\n"
     ]
    }
   ],
   "source": [
    "url= f\"{AI_API_URL}/v2/lm/document-grounding/vector/collections/{collection_id}/documents\"\n",
    "\n",
    "\n",
    "headers = {\"Authorization\": f\"Bearer {access_token}\",\n",
    "           \"AI-Resource-Group\": \"default\",\n",
    "           \"Content-Type\": \"application/json\"}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "\n",
    "def get_document_ids_with_source(response_text):\n",
    "    result = {}\n",
    "    data = json.loads(response_text)\n",
    "    for resource in data.get(\"resources\", []):\n",
    "        doc_id = resource.get(\"id\")\n",
    "        file_name = None\n",
    "        for meta in resource.get(\"metadata\", []):\n",
    "            if meta.get(\"key\") == \"id\":\n",
    "                file_name = meta.get(\"value\", [None])[0]\n",
    "                break\n",
    "        if file_name and doc_id:\n",
    "            result[file_name] = doc_id\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "doc_map = get_document_ids_with_source(response.text)\n",
    "print(doc_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AI Best Practices.pdf': 'e9885601-596b-4cd4-bfd3-d118228d34b6',\n",
       " 'deep seek technical paper.pdf': 'ff546cb0-22d3-49ca-ada9-9834c2cc6ac0',\n",
       " 'multimodal llm paper.pdf': 'c7e08d3a-db4a-43bd-b77f-8564ecb7b45b',\n",
       " 'NeurIPS 2025 CNN Paper.pdf': '9406ab48-5d83-4ceb-aabc-2e256cc8969a',\n",
       " 'Document AI.pdf': '13d62faf-47b7-437b-8504-bfa6192d4534',\n",
       " 'Paper ConTextTab.pdf': '6312faba-b4d4-40ce-bc09-10e9ff3dab29'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this document mapping dictionary to update or delete the chunks for particular document using the document ID. In production, you may want to maintain these mappings in a HANA Table as well.\n",
    "\n",
    "Refer SAP Help page to [Update](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/update-document-adaa7cc44d334d89baf1ef666ac3158c) or [Delete](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/delete-document-529a5e8168604f8c80139c915df9a014) a document\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Retrieve Similar Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 200\n",
      "The concept of receptive ﬁeld is important for understanding and diagnosing how deep CNNs work.\n",
      "Since anywhere in an input image outside the receptive ﬁeld of a unit does not affect the value of that\n",
      "unit, it is necessary to carefully control the receptive ﬁeld, to ensure that it covers the entire relevant\n",
      "image region. In many tasks, especially dense prediction tasks like semantic image segmentation,\n",
      "stereo and optical ﬂow estimation, where we make a prediction for each single pixel in the input image,\n",
      "it is critical for each output pixel to have a big receptive ﬁeld, such that no important information is\n",
      "left out when making the prediction.\n",
      "The receptive ﬁeld size of a unit can be increased in a number of ways. One option is to stack more\n",
      "layers to make the network deeper, which increases the receptive ﬁeld size linearly by theory, as\n",
      "each extra layer increases the receptive ﬁeld size by the kernel size. Sub-sampling on the other hand\n",
      "Understanding the Effective Receptive Field in\n",
      "Deep Convolutional Neural Networks\n",
      "Wenjie Luo∗\n",
      "Yujia Li∗\n",
      "Raquel Urtasun\n",
      "Richard Zemel\n",
      "Department of Computer Science\n",
      "University of Toronto\n",
      "{wenjie, yujiali, urtasun, zemel}@cs.toronto.edu\n",
      "Abstract\n",
      "We study characteristics of receptive ﬁelds of units in deep convolutional networks.\n",
      "The receptive ﬁeld size is a crucial issue in many visual tasks, as the output must\n",
      "respond to large enough areas in the image to capture information about large\n",
      "objects. We introduce the notion of an effective receptive ﬁeld, and show that it\n",
      "both has a Gaussian distribution and only occupies a fraction of the full theoretical\n",
      "receptive ﬁeld. We analyze the effective receptive ﬁeld in several architecture\n",
      "designs, and the effect of nonlinear activations, dropout, sub-sampling and skip\n",
      "connections on it. This leads to suggestions for ways to address its tendency to be\n",
      "too small.\n",
      "1\n",
      "Introduction\n"
     ]
    }
   ],
   "source": [
    "url = f\"{AI_API_URL}/v2/lm/document-grounding/retrieval/search\"\n",
    "\n",
    "headers = {\n",
    "    \"AI-Resource-Group\": \"default\",\n",
    "    \"Authorization\": f\"Bearer {access_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"query\": \"What is efficient receptive field?\",\n",
    "    \"filters\": [\n",
    "        {\n",
    "            \"id\": \"string\",\n",
    "            \"searchConfiguration\": {\n",
    "                \"maxChunkCount\": 2\n",
    "            },\n",
    "            \"dataRepositories\": [collection_id], # Specify your repository ID(s)\n",
    "            \"dataRepositoryType\": \"vector\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "response_text = response.text\n",
    "\n",
    "import json\n",
    "\n",
    "# Parse the JSON string into a dictionary\n",
    "response_dict = json.loads(response_text)\n",
    "retrieved_docs = [] \n",
    "# Loop through and print each \"content\"\n",
    "for result in response_dict.get(\"results\", []):\n",
    "    for res in result.get(\"results\", []):\n",
    "        for document in res.get(\"dataRepository\", {}).get(\"documents\", []):\n",
    "            for chunk in document.get(\"chunks\", []):\n",
    "                retrieved_docs.append(chunk.get(\"content\", \"\"))\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1 [OPTIONAL]: Augment answer generation with retrieved documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ' '.join([c for c in retrieved_docs])\n",
    "\n",
    "query = \"What is receptive field?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Use the following context information to answer to user's query.\n",
    "Here is some context: {context}\n",
    "\n",
    "Based on the above context, answer the following query:\n",
    "{query}\n",
    "\n",
    "The answer tone has to be very professional in nature.\n",
    "\n",
    "If you don't know the answer, politely say that you don't know, don't try to make up an answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The concept of the receptive field pertains to deep Convolutional Neural Networks (CNNs) and is crucial for understanding and diagnosing their operations. In this context, a receptive field refers to the area in the input image that affects the value of a particular unit or neuron in the network. For tasks such as semantic image segmentation, stereo, and optical flow estimation—where predictions are made for each pixel in the image—ensuring a sufficiently large receptive field is critical. This guarantees that important information is not excluded from the analysis, thus influencing prediction accuracy. The receptive field size can be increased by adding more layers to the network, which expands it linearly according to the kernel size used in each layer. Additionally, the notion of an effective receptive field acknowledges that the practical influence of receptive fields may exhibit a Gaussian distribution and cover only a fraction of the full theoretical receptive area, with variations introduced by architectural designs, nonlinear activations, dropout, sub-sampling, and skip connections.\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.native.openai import chat\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an intelligent assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "kwargs = dict(model_name=\"gpt-4o\", messages=messages)\n",
    "\n",
    "response = chat.completions.create(**kwargs)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
