{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG with HANA Vector Store\n",
    "\n",
    "This notebook walks you through building a **Retrieval-Augmented Generation (RAG)** application using:\n",
    "- **HANA Vector Store**\n",
    "- **OpenAI GPT-4o**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Dependencies\n",
    "Ensure the libraries mentioned in the `requirements.txt` file are installed.\n",
    "\n",
    "\n",
    "### .env Variables\n",
    "Create a `.env` file with the following:\n",
    "```\n",
    "HANA_ADDRESS=your_hana_host\n",
    "HANA_PORT=your_port\n",
    "HANA_USER=your_user\n",
    "HANA_PASSWORD=your_password\n",
    "HANA_AUTOCOMMIT=true\n",
    "HANA_SSL_CERT_VALIDATE=false\n",
    "\n",
    "AICORE_AUTH_URL=your_aicore_auth_url\n",
    "AICORE_CLIENT_ID=your_aicore_client_id\n",
    "AICORE_CLIENT_SECRET=your_aicore_secret\n",
    "AICORE_RESOURCE_GROUP=your_resource_group\n",
    "AICORE_BASE_URL=your_base_url\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import section\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from hana_ml import ConnectionContext\n",
    "\n",
    "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client\n",
    "from gen_ai_hub.proxy.native.openai import embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "Use the user's query to retrieve most semantically similar documents from the vector store to create context that will be used to ground LLM for answer generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.00.000.00.1759828011 (fa/CE2025.2)\n",
      "AICOE\n"
     ]
    }
   ],
   "source": [
    "# Connect to SAP HANA\n",
    "cc = ConnectionContext(\n",
    "    address=os.environ.get(\"HANA_ADDRESS\"),\n",
    "    port=os.environ.get(\"HANA_PORT\"),\n",
    "    user=os.environ.get(\"HANA_USER\"),\n",
    "    password=os.environ.get(\"HANA_PASSWORD\"),\n",
    "    encrypt=True\n",
    ")\n",
    "\n",
    "cursor = cc.connection.cursor()\n",
    "\n",
    "print(cc.hana_version())\n",
    "print(cc.get_current_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AI Core proxy client\n",
    "proxy_client = get_proxy_client('gen-ai-hub')\n",
    "\n",
    "def get_embedding(query):\n",
    "    \"\"\"\n",
    "    Create embedding vector for given text.\n",
    "    \"\"\"\n",
    "    embeds = embeddings.create(\n",
    "        model_name=\"text-embedding-ada-002\",\n",
    "        input=query\n",
    "    )\n",
    "    return embeds.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vector_search(query, cursor, table_name, metric=\"COSINE_SIMILARITY\", k=4):\n",
    "    \"\"\"\n",
    "    Performs vector search on indexed documents.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_vector = get_embedding(query)\n",
    "        if not query_vector:\n",
    "            raise ValueError(\"Failed to generate query embedding.\")\n",
    "\n",
    "        sort_order = \"DESC\" if metric != \"L2DISTANCE\" else \"ASC\"\n",
    "        sql_query = f\"\"\"\n",
    "        SELECT TOP {k} ID, MY_TEXT, MY_METADATA\n",
    "        FROM {table_name}\n",
    "        ORDER BY {metric}(MY_VECTOR, TO_REAL_VECTOR('{query_vector}')) {sort_order}\n",
    "        \"\"\"\n",
    "        cursor.execute(sql_query)\n",
    "        return cursor.fetchall()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during vector search: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during vector search: (259, 'invalid table name:  Could not find table/view SCIENCE_DATA_MIT6 in schema AICOE: line 3 col 14 (at pos 60)')\n"
     ]
    }
   ],
   "source": [
    "query = \"How to test for fat in foods?\"\n",
    "\n",
    "# Retrieve top 4 matching docs from vector store\n",
    "context_records = run_vector_search(query, cursor, \"SCIENCE_DATA_MIT6\", 'COSINE_SIMILARITY', 4)\n",
    "# Join the content from retrieved docs\n",
    "context = ' '.join([c[1] for c in context_records])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment\n",
    "\n",
    "Augment the prompt instructions by embedding retrieved context into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Use the following context information to answer to user's query.\n",
    "Here is some context: {context}\n",
    "\n",
    "Based on the above context, answer the following query:\n",
    "{query}\n",
    "\n",
    "The answer tone has to be very professional in nature.\n",
    "\n",
    "If you don't know the answer, politely say that you don't know, don't try to make up an answer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation\n",
    "\n",
    "Use an LLM from Generative AI Hub to generate response for the context augmented prompt. This allows the LLM to be grounded on the context while answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To test for fat in foods, you can follow a few established methods:\n",
      "\n",
      "1. **Solvent Extraction Method**: This involves using a solvent like ether or hexane to dissolve the fat content in the food sample. The extract is then evaporated, and the remaining residue is weighed to determine the fat content.\n",
      "\n",
      "2. **Soxhlet Extraction**: This is a more precise method that also uses a solvent for extraction. The Soxhlet apparatus continuously extracts the fat from the sample, which is then collected, dried, and weighed.\n",
      "\n",
      "3. **Gravimetric Analysis**: This method involves drying the food sample and weighing it before and after the extraction of fat. The difference in weight gives an estimation of the fat content.\n",
      "\n",
      "4. **Infrared Analysis**: Using infrared spectroscopy, fat content can be measured by analyzing the absorption of specific wavelengths of light.\n",
      "\n",
      "5. **NMR (Nuclear Magnetic Resonance)**: This highly accurate method measures the fat content based on the magnetic properties of fat molecules in the food sample.\n",
      "\n",
      "All these methods require specific equipment and expertise to ensure accurate results. For professional analysis, it's recommended to engage a laboratory specializing in food testing.\n",
      "\n",
      "If additional details or specific methods were requested, I would recommend consulting a food scientist or another expert in food analysis, as they can provide more thorough and customized testing procedures.\n",
      "\n",
      "If you have any further questions or need information on a specific method, feel free to ask.\n"
     ]
    }
   ],
   "source": [
    "from gen_ai_hub.proxy.native.openai import chat\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an intelligent assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "kwargs = dict(model_name=\"gpt-4o\", messages=messages)\n",
    "\n",
    "response = chat.completions.create(**kwargs)\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
