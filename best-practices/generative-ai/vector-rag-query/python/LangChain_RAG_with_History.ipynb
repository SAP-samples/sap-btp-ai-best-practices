{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fecf4544",
   "metadata": {},
   "source": [
    "# LangChain RAG with HANA Vector Store and Optional Chat History\n",
    "\n",
    "This notebook walks you through building a **Retrieval-Augmented Generation (RAG)** application using:\n",
    "- **LangChain LCEL**\n",
    "- **HANA Vector Store**\n",
    "- **OpenAI GPT-4o**\n",
    "- And an **optional chat history feature**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e870a251",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Dependencies\n",
    "Ensure the libraries mentinoed in the `requirements.txt` file are installed.\n",
    "\n",
    "\n",
    "### .env Variables\n",
    "Create a `.env` file with the following:\n",
    "```\n",
    "HANA_ADDRESS=your_hana_host\n",
    "HANA_PORT=your_port\n",
    "HANA_USER=your_user\n",
    "HANA_PASSWORD=your_password\n",
    "HANA_AUTOCOMMIT=true\n",
    "HANA_SSL_CERT_VALIDATE=false\n",
    "\n",
    "AICORE_AUTH_URL=your_aicore_auth_url\n",
    "AICORE_CLIENT_ID=your_aicore_client_id\n",
    "AICORE_CLIENT_SECRET=your_aicore_secret\n",
    "AICORE_RESOURCE_GROUP=your_resource_group\n",
    "AICORE_BASE_URL=your_base_url\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and Imports\n",
    "\n",
    "We begin by importing necessary components and setting up the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3f57356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import section\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from hdbcli import dbapi\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_community.vectorstores import HanaDB\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "from gen_ai_hub.proxy.core.proxy_clients import get_proxy_client\n",
    "from gen_ai_hub.proxy.langchain.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9396d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_bool(value: str) -> bool:\n",
    "    '''\n",
    "    Convert environment variable to boolean based on values.\n",
    "    '''\n",
    "    return value.lower() in (\"true\", \"1\", \"yes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5f4de",
   "metadata": {},
   "source": [
    "## Set Up HANA Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d94635da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish connection to HANA Vector Store\n",
    "connection = dbapi.connect(\n",
    "    os.environ[\"HANA_ADDRESS\"],\n",
    "    os.environ[\"HANA_PORT\"],\n",
    "    os.environ[\"HANA_USER\"],\n",
    "    os.environ[\"HANA_PASSWORD\"],\n",
    "    str_to_bool(os.getenv(\"HANA_AUTOCOMMIT\", \"false\")),\n",
    "    str_to_bool(os.getenv(\"HANA_SSL_CERT_VALIDATE\", \"false\"))\n",
    ")\n",
    "\n",
    "# Establish connection to Generative AI Hub on AI Core for LLM access\n",
    "proxy_client = get_proxy_client(\"gen-ai-hub\")\n",
    "\n",
    "# Initialize embeddings model\n",
    "embedding_model = OpenAIEmbeddings(proxy_model_name=\"text-embedding-ada-002\")\n",
    "\n",
    "# Connect to HANA Vector Store and create LangChain object of the vector store for further operations\n",
    "vdb = HanaDB(\n",
    "    embedding=embedding_model,\n",
    "    connection=connection,\n",
    "    distance_strategy=DistanceStrategy.COSINE, # Specify distance metric to use for similarity\n",
    "    table_name=\"SAP_HELP_PUBLIC\",\n",
    "    # content_column=\"your_text_column\", # uncomment if the vector store was not created with LangChain and column names are not default ones\n",
    "    # metadata_column=\"your_metadata_column\", # uncomment if the vector store was not created with LangChain and column names are not default ones\n",
    "    # vector_column=\"your_vector_column\", # uncomment if the vector store was not created with LangChain and column names are not default ones\n",
    "    # vector_column_length=<your_vector_length>, # uncomment and specify if vector length was not set to default at the time of creation\n",
    ")\n",
    "\n",
    "# Set vector store as a retriever for retrieval related operations\n",
    "retriever = vdb.as_retriever(search_kwargs={\"k\": 2}) # specify number top matching docs to retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45b5fcf",
   "metadata": {},
   "source": [
    "## Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bae4865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt\n",
    "user_prompt = '''\n",
    "Use the following context information to answer to user's query.\n",
    "Here is some context: {context}\n",
    "\n",
    "Based on the above context, answer the following query:\n",
    "{question}\n",
    "\n",
    "The answer tone has to be very professional in nature.\n",
    "\n",
    "If you don't know the answer, politely say that you don't know.\n",
    "'''\n",
    "\n",
    "# Create langchain prompt object to fit into langchain pipeline\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"user\", user_prompt)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45611c82",
   "metadata": {},
   "source": [
    "## Chat History (Optional)\n",
    "\n",
    "We implement a capped in-memory message history. You can toggle storing this history using a flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f203d6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports for processing conversation history messages\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class InMemoryHistory(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"\n",
    "    A class with methods add/get/delete operations on history data.\n",
    "    \"\"\"\n",
    "    messages: List[BaseMessage] = Field(default_factory=list)\n",
    "\n",
    "    def add_message(self, message: BaseMessage) -> None:\n",
    "        self.messages.append(message)\n",
    "        self.messages = self.messages[-4:]\n",
    "\n",
    "    def add_messages(self, messages: List[BaseMessage]) -> None:\n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-4:]\n",
    "\n",
    "    def get_messages(self) -> List[BaseMessage]:\n",
    "        return self.messages\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []\n",
    "\n",
    "store = {} # Global dictionary to store history messages in memory\n",
    "\n",
    "def get_by_session_id(session_id: str) -> BaseChatMessageHistory:\n",
    "    \"\"\"\n",
    "    Retrieve history messages for given session id.\n",
    "    \"\"\"\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryHistory()\n",
    "    return store[session_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1278605",
   "metadata": {},
   "source": [
    "## Define the LLM and RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2e8e606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.openai import ChatOpenAI\n",
    "\n",
    "# Initialize LLM from Generative AI Hub. For now using OpenAI model. Other models are supported as well as mentioned on help.sap.com\n",
    "llm = ChatOpenAI(\n",
    "    proxy_model_name='gpt-4o', \n",
    "    proxy_client=proxy_client,\n",
    "    max_tokens=2000,\n",
    "    temperature=0.5\n",
    ")\n",
    "\n",
    "# Define langchain pipeline chain from taking a question till output parsing\n",
    "base_chain = (\n",
    "    {\n",
    "        \"context\": lambda x: retriever.invoke(x[\"question\"]),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"history\": lambda x: x.get(\"history\", [])\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671f490f",
   "metadata": {},
   "source": [
    "## Add Optional Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fe751fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the chain with history message processing runnable to add history messages to conversation if opted.\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_by_session_id,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccad97d",
   "metadata": {},
   "source": [
    "## Chat Function with History Toggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41244fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_rag(question: str, session_id: str, use_history: bool = True):\n",
    "    \"\"\"\n",
    "    Function to decide if history messages to be processed based user's input\n",
    "    \"\"\"\n",
    "    if use_history:\n",
    "        return chain_with_history.invoke(\n",
    "            {\"question\": question},\n",
    "            config={\"configurable\": {\"session_id\": session_id}}\n",
    "        )\n",
    "    else:\n",
    "        return base_chain.invoke({\"question\": question, \"history\": []})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec30402",
   "metadata": {},
   "source": [
    "## Run It!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "550f8796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business AI refers to the application of artificial intelligence technologies within a business context to achieve real business results. It encompasses various components that enhance business operations and decision-making processes. According to the context provided, Business AI includes:\n",
      "\n",
      "1. **AI Foundation on Business Technology Platform**: This serves as the base for integrating AI into business processes, ensuring that AI is responsible, reliable, and relevant.\n",
      "\n",
      "2. **Natural User Experience**: Facilitates human-machine interaction, making it easier for users to interact with AI systems in a natural and intuitive manner.\n",
      "\n",
      "3. **Automation**: Enables machines to perform repetitive tasks that humans typically handle, thereby increasing efficiency and productivity.\n",
      "\n",
      "4. **Insights, Optimization, & Predictions**: Augments human decision-making and cognitive processes by providing valuable insights, optimizing operations, and predicting future trends.\n",
      "\n",
      "Overall, Business AI aims to deliver tangible improvements in business performance and outcomes by leveraging advanced AI technologies.\n",
      "\n",
      "If you have any further questions or need additional information, please feel free to ask.\n",
      "\n",
      "-+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+-\n",
      "\n",
      "Certainly. Based on the context provided, Business AI refers to the application of artificial intelligence technologies within a business framework to achieve meaningful business outcomes. The information highlights that Business AI is designed to be relevant, reliable, and responsible, as indicated by SAP SE or an SAP affiliate company.\n",
      "\n",
      "If you require further details or have additional inquiries, please do not hesitate to ask.\n",
      "\n",
      "-+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+--+-\n",
      "\n",
      "I apologize, but I don't have access to previous interactions or answers. If you have any specific questions or need information based on the provided context, please feel free to ask, and I'll be happy to assist you.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    session_id = \"user-123\"\n",
    "    print(chat_with_rag(\"What is Business AI and can you summarize the context you received?\", session_id, use_history=True))\n",
    "    print()\n",
    "    print(\"-+-\" * 30)\n",
    "    print()\n",
    "    print(chat_with_rag(\"Can you repeat?\", session_id, use_history=True))\n",
    "\n",
    "    # Without history\n",
    "    print()\n",
    "    print(\"-+-\" * 30)\n",
    "    print()\n",
    "    print(chat_with_rag(\"What was your previous answer?\", session_id, use_history=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f5abb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- This setup uses **SAP HANA Vector Store** for retrieval.\n",
    "- Responses are generated using **GPT-4o** from OpenAI available on Generative AI Hub. Other models can be used as well as listed on [help.sap.com](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/supported-models)\n",
    "- **Chat history** is optional and supports follow-up questions. Current implementation processes history messages in-memory. However, the same can be stored in files or Database as well. Please refer [LangChain document](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html) for processing history messages.\n",
    "- Implemented using **LangChain Expression Language (LCEL)** for composability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281650fa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
