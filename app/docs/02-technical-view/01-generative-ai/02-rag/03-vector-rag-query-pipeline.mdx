---
title: "Vector-based RAG Query Pipeline"
description: ""
hide_title: true
---

import TabItem from "@theme/TabItem";
import Tabs from "@theme/Tabs";
import useBaseUrl from "@docusaurus/useBaseUrl";
import Icon from "@site/src/components/Icon";
import LoginWall from "@site/src/components/LoginWall";
import "@ui5/webcomponents-icons/dist/video.js";
import "@ui5/webcomponents-icons/dist/pdf-attachment.js";
import TrackableLink from "@site/src/components/TrackableLink";
import IconLinkButton from "@site/src/components/IconLinkButton";
import PageViewTracker from "@site/src/components/tracking/PageViewTracker";
import Link from "@docusaurus/Link";

<PageViewTracker />

<div className="hero-header">
  <h1>Vector-based RAG Query Pipeline</h1>
</div>

<div className="section-with-background">
  <div className="row">
    <div className="col col--8">
      <h2>Steps</h2>
      <ol className="steps-list">
        <li>
          <Link to="#1-overview">Overview</Link>
        </li>
        <li>
          <Link to="#2-pre-requisites">Pre-requisites</Link>
        </li>
        <li>
          <Link to="#3-key-choices-and-guidelines">Key Choices and Guidelines</Link>
        </li>
        <li>
          <Link to="#4-implementation">Implementation</Link>
        </li>
      </ol>
    </div>
    <div className="col col--4">
      <ul className="resource-links">
        <li>
          <Link
            target="_blank"
            to={useBaseUrl("generative-ai/vector-rag-query-pipeline/videos/BTP%20AI%20Best%20Practices%20-%20Vector-based%20RAG%20Query%20Pipeline%20intro.mp4")}
          >
            <Icon name="video" />
            <span>Teaser</span>
          </Link>
        </li>
        <LoginWall renderOnlyWhenLoggedIn={true}>
          <li>
            <Link target="_blank" to={useBaseUrl("generative-ai/vector-rag-query-pipeline/videos/BTP%20AI%20Best%20Practices%20-%20Vector-based%20RAG%20Query%20Pipeline.mp4")}>
              <Icon name="video" />
              <span>Webinar</span>
            </Link>
          </li>
          <li>
            <Link target="_blank" to={useBaseUrl("generative-ai/vector-rag-query-pipeline/pdfs/BTP%20AI%20Best%20Practices%20-%20Vector-based%20RAG%20Query%20Pipeline.pdf")}>
              <Icon name="pdf-attachment" />
              <span>Webinar (PDF Presentation)</span>
            </Link>
          </li>
        </LoginWall>
      </ul>
    </div>
  </div>
</div>

<LoginWall renderOnlyWhenLoggedIn={true}>
  <TrackableLink
    href="https://github.com/SAP-samples/sap-btp-ai-best-practices/tree/main/best-practices/vector-rag-query"
    className="button button--primary button--lg download-source-btn"
    target="_blank"
    trackingFeature="DOWNLOAD_VECTOR_RAG_QUERY_PIPELINE"
  >
    <span>Download Source Code</span>
  </TrackableLink>
</LoginWall>

## <span className="step-number">1</span> <span className="step-title">Overview</span>

### Description

Retrieval-Augmented Generation (RAG) is a powerful technique that combines the strengths of retrieval-based and [Large Language generative models (LLMs)](/docs/glossary#llm-large-language-model) to enhance the quality and relevance of generated content. Implementing a Vector Based RAG is a two step approach-

1. Embedding Creation - Embeddings are the numerical vector representation of the knowledge segments. In this step, you convert knowledge base to vectors and store them in [vector database](https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-vector-engine-guide/introduction?locale=en-US&q=HANA+VEctor+Engine). To have a better understanding of this topic and related best practices, it is recommended to go through [Vector Embedding(1/1)](/docs/technical-view/generative-ai/rag/vector-rag-embedding) part in this series.
2. Query Pipeline - In this step we leverage the vector representation of the knowledge base and augment the LLM context. To achieve this, first we convert the question into vector embedding and then perform similarity search in Vector database and fetch the best matches. Post that we leverage these best matches as augmented context along with prompt to LLM to generate a response for the user's question.

In this best practice we will focus on querying aspect when using RAG technique.

![xample Process Flow for a Vector Engine-powered RAG approach](@site/static/generative-ai/vector-rag-query-pipeline/images/RAG.png)

<LoginWall>

In the context of SAP's Generative AI Hub, it serves as a central platform for orchestrating AI processes, allowing developers to leverage pre-trained models and integrate external, domain-specific data to improve AI outputs. This approach is particularly beneficial in business scenarios where contextual relevance and data privacy are paramount. By utilizing SAP [HANA Cloud's Vector Engine](https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-vector-engine-guide/introduction?locale=en-US&q=HANA+VEctor+Engine) and the [Generative AI Hub SDK](https://pypi.org/project/generative-ai-hub-sdk/), developers can create sophisticated RAG applications that not only generate high-quality content but also maintain data privacy through techniques like data masking.

RAG technique is applicable and beneficial in a variety of GenAI scenarios. Some of the use cases are as follows:

- **Question Answering Systems:** RAG can power Q&A systems that deliver accurate and up-to-date responses, especially in domains with frequently changing information (e.g., finance, healthcare)
- **Chatbots and Virtual Assistants:** RAG-powered chatbots can provide more helpful and informative responses, drawing on relevant information from knowledge bases
- **Code Completion and Suggestion Systems:** RAG can enhance code suggestion systems by incorporating context-specific knowledge from code repositories or documentation.

### Expected Outcome

RAG technique helps to generate content to increase the quality while keeping it relevant to the business context (also called [grounding](/docs/glossary#grounding) in LLM space). Some of the key outcomes of the RAG technique are:

- **More Accurate and Relevant Answers:** Because the system retrieves context that aligns with the meaning of the query, the LLM is more likely to produce useful, correct, and directly related responses
- **Better User Experience:** Users can phrase queries naturally (like how they’d talk to a human) and still get meaningful answers. No need to “speak the system’s language.” This lowers the learning curve and boosts adoption.
- **Improved Trust & Reliability:** When answers are grounded in semantically relevant source material, it’s easier to show where the answer came from (source documents, paragraphs, etc.). This increases transparency and user trust, especially in enterprise or high-stakes settings.

### Benefits

- **Contextual Relevance:** Response contain passages that match the intent and meaning, not just the literal words.
- **Dynamic Adaptability:** You can ask open-ended or unstructured questions and still get good results.
- **Reduced Hallucinations:** High-quality, semantically matched context improves the factual accuracy of the final answer.

### Key Concepts

1. **Vector Similarity Search:** The process of finding the most relevant pieces of content based on closeness in vector space (using similarity metrics like cosine similarity).
2. **Data Masking:** A technique used to protect sensitive information by anonymizing or pseudonymizing data inputs, ensuring privacy and compliance. Data masking feature is available in Generative AI Hub orchestration layer, which helps you protect your sensitive information.
3. **Templating:** Templating is feature of the orchestration layer in Generative AI Hub that allow for the creation of structured prompts and the integration of domain-specific data to enhance AI model outputs.
4. **Context Window:** The amount of retrieved content the LLM can “see” when generating a response.
5. **Hybrid Search:** Combining vector search with keyword search for even better accuracy.

## <span className="step-number">2</span> <span className="step-title">Pre-requisites</span>

### Commercial

- SAP AI Core with the “Extended” tier on SAP Business Technology Platform
- SAP HANA Cloud on SAP BTP
- SAP AI Launchpad

You can find pricing details for the above services in the [SAP Discovery Center](https://discovery-center.cloud.sap/serviceCatalog?commercialModel=btpea&regions=all).

### Technical

1. Setup SAP Business Technology Platform (SAP BTP) subaccount ([Setup Guide](/docs/technology/sap-business-technology-platform))
2. Create an instance of SAP HANA Cloud ([Setup Guide](/docs/technology/sap-hana-cloud))
3. Create an instance of SAP AI Core ([Setup Guide](/docs/technology/sap-ai-core#setup))
4. Subscribe to SAP AI Launchpad ([Setup Guide](/docs/technology/sap-ai-launchpad))

### High-level reference architecture

![High-level Reference Architecture](@site/static/generative-ai/vector-rag-query-pipeline/images/architecture.png)

## <span className="step-number">3</span> <span className="step-title">Key Choices and Guidelines</span>

### Similarity Metric

Which [similarity metric](https://help.sap.com/docs/hana-cloud-database/sap-hana-cloud-sap-hana-database-vector-engine-guide/similarity-measures?locale=en-US&q=HANA+VEctor+Engine) to use, Cosine similarity, dot product, or Euclidean distance?

#### Guidelines

- Cosine similarity is the most common and robust for text embeddings.
- Use the metric best supported by your embedding model and vector database.

### Similarity Search

How to improve retrieval results?

#### Guidelines

- Different retrieval technique (e.g. Query Expansion, reranking, hybrid search etc.) can be employed to fetch the accurate and relevant chunks for the query asked by user

### Retrieval Parameter and Context Size

How many top results to retrieve (top_k).

#### Guidelines

- Start with top_k = 3–5, and tune based on output quality.
- Consider relevance scoring thresholds to avoid retrieving weak matches.
- Context size is a trade-off between information coverage vs no. of tokens (also cost).

### LLM Prompt Design

How to format the final prompt for the language model.

#### Guidelines

- Include retrieved context clearly (e.g., “Based on the following documents…”).
- Provide instructions or constraints (e.g., “Answer using only the context below.”)
- Use separators (--- or XML tags) to help the LLM distinguish source material from the prompt.

### Evaluation Strategy

How to measure system performance.

#### Guidelines

- Use retrieval metrics (e.g., precision@k, recall@k) and generation metrics (e.g., ROUGE, BLEU, factual consistency).
- Gather qualitative feedback from users (“Does the answer feel grounded?”)

## <span className="step-number">4</span> <span className="step-title">Implementation</span>

### Vector Embeddings

RAG is a powerful technique to ground context and control the size of information sent to LLMs. Before querying your RAG-powered chatbot or application, ensure that vector embeddings are created using your chosen vector engine. A well-designed chunking strategy is key to balancing context completeness with token limits.

**If you haven't created the embeddings yet, please refer to the [Embedding Creation](/docs/technical-view/generative-ai/rag/vector-rag-embedding) best practice page.**

### Programming Model Selection Guidelines

Programming and Runtime Selection Guidelines
While the Business Technology platform provides support for different development languages and several application runtimes, here are some of the recommendations while developing AI applications within the SAP ecosystem:

- **Backend-Only API:** Use **Python** or **JavaScript/TypeScript** (strong async capabilities, Node.js ecosystem).
  - **AI Workloads** - Use Python or any other language to build Docker images and host the AI application backend in AI Core. For an application that requires GPU resources for training/inference, AI Core-based deployment helps you scale the applications easily. For the code samples below, you would not require Docker-based development.
- **Fullstack Application (UI & Backend):** Use **CAP** (Cloud Application Programming Model) for optimized performance, scalability, and seamless SAP integration.

SAP offers strong development libraries and SDKs to support your development in the language of your choice. Refer to the [official SAP Libraries and SDKs](https://help.sap.com/docs/sap-ai-core/sap-ai-core-service-guide/libraries-and-sdks) page for the list of all available options.

<div className="tabs-with-background">
<Tabs groupId="language">
<TabItem value="python" label="Python" default>

<TrackableLink
  href="https://github.com/SAP-samples/sap-btp-ai-best-practices/tree/main/best-practices/vector-rag-query/python"
  className="button button--primary button--lg download-source-btn--tab"
  target="_blank"
  trackingFeature="DOWNLOAD_VECTOR_RAG_QUERY_PIPELINE"
>
  <span>Download Source Code</span>
</TrackableLink>

<h3>Recommendation</h3>

Python is one of the well-maintained and industry wide accepted language for AI/ML related developments. SAP also provides multiple SDKs to support development in Python. We recommend using the [SAP Generative AI hub SDK](https://pypi.org/project/generative-ai-hub-sdk/) to harness the power of generative AI models. If you are leveraging AI Core for deploy AI workload, you can use [SAP AI Core SDK](https://pypi.org/project/ai-core-sdk/) along with the [AI API Client SDK](https://pypi.org/project/ai-api-client-sdk/) for managing the AI Core lifecycle efficiently.

<h3>SDK</h3>

With this SDK you can leverage the power of generative models available in the generative AI Hub of SAP AI Core, it provides model access by wrapping the native SDKs of the model providers (OpenAI, Amazon, Google), through langchain or the orchestration service.

- [SAP Generative AI hub SDK](https://pypi.org/project/generative-ai-hub-sdk/)
- [SAP AI Core SDK](https://help.sap.com/docs/link-disclaimer?site=https://pypi.org/project/ai-core-sdk/) and [AI API Client SDK](https://pypi.org/project/ai-api-client-sdk/) (managing the AI Core lifecycle)
- [AI API Client SDK](https://pypi.org/project/ai-api-client-sdk/) (Unified interface to integrate apps via AI API)
- [HANA_ML SDK](https://pypi.org/project/hana-ml/) (Unified interface to leverage AI/ML capability in native HANA)

<h3>Tutorials and Learning Journeys</h3>

- If you are new to AI core and lifecycle management in AI Launchpad, we recommend going through [Predictive AI with SAP AI Core](https://developers.sap.com/group.ai-core-get-started-basics.html) tutorial to make yourself comfortable with its features and available best practices.
- We also recommend that you to go through the [Access to Generative AI Models](/docs/technical-view/generative-ai/plain/access-to-generative-ai-models) page and experiment with GenAI hub models.
- [RAG with HANA Vector Engine](https://developers.sap.com/tutorials/ai-core-genai-hana-vector.html) curates step by step process to do hands-on exercise on RAG leveraging HANA Vector Database.

<h3>Reference Code</h3>

<ul className="download-source-link-in-list-container">
  <li>
    <strong>
      <TrackableLink
        href="https://github.com/SAP-samples/sap-btp-ai-best-practices/tree/main/best-practices/vector-rag-query/python"
        target="_blank"
        trackingFeature="DOWNLOAD_VECTOR_RAG_QUERY_PIPELINE"
      >
        <span>SAP BTP AI Best Practices - Sample Code (End-to-end retrieval and generation scenario)</span>
      </TrackableLink>
    </strong>
  </li>
</ul>

You can also kickstart your development by referencing the below Python notebooks, which leverage [SAP GenAI hub SDK](https://pypi.org/project/generative-ai-hub-sdk/) to harness the LLM capabilities for RAG implementation:

- [RAG with HANA VectorDB Example](https://github.com/SAP-samples/ai-core-samples/blob/main/08_VectorStore/Hana/rag_hana_vector.ipynb)
- [GenAI - Vector DB Example](https://github.tools.sap/IA-C/sap-genai-hub-with-sap-hana-cloud-vector-engine/blob/main/genAI_vectordb.ipynb)
- [Short video](https://www.sap.com/assetdetail/2023/12/08854b59-9f7e-0010-bca6-c68f7e60039b.html) that covers a step-by-step walkthrough of a Python notebook for the RAG scenario leveraging HANA Vector Engine.

Additionally, you can also refer [SAP community blog](https://community.sap.com/t5/technology-blogs-by-sap/hana-vector-engine-and-langchain/ba-p/13636959) on the usage of HANA vectorDB and the Langchain framework.

<h4>Relevant Code</h4>

Below is a Python code snippet demonstrating the implementation of a RAG application using the Generative AI Hub SDK and SAP HANA Vector DB.

```python
#Assuming HANA Vector Store instance is already initialized and have all the vectors loaded already.

#db = HanaDB(
#    embedding=embeddings, connection=connection, table_name="AICORE_HELP"
#)

# Retrieve from vector store using Cosine Similarity
query = "What Is SAP AI Core?"
docs = db.similarity_search(query, k=2)

# Retrieve from vector store using Maximal Marginal Relevance Search (MMR)
# Maximal marginal relevance optimizes for similarity to query AND diversity among selected documents.
# The first 20 (fetch_k) items will be retrieved from the DB. The MMR algorithm will then find the best 2 (k) matches.
docs = db.max_marginal_relevance_search(query, k=2, fetch_k=20)
```

If you would like to augment LLM response with additional context without having a dedicated vector Database setup, you can leverage below code snippet to achieve the grounded response based on context provided. This technique is very helpful if you would like to provide document summary creation or in-document QnA feature in your application.

```python
from gen_ai_hub.proxy.langchain.init_models import init_embedding_model, init_llm
# Initialize the embedding model
embedding_model = init_embedding_model(model_name="your_embedding_model_name")
# Initialize the language model

llm = init_llm(model_name="your_llm_name")
# Example function to perform RAG
def perform_rag(query, context):
  # Retrieve relevant information using the embedding model
  retrieved_info = embedding_model.retrieve(query, context)
  # Generate response using the language model
  response = llm.generate(query, retrieved_info)
  return response
# Example usage
query = "What is the impact of data masking in AI?"
context = "Data masking is a technique used to protect sensitive information..."
response = perform_rag(query, context)
print(response)
```

</TabItem>

<TabItem value="javascript-typescript" label="JavaScript/TypeScript">

<TrackableLink
  href="https://github.com/SAP-samples/sap-btp-ai-best-practices/tree/main/best-practices/vector-rag-query/typescript"
  className="button button--primary button--lg download-source-btn--tab"
  target="_blank"
  trackingFeature="DOWNLOAD_VECTOR_RAG_QUERY_PIPELINE"
>
  <span>Download Source Code</span>
</TrackableLink>

<h3>Recommendation</h3>

The [SAP Cloud SDK for AI](https://github.com/SAP/ai-sdk-js) is the official SAP AI SDK for TypeScript.

<h3>SDK</h3>

With this SDK you can leverage the power of generative models available in the generative AI Hub of SAP AI Core, it provides model access by wrapping the native SDKs of the model providers (OpenAI, Amazon, Google), through langchain or the orchestration service.

- [SAP Cloud SDK for AI](https://github.com/SAP/ai-sdk-js)

<h3>Learning Journeys</h3>

#### Recommended

There are currently no learning journeys using the official SDK.

<h3>Reference Code</h3>

<ul className="download-source-link-in-list-container">
  <li>
    <strong>
      <TrackableLink
        href="https://github.com/SAP-samples/sap-btp-ai-best-practices/tree/main/best-practices/vector-rag-query/typescript"
        target="_blank"
        trackingFeature="DOWNLOAD_VECTOR_RAG_QUERY_PIPELINE"
      >
        <span>SAP BTP AI Best Practices - Sample Code</span>
      </TrackableLink>
    </strong>
  </li>
</ul>
- [SAP Cloud SDK for AI - Compute Embedding](https://github.com/SAP/ai-sdk-js/blob/main/sample-code/src/foundation-models/azure-openai.ts) (Look at the computeEmbedding() function to
create embeddings to store in a Vector Database) - [SAP Cloud SDK for AI - Document Grounding](https://github.com/SAP/ai-sdk-js/blob/main/sample-code/src/document-grounding.ts) (To
use the Document Grounding feature)

#### Relevant Code

```typescript
/**
 * Retrieve documents across data repositories.
 * @returns Search results.
 */
export async function retrieveDocuments(): Promise<RetievalSearchResults> {
  return RetrievalApi.search(
    {
      query: "When was the last time SAP AI SDK JavaScript end to end test was executed?",
      filters: [
        {
          id: "my-filter",
          searchConfiguration: {
            maxChunkCount: 10
          },
          dataRepositories: ["*"],
          dataRepositoryType: "vector",
          dataRepositoryMetadata: [],
          documentMetadata: [],
          chunkMetadata: []
        }
      ]
    },
    {
      "AI-Resource-Group": "ai-sdk-js-e2e"
    }
  ).execute();
}
```

</TabItem>

<TabItem value="cap-app" label="CAP">

<TrackableLink
  href="https://github.com/SAP-samples/sap-btp-ai-best-practices/tree/main/best-practices/vector-rag-query/cap"
  className="button button--primary button--lg download-source-btn--tab"
  target="_blank"
  trackingFeature="DOWNLOAD_VECTOR_RAG_QUERY_PIPELINE"
>
  <span>Download Source Code</span>
</TrackableLink>

<h3>Recommendation</h3>

There are 2 SDKs available for developing CAP applications, we recommend using the [SAP Cloud SDK for AI](https://github.com/SAP/ai-sdk-js) because it's the official SAP AI SDK, but the [CAP LLM Plugin](https://www.npmjs.com/package/cap-llm-plugin) has great code sample code to learn from.

<h3>SDKs</h3>

With this SDK you can leverage the power of generative models available in the generative AI Hub of SAP AI Core, it provides model access by wrapping the native SDKs of the model providers (OpenAI, Amazon, Google), through langchain or the orchestration service.

- **[SAP Cloud SDK for AI](https://github.com/SAP/ai-sdk-js) (Recommended)**
- [CAP LLM Plugin](https://www.npmjs.com/package/cap-llm-plugin)

<h3>Learning Journeys</h3>

#### Recommended

There are currently no learning journeys using the official SDK.

#### Other

- While you build your own application using SAP Cloud SDK or CAP LLM Plugin, you can refer SAP Discovery Center mission [GenAI Mail Insights: Develop a CAP application using GenAI and Retrieval Augmented Generation (RAG)](https://discovery-center.cloud.sap/missiondetail/4371/4655/). This mission covers a step-by-step process customer support scenario.

<h3>Reference Code</h3>

#### Recommended

<ul className="download-source-link-in-list-container">
  <li>
    <strong>
      <TrackableLink
        href="https://github.com/SAP-samples/sap-btp-ai-best-practices/tree/main/best-practices/vector-rag-query/cap"
        target="_blank"
        trackingFeature="DOWNLOAD_VECTOR_RAG_QUERY_PIPELINE"
      >
        <span>SAP BTP AI Best Practices - Sample Code</span>
      </TrackableLink>
    </strong>
  </li>
</ul>
- [SAP Cloud SDK for AI - Compute Embedding](https://github.com/SAP/ai-sdk-js/blob/main/sample-code/src/foundation-models/azure-openai.ts) (Look at the computeEmbedding() function to
create embeddings to store in a Vector Database) - [SAP Cloud SDK for AI - Document Grounding](https://github.com/SAP/ai-sdk-js/blob/main/sample-code/src/document-grounding.ts) (To
use the Document Grounding feature)

#### Other

- [CAP LLM Plugin - Harmonized RAG Quickstart CAP Application using CAP LLM Plugin's harmonized chat completion powered by SAP AI Core's orchestration service](https://github.com/SAP-samples/cap-llm-plugin-samples/tree/main/samples/harmonized-rag-app)
- [CAP LLM Plugin - RAG Quickstart using CAP LLM Plugin](https://github.com/SAP-samples/cap-llm-plugin-samples/tree/main/samples/rag-quickstart-app)

#### Relevant code

```typescript
/**
 * Retrieve documents across data repositories.
 * @returns Search results.
 */
export async function retrieveDocuments(): Promise<RetievalSearchResults> {
  return RetrievalApi.search(
    {
      query: "When was the last time SAP AI SDK JavaScript end to end test was executed?",
      filters: [
        {
          id: "my-filter",
          searchConfiguration: {
            maxChunkCount: 10
          },
          dataRepositories: ["*"],
          dataRepositoryType: "vector",
          dataRepositoryMetadata: [],
          documentMetadata: [],
          chunkMetadata: []
        }
      ]
    },
    {
      "AI-Resource-Group": "ai-sdk-js-e2e"
    }
  ).execute();
}
```

</TabItem>

</Tabs>
</div>

<br />

<div className="section-with-background blue">

## <span className="post-article-first-title">Related Best Practices</span>

<ul className="button-grid">
  <IconLinkButton href="/docs/technical-view/generative-ai/rag/vector-rag-embedding" text="Vector-based RAG Embedding" />
</ul>

</div>

<br />

<div className="section-with-background blue">

## Related AI Capabilities

<h3>
  <Link to={useBaseUrl("/docs/functional-view/content-creation")}>Content Creation</Link>
</h3>

<ul className="button-grid">
  <IconLinkButton href="/docs/functional-view/content-creation/generate-text-specific-knowledge" text="Generate Text based on Proprietary, Specific Knowledge" />
</ul>

<h3>
  <Link to={useBaseUrl("/docs/functional-view/information-analysis-and-processing")}>Information Analysis & Processing</Link>
</h3>

<ul className="button-grid">
  <IconLinkButton href="/docs/functional-view/information-analysis-and-processing/qa-on-enterprise-knowledge-base" text="Question & Answers on Enterprise Knowledge Base" />
</ul>

</div>

<br />

<div className="section-with-background gradient-violet-blue">

## Contributors

- Gupta, Chirag
- Behera, Bhagabat Prasad
- Marques, Luis
- CIGAINA, MARCO
- Robledo, Francisco
- Antonio, Dan
- Humphries, Ian

</div>

</LoginWall>
